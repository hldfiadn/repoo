# -*- coding: utf-8 -*-
"""Salinan Prediksi Penyakit Kronis 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10R4bap9lO2xL-mviqAYrzJJb33MNu7H5
"""

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

"""# Import Library"""

!pip install split-folders
!pip install tensorflow
!pip install keras_tuner
!pip install tensorflowjs

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.multioutput import MultiOutputClassifier

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

import pickle
import tensorflowjs as tfjs

from google.colab import drive
drive.mount('/content/drive')

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder

from google.colab import files
files.upload()

"""# Setup Folder dan Split Dataset Manual"""

!ls -lha kaggle.json
!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d iammustafatz/diabetes-prediction-dataset
!unzip diabetes-prediction-dataset.zip

!kaggle datasets download -d fedesoriano/stroke-prediction-dataset
!unzip stroke-prediction-dataset.zip

!kaggle datasets download -d alexteboul/heart-disease-health-indicators-dataset
!unzip heart-disease-health-indicators-dataset.zip

# Muat setiap dataset
df_heart = pd.read_csv('heart_disease_health_indicators_BRFSS2015.csv')
df_stroke = pd.read_csv('healthcare-dataset-stroke-data.csv')
df_diabetes = pd.read_csv('diabetes_prediction_dataset.csv')
# df_lung_cancer = pd.read_csv('lung_cancer_prediction_dataset.csv') # Opsional, jika ingin menambahkan kanker paru

print("Dataset berhasil dimuat.")
print(f"Shape Heart Disease: {df_heart.shape}")
print(f"Shape Stroke: {df_stroke.shape}")
print(f"Shape Diabetes: {df_diabetes.shape}")
# print(f"Shape Lung Cancer: {df_lung_cancer.shape}")

df_heart

df_stroke

df_diabetes

"""# Eksplorasi Data Awal (EDA) dan Pembersihan

## EDA untuk Heart Disease Dataset
"""

print("\n--- EDA untuk Heart Disease Dataset ---")
print(df_heart.head())
print(df_heart.info())
print("\nMissing Value:\n", df_heart.isnull().sum())
print("\n", df_heart['HeartDiseaseorAttack'].value_counts(normalize=True)) # Distribusi target

'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk'

kolom_kategorik1 = ['Sex', 'Education', 'Smoker', 'Diabetes', 'Income', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk']
for kolom in kolom_kategorik1:
    print(f"Unique values di kolom '{kolom}':")
    print(df_heart[kolom].unique())
    print('-' * 40)

df_heart = df_heart[df_heart['Diabetes'] != 1]

print("\nBanyaknya Data perstatus Stroke ", df_heart['Stroke'].value_counts(normalize=False)) # Distribusi target
print("\nBanyaknya Data perstatus Diabetes ", df_heart['Diabetes'].value_counts(normalize=False)) # Distribusi target
print("\nBanyaknya Data perstatus Penyakit Jantung ", df_heart['HeartDiseaseorAttack'].value_counts(normalize=False)) # Distribusi target

"""## EDA untuk Stroke Dataset"""

print("\n--- EDA untuk Stroke Dataset ---")
print(df_stroke.head())
print("\n", df_stroke.info())
print("\nMissing Value:\n", df_stroke.isnull().sum())
print("\nBanyaknya Data per-Target ", df_stroke['stroke'].value_counts(normalize=False)) # Distribusi target
print("\nDistribusi Target ", df_stroke['stroke'].value_counts(normalize=True)) # Distribusi target

print("\nBanyaknya Data MV per-ever_married ", df_stroke['ever_married'].value_counts(normalize=False)) # Distribusi target
print("\nBanyaknya Data MV per-work_type ", df_stroke['work_type'].value_counts(normalize=False)) # Distribusi target
print("\nBanyaknya Data MV per-Residence_type ", df_stroke['Residence_type'].value_counts(normalize=False)) # Distribusi target

df_stroke_mv = df_stroke[df_stroke['bmi'].isnull()]
print("\nBanyaknya Data MV per-Target ", df_stroke_mv['stroke'].value_counts(normalize=False)) # Distribusi target
print("\nDistribusi Data MV Target ", df_stroke_mv['stroke'].value_counts(normalize=True)) # Distribusi target
df_stroke_mv

df_stroke['bmi'] = df_stroke.groupby(['stroke','gender', 'hypertension', 'heart_disease', 'smoking_status'])['bmi'].transform(
    lambda x: x.fillna(x.median()))

print(df_stroke.head())
print("\nMissing Value:\n", df_stroke.isnull().sum())
print("\nBanyaknya Data per-Target ", df_stroke['stroke'].value_counts(normalize=False)) # Distribusi target
print("\nDistribusi Target ", df_stroke['stroke'].value_counts(normalize=True)) # Distribusi target

kolom_kategorik2 = df_stroke.select_dtypes(include=['object', 'category']).columns
for kolom in kolom_kategorik2:
    print(f"Unique values di kolom '{kolom}':")
    print(df_stroke[kolom].unique())
    print('-' * 40)

df_stroke_go = df_stroke[df_stroke['gender'] == 'Other']
print("\nBanyaknya Data gender = other per-Target ", df_stroke_go['stroke'].value_counts(normalize=False)) # Distribusi target
df_stroke_go

df_stroke = df_stroke[df_stroke['gender'] != 'Other']
kolom_kategorik2 = df_stroke.select_dtypes(include=['object', 'category']).columns
for kolom in kolom_kategorik2:
    print(f"Unique values di kolom '{kolom}':")
    print(df_stroke[kolom].unique())
    print('-' * 40)

"""## EDA untuk Diabetes Dataset"""

print("\n--- EDA untuk Diabetes Dataset ---")
print(df_diabetes.head())
print(df_diabetes.info())
print("\nMissing Value:\n", df_diabetes.isnull().sum())
print("\nBanyaknya Data MV per-Target ", df_diabetes['diabetes'].value_counts(normalize=False)) # Distribusi target
print("\nDistribusi Target ", df_diabetes['diabetes'].value_counts(normalize=True)) # Distribusi target

kolom_kategorik3 = df_diabetes.select_dtypes(include=['object', 'category']).columns
for kolom in kolom_kategorik3:
    print(f"Unique values di kolom '{kolom}':")
    print(df_diabetes[kolom].unique())
    print('-' * 40)

df_diabetes_go = df_diabetes[df_diabetes['gender'] == 'Other']
print("\nBanyaknya Data gender = other per-Target ", df_diabetes_go['diabetes'].value_counts(normalize=False)) # Distribusi target
df_diabetes_go

df_diabetes = df_diabetes[df_diabetes['gender'] != 'Other']
kolom_kategorik3 = df_diabetes.select_dtypes(include=['object', 'category']).columns
for kolom in kolom_kategorik3:
    print(f"Unique values di kolom '{kolom}':")
    print(df_diabetes[kolom].unique())
    print('-' * 40)

"""## Visualisasi Data"""

# Contoh visualisasi distribusi IMT dari dataset stroke
plt.figure(figsize=(8, 6))
sns.histplot(df_stroke['bmi'].dropna(), kde=True)
plt.title('Distribusi BMI di Dataset Stroke')
plt.xlabel('BMI')
plt.ylabel('Frekuensi')
plt.show()

plt.figure(figsize=(12, 10))
sns.heatmap(df_stroke.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap Korelasi untuk Stroke Dataset')
plt.show()

# Visualisasi korelasi di dataset jantung
plt.figure(figsize=(12, 10))
sns.heatmap(df_heart.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap Korelasi untuk Heart Disease Dataset')
plt.show()

# Contoh visualisasi distribusi IMT dari dataset dabetes
plt.figure(figsize=(8, 6))
sns.histplot(df_diabetes['bmi'].dropna(), kde=True)
plt.title('Distribusi BMI di Dataset Diabetes')
plt.xlabel('BMI')
plt.ylabel('Frekuensi')
plt.show()

plt.figure(figsize=(12, 10))
sns.heatmap(df_diabetes.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap Korelasi untuk Diabetes Dataset')
plt.show()

"""# Harmonisasi dan Penggabungan Dataset"""

# Rename kolom target untuk kejelasan dan konsistensi
df_heart = df_heart.rename(columns={'HeartDiseaseorAttack': 'HeartDisease'})
df_stroke = df_stroke.rename(columns={'stroke': 'Stroke'})
df_diabetes = df_diabetes.rename(columns={'diabetes': 'Diabetes'})

# Pilih fitur umum yang akan digunakan dan harmonisasikan nama kolom
# Ini adalah contoh, Anda mungkin perlu menyesuaikannya berdasarkan EDA Anda
common_features = [
    'Age', 'Sex', 'BMI', 'SmokingStatus', 'Hypertension', 'HeartDisease',
    'Glucose'
] # Ini harus diverifikasi lagi dengan kolom asli dari setiap dataset!

# Perbaiki nama kolom yang berbeda antar dataset (contoh)
df_heart = df_heart.rename(columns={
    'Age': 'Age', 'Sex': 'Gender', 'BMI': 'BMI', 'Smoker': 'SmokingStatus',
    'HighBP': 'Hypertension'
})
df_stroke = df_stroke.rename(columns={
    'gender': 'Gender', 'age': 'Age', 'avg_glucose_level': 'Glucose',
    'bmi': 'BMI', 'hypertension': 'Hypertension', 'heart_disease': 'HeartDisease',
    'smoking_status': 'SmokingStatus'
})
df_diabetes = df_diabetes.rename(columns={
    'gender': 'Gender', 'age': 'Age', 'bmi': 'BMI', 'hypertension': 'Hypertension',
    'heart_disease': 'HeartDisease', 'smoking_history': 'SmokingStatus',
    'blood_glucose_level': 'Glucose'
})

df_heart['Diabetes'] = df_heart['Diabetes'].map({0:0, 2: 1})
df_stroke['Gender'] = df_stroke['Gender'].map({'Female':0, 'Male':1})
df_diabetes['Gender'] = df_diabetes['Gender'].map({'Female':0, 'Male':1})
df_stroke['SmokingStatus'] = df_stroke['SmokingStatus'].map({'never smoked': 0, 'smokes': 1, 'formerly smoked': 2, 'Unknown': 3})
df_diabetes['SmokingStatus'] = df_diabetes['SmokingStatus'].map({'never': 0, 'current': 1, 'former': 2, 'ever': 2, 'not current': 2, 'No Info': 3})

df_heart

"""## Imputasi Nilai Glucose Dataset HeartDisease"""

df_heart['Glucose'] = np.nan

df_heart

df_diabetes

selected_features = ['Gender', 'Age', 'Hypertension', 'HeartDisease', 'SmokingStatus', 'BMI', 'Diabetes']

# Fitur dan target untuk model
X_train = df_diabetes[selected_features]
y_train = df_diabetes['Glucose']

X_missing = df_heart[selected_features]

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_missing_scaled = scaler.transform(X_missing)

# Menggunakan Random Forest
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

predicted_glucose = model.predict(X_missing_scaled)
# Masukkan prediksi ke df_missing
df_heart.loc[:, 'Glucose'] = predicted_glucose
df_heart['Glucose'] = df_heart['Glucose'].round(0).astype(int)

df_heart

"""# Pemisahan Fitur dan Target"""

# --- 3. Pemisahan Fitur dan Target ---
# Fitur (X) dan Target (Y)
# Target di sini adalah multi-output: penyakit jantung, stroke, diabetes
target_columns = ['HeartDisease', 'Stroke', 'Diabetes']

#features_columns = [col for col in df_heart.columns if col not in target_columns]
features_columns = ['Hypertension', 'Gender', 'BMI', 'SmokingStatus', 'Age', 'Glucose']
features_columns1 = ['Hypertension', 'Gender', 'BMI', 'SmokingStatus', 'Age', 'Glucose']

X2 = df_heart[features_columns]
X = df_heart.drop(columns=target_columns)
y = df_heart[target_columns]


print("\nBentuk X:", X.shape)
# print("Bentuk X1:", X1.shape)
print("Bentuk y:", y.shape)

X2.columns

X2

X

X.info()

X.hist(figsize=(12, 8), bins=20)
plt.tight_layout()
plt.show()

X2.hist(figsize=(12, 8), bins=20)
plt.tight_layout()
plt.show()

"""# Penanganan Data Imbalance

## 1

SMOTE per label
"""

X_balanced_list = []
y_balanced_list = []

min_class_len = []

"""SMOTE untuk setiap label, dan catat jumlah minimum antar kelas"""

for col in target_columns:
    smote = SMOTE(random_state=42)
    X_res, y_res = smote.fit_resample(X, y[col])

    # Cari jumlah terkecil antara kelas 0 dan 1
    count_0 = (y_res == 0).sum()
    count_1 = (y_res == 1).sum()
    min_len = min(count_0, count_1)
    min_class_len.append(min_len)

    # Simpan hasil SMOTE sementara
    X_balanced_list.append(X_res)
    y_balanced_list.append(y_res)

"""Ambil jumlah minimum per label setelah SMOTE dan potong ke jumlah itu per kelas"""

# Cari min_len global dari hasil SMOTE per label
min_len_global = min(
    min((y_res == 0).sum(), (y_res == 1).sum())
    for y_res in y_balanced_list
)

# Inisialisasi ulang untuk final list
final_X_list = []
final_y_list = []

for i in range(len(target_columns)):
    X_res = X_balanced_list[i]
    y_res = y_balanced_list[i]

    # Ambil indeks 0 dan 1 sesuai jumlah min_len_global
    idx_0 = np.where(y_res == 0)[0][:min_len_global]
    idx_1 = np.where(y_res == 1)[0][:min_len_global]
    selected_idx = np.concatenate([idx_0, idx_1])

    final_X_list.append(X_res.iloc[selected_idx])
    final_y_list.append(y_res.iloc[selected_idx])

"""Menggabungkan semuanya dan konversi ke DF"""

X_final = final_X_list[0]
y_final = np.column_stack(final_y_list)

X = pd.DataFrame(X_final, columns=X.columns)
y = pd.DataFrame(y_final, columns=target_columns)

X

"""Pembulatan Nilai"""

# List variabel (kolom) yang akan dikecualikan dari pembulatan
exclude_cols = ['Glucose']

# Identifikasi kolom numerik yang akan dibulatkan
numeric_cols_to_round = [col for col in X.select_dtypes(include=np.number).columns if col not in exclude_cols]

# Bulatkan kolom numerik yang terpilih menjadi 0 angka desimal
for col in numeric_cols_to_round:
    X[col] = X[col].round(0) # Perubahan di sini: .round(0)

# Tentukan kolom-kolom float yang ingin diubah menjadi int
cols_to_convert = numeric_cols_to_round

# Lakukan perubahan tipe data
for col in cols_to_convert:
    # Pastikan tidak ada NaN atau isi NaN jika perlu (misal dengan 0)
    # Jika Anda yakin tidak ada NaN, baris di bawah ini bisa diabaikan
    # df[col] = df[col].fillna(0) # Contoh: mengisi NaN dengan 0

    X[col] = X[col].astype(int)

print("\nDataFrame setelah pembulatan menjadi 0 angka desimal:")
X

# Saring kolom yang dikecualikan
cols_to_convert_to_cat = ['Hypertension', 'HighChol', 'CholCheck', 'Gender', 'Education', 'SmokingStatus', 'Income', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'DiffWalk']

print(f"\nKolom yang akan diubah ke 'category': {cols_to_convert_to_cat}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_cat:
    X[col] = X[col].astype('category')

print("\nX Info:", X.info())
X

"""Cek distribusi tiap label"""

for col in y.columns:
    print(f"Label '{col}' value counts:")
    print(y[col].value_counts())
    print()

print("\nX Info:", y.info())
y

# Untuk mengunduh sebagai CSV
y.to_csv('y_final_data.csv', index=False)
print("File 'y_final_data.csv' telah berhasil disimpan di sesi Colab Anda.")

# Saring kolom yang dikecualikan
cols_to_convert_to_int = y.columns
print(f"\nKolom yang akan diubah ke 'integer': {cols_to_convert_to_int}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_int:
    y[col] = y[col].astype('int')

print("\nX Info:", y.info())
y

df_target = y.copy()
df_target['combined_target'] = df_target['HeartDisease'].astype(str) + \
                        df_target['Stroke'].astype(str) + \
                        df_target['Diabetes'].astype(str)

kolomm = ['combined_target']
for kolom in kolomm:
    print(f"Unique values di kolom '{kolom}':")
    print(df_target[kolom].unique())
    print('-' * 40)

"""## 3"""

import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE # Menggunakan SMOTE
from collections import Counter

# Asumsi: Anda sudah memiliki DataFrame Anda bernama 'df'

dataframe = df_heart.copy()
cols_to_convert_to_int = target_columns
print(f"\nKolom yang akan diubah ke 'integer': {cols_to_convert_to_int}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_int:
    dataframe[col] = dataframe[col].astype('int')

print("\nX Info:", dataframe.info())
dataframe

print("Shape DataFrame Awal:", dataframe.shape)
print("\nDistribusi Variabel Target Awal:")
print(dataframe[['HeartDisease', 'Stroke', 'Diabetes']].apply(pd.Series.value_counts))

# --- 2. Menggabungkan 3 Variabel Target menjadi 1 Variabel 8 Kelas ---

# Membuat string kombinasi dari 3 target
# Contoh: '000', '001', '010', '011', '100', '101', '110', '111'
dataframe['combined_target'] = dataframe['HeartDisease'].astype(str) + \
                        dataframe['Stroke'].astype(str) + \
                        dataframe['Diabetes'].astype(str)

# Untuk SMOTE, variabel target (y) harus berupa integer atau numerik.
# Kita perlu mengonversi string kombinasi menjadi nilai numerik.
# Salah satu caranya adalah dengan menganggapnya sebagai bilangan biner, lalu mengonversinya ke desimal.
# '000' -> 0, '001' -> 1, '010' -> 2, ..., '111' -> 7
dataframe['combined_target_numeric'] = dataframe['combined_target'].apply(lambda x: int(x, 2))


print("\nDistribusi 'combined_target_numeric' sebelum balancing:")
print(dataframe['combined_target_numeric'].value_counts())

# Memisahkan fitur (X) dan target (y)
# Pastikan X hanya berisi fitur numerik saat menggunakan SMOTE
X = dataframe.drop(['HeartDisease', 'Stroke', 'Diabetes', 'combined_target', 'combined_target_numeric'], axis=1)
y = dataframe['combined_target_numeric'] # Menggunakan target numerik

# --- 3. Melakukan Prosedur Balancing Data ---

# Inisialisasi SMOTE
# SMOTE bekerja dengan fitur input numerik.
smote = SMOTE(random_state=42) # random_state untuk reproduktifitas

print(f"\nJumlah sampel per kelas sebelum balancing: {Counter(y)}")

X_resampled, y_resampled = smote.fit_resample(X, y)

print(f"Jumlah sampel per kelas setelah balancing: {Counter(y_resampled)}")
print(f"Shape X_resampled setelah balancing: {X_resampled.shape}")

# Menggabungkan kembali X_resampled dan y_resampled menjadi DataFrame baru
df_balanced = pd.DataFrame(X_resampled, columns=X.columns)
df_balanced['combined_target_numeric'] = y_resampled

print("\nShape DataFrame setelah balancing:", df_balanced.shape)

# --- 4. Mengubah Format Variabel Target Kembali Menjadi 3 Variabel Target Semula ---

# Mengonversi kembali 'combined_target_numeric' ke string biner 3 digit
# Misalnya, 0 -> '000', 1 -> '001', ..., 7 -> '111'
df_balanced['combined_target_str'] = df_balanced['combined_target_numeric'].apply(lambda x: format(x, '03b'))

# Memisahkan 'combined_target' menjadi 3 kolom target asli
df_balanced['HeartDisease'] = df_balanced['combined_target_str'].str[0].astype(int)
df_balanced['Stroke'] = df_balanced['combined_target_str'].str[1].astype(int)
df_balanced['Diabetes'] = df_balanced['combined_target_str'].str[2].astype(int)

# Opsional: Hapus kolom 'combined_target_numeric' dan 'combined_target_str' jika tidak diperlukan lagi
df_balanced = df_balanced.drop(['combined_target_numeric', 'combined_target_str'], axis=1)

print("\nDistribusi Variabel Target Setelah Resampling dan Pemisahan:")
print(df_balanced[['HeartDisease', 'Stroke', 'Diabetes']].apply(pd.Series.value_counts))

print("\nDataFrame Akhir setelah balancing dan pemisahan kembali (beberapa baris pertama):")
print(df_balanced.head())

X = df_balanced.drop(columns=target_columns)
y = df_balanced[target_columns]

X

# List variabel (kolom) yang akan dikecualikan dari pembulatan
exclude_cols = ['Glucose']

# Identifikasi kolom numerik yang akan dibulatkan
numeric_cols_to_round = [col for col in X.select_dtypes(include=np.number).columns if col not in exclude_cols]

# Bulatkan kolom numerik yang terpilih menjadi 0 angka desimal
for col in numeric_cols_to_round:
    X[col] = X[col].round(0) # Perubahan di sini: .round(0)

# Tentukan kolom-kolom float yang ingin diubah menjadi int
cols_to_convert = numeric_cols_to_round

# Lakukan perubahan tipe data
for col in cols_to_convert:
    # Pastikan tidak ada NaN atau isi NaN jika perlu (misal dengan 0)
    # Jika Anda yakin tidak ada NaN, baris di bawah ini bisa diabaikan
    # df[col] = df[col].fillna(0) # Contoh: mengisi NaN dengan 0

    X[col] = X[col].astype(int)

print("\nDataFrame setelah pembulatan menjadi 0 angka desimal:")
X

# Saring kolom yang dikecualikan
cols_to_convert_to_cat = ['Hypertension', 'HighChol', 'CholCheck', 'Gender', 'Education', 'SmokingStatus', 'Income', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'DiffWalk']

print(f"\nKolom yang akan diubah ke 'category': {cols_to_convert_to_cat}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_cat:
    X[col] = X[col].astype('category')

print("\nX Info:", X.info())
X

print("\nX Info:", y.info())
y

for col in y.columns:
    print(f"Label '{col}' value counts:")
    print(y[col].value_counts())
    print()

# Saring kolom yang dikecualikan
cols_to_convert_to_int = y.columns
print(f"\nKolom yang akan diubah ke 'integer': {cols_to_convert_to_int}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_int:
    y[col] = y[col].astype('int')

print("\nX Info:", y.info())
y

"""## SMOTEN"""

import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTEN # Contoh teknik balancing untuk data kategorikal
from collections import Counter

# Asumsi: Anda sudah memiliki DataFrame Anda bernama 'df'

dataframe = df_heart.copy()
cols_to_convert_to_int = target_columns
print(f"\nKolom yang akan diubah ke 'integer': {cols_to_convert_to_int}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_int:
    dataframe[col] = dataframe[col].astype('int')

print("\nX Info:", dataframe.info())
dataframe

print("Shape DataFrame Awal:", dataframe.shape)
print("\nDistribusi Variabel Target Awal:")
print(dataframe[['HeartDisease', 'Stroke', 'Diabetes']].apply(pd.Series.value_counts))

# --- 2. Menggabungkan 3 Variabel Target menjadi 1 Variabel 8 Kelas ---

# Membuat string kombinasi dari 3 target
# Contoh: '000', '001', '010', '011', '100', '101', '110', '111'
dataframe['combined_target'] = dataframe['HeartDisease'].astype(str) + \
                        dataframe['Stroke'].astype(str) + \
                        dataframe['Diabetes'].astype(str)

# Untuk SMOTE, variabel target (y) harus berupa integer atau numerik.
# Kita perlu mengonversi string kombinasi menjadi nilai numerik.
# Salah satu caranya adalah dengan menganggapnya sebagai bilangan biner, lalu mengonversinya ke desimal.
# '000' -> 0, '001' -> 1, '010' -> 2, ..., '111' -> 7
dataframe['combined_target_numeric'] = dataframe['combined_target'].apply(lambda x: int(x, 2))


# Mengubah string kombinasi menjadi nilai numerik/kategorikal
# Kita bisa mapping string ini ke integer 0-7, atau biarkan sebagai string
# Untuk SMOTEN, lebih baik jika target adalah integer atau string kategorikal
# Biarkan sebagai string untuk kemudahan representasi dan SMOTEN dapat menanganinya.

print("\nDistribusi 'combined_target_numeric' sebelum balancing:")
print(dataframe['combined_target_numeric'].value_counts())

# Memisahkan fitur (X) dan target (y)
X = dataframe.drop(['HeartDisease', 'Stroke', 'Diabetes', 'combined_target', 'combined_target_numeric'], axis=1)
y = dataframe['combined_target_numeric']

# --- 3. Melakukan Prosedur Balancing Data ---

# Inisialisasi SMOTEN (untuk data dengan fitur dan target kategorikal/campuran)
# Jika fitur Anda hanya numerik, Anda bisa pakai SMOTE atau ADASYN.
# Jika fitur Anda campuran (numerik dan kategorikal), SMOTENC atau SMOTEN lebih cocok.
# SMOTEN secara spesifik cocok untuk dataset di mana fitur dan target bisa berupa kategorikal.
smoten = SMOTEN(random_state=42) # random_state untuk reproduktifitas

print(f"\nJumlah sampel per kelas sebelum balancing: {Counter(y)}")

X_resampled, y_resampled = smoten.fit_resample(X, y)

print(f"Jumlah sampel per kelas setelah balancing: {Counter(y_resampled)}")
print(f"Shape X_resampled setelah balancing: {X_resampled.shape}")

# Menggabungkan kembali X_resampled dan y_resampled menjadi DataFrame baru
df_balanced = pd.DataFrame(X_resampled, columns=X.columns)
df_balanced['combined_target_numeric'] = y_resampled

print("\nShape DataFrame setelah balancing:", df_balanced.shape)

# --- 4. Mengubah Format Variabel Target Kembali Menjadi 3 Variabel Target Semula ---

# Mengonversi kembali 'combined_target_numeric' ke string biner 3 digit
# Misalnya, 0 -> '000', 1 -> '001', ..., 7 -> '111'
df_balanced['combined_target_str'] = df_balanced['combined_target_numeric'].apply(lambda x: format(x, '03b'))

# Memisahkan 'combined_target' menjadi 3 kolom target asli
df_balanced['HeartDisease'] = df_balanced['combined_target_str'].str[0].astype(int)
df_balanced['Stroke'] = df_balanced['combined_target_str'].str[1].astype(int)
df_balanced['Diabetes'] = df_balanced['combined_target_str'].str[2].astype(int)

# Opsional: Hapus kolom 'combined_target_numeric' dan 'combined_target_str' jika tidak diperlukan lagi
df_balanced = df_balanced.drop(['combined_target_numeric', 'combined_target_str'], axis=1)

print("\nDistribusi Variabel Target Setelah Resampling dan Pemisahan:")
print(df_balanced[['HeartDisease', 'Stroke', 'Diabetes']].apply(pd.Series.value_counts))

print("\nDataFrame Akhir setelah balancing dan pemisahan kembali (beberapa baris pertama):")
print(df_balanced.head())

file_path = "/content/drive/My Drive/Capstone LaskarAI/Data SMOTEN.csv"  # Nama file output
df_balanced.to_csv(file_path, index=True)  # index=False agar indeks tidak disertakan dalam file Excel

"""## Preprocessing Data Balancing"""

# df_balanced = pd.read_csv('/content/drive/My Drive/Capstone LaskarAI/Data_SMOTEN.csv')
df_balanced = pd.read_csv('/content/drive/My Drive/Capstone LaskarAI/Data_SMOTEN2.csv') #Tanpa Variabel Gula Darah

# atau upload file nya
# df_balanced = pd.read_csv('Data SMOTEN.csv')
df_balanced.drop(columns=['Unnamed: 0'], inplace=True)
df_balanced

df_balanced = pd.read_csv('Data SMOTEN.csv')
df_balanced.drop(columns=['Unnamed: 0'], inplace=True)
df_balanced

df_balanced.describe()

df_balanced.info()

df = df_balanced.copy()
numeric_cols_to_round = [col for col in df.select_dtypes(include=np.number).columns]
# Bulatkan kolom numerik yang terpilih menjadi 0 angka desimal
for col in numeric_cols_to_round:
    df[col] = df[col].round(0).astype(int) # Perubahan di sini: .round(0)

print("\nDataFrame setelah pembulatan menjadi 0 angka desimal:")
df.info()

# Saring kolom yang dikecualikan
cols_to_convert_to_cat = ['GenHlth', 'Education', 'Income']
print(f"\nKolom yang akan diubah ke 'category': {cols_to_convert_to_cat}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_cat:
    df[col] = df[col].astype('category')

print("\ndf Info:", df.info())
df

onehot_cols = ['GenHlth', 'Education', 'Income']

# 2. One-hot encoding untuk kolom kategorikal dengan unique > 2
df_encode = pd.get_dummies(df, columns=onehot_cols, drop_first=False).astype(int)
df_encode

df

target_columns = ['HeartDisease', 'Stroke', 'Diabetes']
X = df_encode.drop(columns=target_columns)
y = df_encode[target_columns]

#TARGET_COLUMNS = ['HeartDisease', 'Stroke', 'Diabetes']
target_columns = ['HeartDisease', 'Stroke', 'Diabetes']
features = [col for col in df_encode.columns if col not in target_columns + ['stratify_label']] # Tambahkan 'stratify_label' agar tidak jadi fitur

# --- Stratifikasi Kustom untuk Multi-Label ---
df_encode['stratify_label'] = df_encode[target_columns].astype(str).agg(''.join, axis=1)

# --- 3. Split Data into Training and Testing Sets ---
train_df, test_df = train_test_split(
    df_encode, test_size=0.2, random_state=42,
    stratify=df_encode['stratify_label']
)

# Hapus kolom 'stratify_label' dari DataFrame training dan testing
train_df = train_df.drop(columns=['stratify_label'])
test_df = test_df.drop(columns=['stratify_label'])

# Identifikasi kolom numerik yang perlu di-scaling (misalnya, 'BMI', 'PhysicalHealth', 'MentalHealth', 'SleepTime')
# Jangan scaling kolom yang sudah kategorikal atau biner

numerical_features = ['BMI', 'PhysHlth', 'MentHlth', 'Age'] # Contoh, sesuaikan dengan kolom X Anda
scaler = StandardScaler()

# Terapkan scaling HANYA pada fitur numerik di data training dan testing
train_df[numerical_features] = scaler.fit_transform(train_df[numerical_features])
test_df[numerical_features] = scaler.transform(test_df[numerical_features])

X_train = train_df.drop(columns=target_columns)
y_train = train_df[target_columns]

X_test = test_df.drop(columns=target_columns)
y_test = test_df[target_columns]

print(X_train.info(), y_train.info(), X_test.info(), y_test.info(),)

# List variabel (kolom) yang akan dikecualikan dari pembulatan
#exclude_cols = ['Glucose']

# Identifikasi kolom numerik yang akan dibulatkan
#numeric_cols_to_round = [col for col in X.select_dtypes(include=np.number).columns if col not in exclude_cols]
numeric_cols_to_round = [col for col in X.select_dtypes(include=np.number).columns]
# Bulatkan kolom numerik yang terpilih menjadi 0 angka desimal
for col in numeric_cols_to_round:
    X[col] = X[col].round(0).astype(int) # Perubahan di sini: .round(0)

print("\nDataFrame setelah pembulatan menjadi 0 angka desimal:")
X

# Saring kolom yang dikecualikan
cols_to_convert_to_cat = ['GenHlth', 'Education', 'Income']
print(f"\nKolom yang akan diubah ke 'category': {cols_to_convert_to_cat}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_cat:
    X[col] = X[col].astype('category')

print("\nX Info:", X.info())
X

print("\nX Info:", y.info())
y

for col in y.columns:
    print(f"Label '{col}' value counts:")
    print(y[col].value_counts())
    print()

# Saring kolom yang dikecualikan
cols_to_convert_to_int = y.columns
print(f"\nKolom yang akan diubah ke 'integer': {cols_to_convert_to_int}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_int:
    y[col] = y[col].astype('int')

print("\nX Info:", y.info())
y

df_target = y.copy()
df_target['combined_target'] = df_target['HeartDisease'].astype(str) + \
                        df_target['Stroke'].astype(str) + \
                        df_target['Diabetes'].astype(str)

kolomm = ['combined_target']
for kolom in kolomm:
    print(f"Unique values di kolom '{kolom}':")
    print(df_target[kolom].unique())
    print('-' * 40)

y

"""# Tensorflow"""

# Install YDF!
# If you haven't already, run this in your environment:
# pip install ydf -U

import ydf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    confusion_matrix,
    roc_curve,
    accuracy_score
)
import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. Load Your Dataset ---
# Replace this with how you load your actual data.
# For demonstration, I'm creating a dummy DataFrame that matches your description.
np.random.seed(42)
num_samples = 1550344 # Matches your 'count'

data = {
    'Hypertension': np.random.randint(0, 2, num_samples),
    'HighChol': np.random.randint(0, 2, num_samples),
    'CholCheck': np.random.randint(0, 2, num_samples),
    'BMI': np.random.uniform(12, 98, num_samples),
    'SmokingStatus': np.random.randint(0, 2, num_samples),
    'PhysActivity': np.random.randint(0, 2, num_samples),
    'Fruits': np.random.randint(0, 2, num_samples),
    'Veggies': np.random.randint(0, 2, num_samples),
    'HvyAlcoholConsump': np.random.randint(0, 2, num_samples),
    'AnyHealthcare': np.random.randint(0, 2, num_samples),
    'MentHlth': np.random.randint(0, 31, num_samples),
    'PhysHlth': np.random.randint(0, 31, num_samples),
    'DiffWalk': np.random.randint(0, 2, num_samples),
    'Gender': np.random.randint(0, 2, num_samples),
    'Age': np.random.randint(1, 14, num_samples),
    'Education': np.random.randint(1, 7, num_samples),
    'Income': np.random.randint(1, 9, num_samples),
    'HeartDisease': np.random.randint(0, 2, num_samples),
    'Stroke': np.random.randint(0, 2, num_samples),
    'Diabetes': np.random.randint(0, 2, num_samples),
}
df = pd.DataFrame(data)

print("Data loaded and dummy data created successfully.")
print(df.head())
print(df.info())

# --- 2. Define Target Columns ---
TARGET_COLUMNS = ['HeartDisease', 'Stroke', 'Diabetes']
FEATURES = [col for col in df.columns if col not in TARGET_COLUMNS]

# --- 3. Split Data into Training and Testing Sets ---
# For YDF, we pass the full DataFrame with features and the specific label.
# So, we'll split the entire DataFrame first, then select labels within the training loop.
train_df, test_df = train_test_split(
    df, test_size=0.2, random_state=42,
    # Stratify by one of the targets, or consider a custom stratification for multi-label
    # For simplicity, we'll stratify by HeartDisease for now.
    stratify=df['HeartDisease']
)

print(f"\nTraining set size: {len(train_df)} samples")
print(f"Testing set size: {len(test_df)} samples")

# --- 4. Train and Evaluate a Random Forest Model for Each Target ---

# Dictionary to store trained models
trained_models = {}

for target in TARGET_COLUMNS:
    print(f"\n--- Training Random Forest for target: {target} ---")

    # Initialize the Random Forest Learner
    # YDF automatically detects feature types based on Pandas dtypes
    # For classification, set the task to ydf.Task.CLASSIFICATION
    # For multi-class (if your labels were > 2 unique values), it would detect automatically.
    # For binary, it's just a special case of classification.
    # num_trees is equivalent to n_estimators in scikit-learn
    # max_depth and min_examples are useful hyperparameters to tune
    rf_learner = ydf.RandomForestLearner(
        label=target,
        task=ydf.Task.CLASSIFICATION,
        num_trees=100, # Number of trees
        max_depth=10,  # Maximum depth of trees
        min_examples=5, # Minimum examples per leaf
        # random_seed=42 # For reproducibility
    )

    # Train the model
    # YDF's .train() method takes the DataFrame directly
    model = rf_learner.train(train_df)
    trained_models[target] = model

    print(f"Model for {target} trained successfully.")
    # model.describe() # Uncomment to see full model description and logs

    # --- 5. Evaluate the Model ---
    print(f"\n--- Evaluating Model for target: {target} ---")
    evaluation_results = model.evaluate(test_df)

    # YDF's evaluate() method returns an Evaluation object.
    # We need to extract predictions and true labels for scikit-learn metrics.
    true_labels = test_df[target].values
    predictions_proba = model.predict(test_df) # This gives raw predictions (e.g., logits or scores)

    # For classification, YDF's predict for binary often gives probabilities for the positive class (1).
    # If it gives a 2D array like [[prob_0, prob_1], ...], we take the prob of class 1.
    # If it's a 1D array of probabilities, it's already good.
    if predictions_proba.ndim > 1 and predictions_proba.shape[1] > 1:
        # Assuming binary classification, we want the probability of the positive class (index 1)
        # This might vary depending on how YDF presents its output; check the docs if results seem off.
        # Often, ydf.predict for classification returns the probability of the "positive" class.
        # If your labels are 0 and 1, it generally returns prob of 1.
        predicted_prob_positive_class = predictions_proba[:, 1]
    else:
        predicted_prob_positive_class = predictions_proba.flatten()


    predicted_classes = (predicted_prob_positive_class > 0.5).astype(int)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(true_labels, predicted_classes, zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(true_labels, predicted_classes)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No', 'Yes'],
                yticklabels=['No', 'Yes'])
    plt.title(f'Confusion Matrix - {target}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC Score
    auc_score = roc_auc_score(true_labels, predicted_prob_positive_class)
    print(f"ROC AUC Score for {target}: {auc_score:.4f}")

    # Plot ROC Curve
    fpr, tpr, _ = roc_curve(true_labels, predicted_prob_positive_class)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'{target} (AUC = {auc_score:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {target}')
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

    # Accuracy
    accuracy = accuracy_score(true_labels, predicted_classes)
    print(f"Accuracy for {target}: {accuracy:.4f}")

    # --- 6. Save the Model ---
    # YDF models can be saved directly to a directory
    # This will save it in YDF's native format (a directory structure)
    model_save_dir = f"/content/drive/My Drive/Capstone LaskarAI/ydf_model_{target}"
    model.save(model_save_dir)
    print(f"Model for {target} saved to: {model_save_dir}")

    # --- (Optional) Export as TensorFlow Saved Model ---
    # This converts the YDF model to a TF SavedModel format, allowing use with TF.Keras
    # tf_saved_model_dir = f"/content/drive/My Drive/Capstone LaskarAI/tf_saved_model_{target}"
    # model.to_tensorflow_saved_model(tf_saved_model_dir)
    # print(f"Model for {target} exported as TF SavedModel to: {tf_saved_model_dir}")


print("\n--- All models trained, evaluated, and saved ---")

# --- How to Load a YDF Model ---
# You can load them back using:
# loaded_ydf_model_heart_disease = ydf.load("/content/drive/My Drive/Capstone LaskarAI/ydf_model_HeartDisease")
# print(f"HeartDisease model loaded successfully from YDF native format.")

# If you saved to TensorFlow SavedModel:
# import tensorflow as tf
# loaded_tf_model_heart_disease = tf.keras.models.load_model("/content/drive/My Drive/Capstone LaskarAI/tf_saved_model_HeartDisease")
# print(f"HeartDisease model loaded successfully from TF SavedModel format.")

X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

X_train_tf.info()

# Kolom berdasarkan informasi gambar
binary_cols = [
    'Hypertension', 'HighChol', 'CholCheck', 'SmokingStatus',
    'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump',
    'AnyHealthcare', 'NoDocbcCost', 'DiffWalk', 'Gender'
]

onehot_cols = [
    'GenHlth', 'Education', 'Income'
]

# 1. Ubah kolom binary menjadi tipe integer
X_train_tf[binary_cols] = X_train_tf[binary_cols].astype(int)

# 2. One-hot encoding untuk kolom kategorikal dengan unique > 2
X_train_tf = pd.get_dummies(X_train_tf, columns=onehot_cols, drop_first=False)

X_train_tf.info()

y_train_tf.info()

X_train_tf.describe(include="all").T

# Identifikasi kolom numerik yang perlu di-scaling (misalnya, 'BMI', 'PhysicalHealth', 'MentalHealth', 'SleepTime')
# Jangan scaling kolom yang sudah kategorikal atau biner
numerical_features = ['BMI', 'PhysHlth', 'MentHlth', 'Age', 'Glucose'] # Contoh, sesuaikan dengan kolom X Anda

# Perhatikan: Nama kolom Anda mungkin sudah berubah karena proses SMOTE.
# Pastikan Anda menggunakan nama kolom yang benar dari X_train_tf.columns

scaler = StandardScaler()

# Terapkan scaling HANYA pada fitur numerik di data training dan testing
X_train_tf[numerical_features] = scaler.fit_transform(X_train_tf[numerical_features])
X_test_tf[numerical_features] = scaler.transform(X_test_tf[numerical_features])

# Jika ada fitur kategorikal yang belum di-one-hot-encode,
# Anda perlu melakukan One-Hot Encoding pada X_train_tf dan X_test_tf
# sebelum masuk ke Neural Network, karena NN biasanya membutuhkan input numerik.
# Namun, karena Anda sudah mengubah ke tipe 'category' di Pandas,
# Keras dapat menanganinya jika Anda menggunakan embedding layer,
# atau Anda bisa secara manual melakukan pd.get_dummies()
# Jika fitur kategorikal Anda sudah berupa 0/1 integer, itu sudah siap.

# Jumlah fitur input
input_shape = X_train_tf.shape[1]
# Jumlah label output (jumlah kolom di y)
output_labels = y_train_tf.shape[1]

model = keras.Sequential([
    # Layer input (Dense: setiap neuron terhubung ke setiap neuron di layer sebelumnya)
    layers.Dense(128, activation='relu', input_shape=(input_shape,)),
    layers.Dropout(0.3), # Dropout untuk mengurangi overfitting

    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),

    # Output layer untuk klasifikasi multi-label
    # Jumlah unit sama dengan jumlah kolom target Anda (misal 3 untuk HeartDisease, Stroke, Diabetes)
    # 'sigmoid' karena setiap label adalah klasifikasi biner independen (Ya/Tidak)
    layers.Dense(output_labels, activation='sigmoid')
])

# Tampilkan ringkasan model
model.summary()

# Optimizer: Adam adalah pilihan yang umum dan bagus
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# Loss function: BinaryCrossentropy karena setiap label adalah biner independen
loss_function = tf.keras.losses.BinaryCrossentropy()

# Metrics: Untuk memantau performa selama pelatihan
metrics = ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]

model.compile(optimizer=optimizer,
              loss=loss_function,
              metrics=metrics)

epochs = 50 # Jumlah iterasi pelatihan
batch_size = 32 # Ukuran batch data yang diproses per iterasi

history = model.fit(X_train_tf, y_train_tf,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_split=0.2, # Mengambil 20% dari data training untuk validasi
                    verbose=1) # Menampilkan progress pelatihan

# Simpan model format SavedModel
model.export("/content/drive/My Drive/tensorflow1")

# Simpan model format HDF5
model.save("/content/drive/My Drive/tensorflow1.h5")

# Simpan model format Keras
model.save("/content/drive/My Drive/tensorflow1.keras")

# Simpan model format TFLite
converter2 = tf.lite.TFLiteConverter.from_saved_model("/content/drive/My Drive/tensorflow1")
tflite_model2 = converter2.convert()
with open("/content/drive/My Drive/tensorflow1.tflite", "wb") as f:
    f.write(tflite_model2)

# Simpan History Model
with open("/content/drive/My Drive/tensorflow1.pkl", "wb") as f:
    pickle.dump(history.history, f)

# Simpan model format tensorflow.js
tfjs1_path = "/content/drive/My Drive/tensorflow1"
tfjs.converters.save_keras_model(model, tfjs1_path)
print("TensorFlow.js model saved successfully")

cnn_model1 = load_model("/content/drive/My Drive/Laskar AI/Submission/a001ybm349 - Submission 09 Part 2/saved_model/model_cnn.h5")

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

# --- Evaluasi Model pada Data Uji ---
print("\n=== Evaluasi Model pada Data Uji ===")
evaluation_results = model.evaluate(X_test_tf, y_test_tf, verbose=0)

# Hasil evaluasi akan menjadi list, urutan sesuai dengan compile metrics
print(f"Total Loss: {evaluation_results[0]:.4f}")
# Cetak metrik untuk setiap output
for i in range(len(targets)):
    print(f"  Target {i+1} Loss: {evaluation_results[i+1]:.4f}")
    print(f"  Target {i+1} Accuracy: {evaluation_results[i+1+len(targets)]:.4f}")

# --- Membuat Prediksi ---
predictions = model.predict(X_test_processed)

# --- Evaluasi Detail per Target Output ---
for i, target_col in enumerate(targets):
    print(f"\n--- Metrik Evaluasi untuk Target: {target_col} ---")

    # Ambil probabilitas prediksi untuk target ini
    pred_prob = predictions[i]
    # Konversi probabilitas ke kelas prediksi (indeks dengan probabilitas tertinggi)
    predicted_classes = np.argmax(pred_prob, axis=1)

    # Ambil kelas sebenarnya (dari one-hot encoded ke integer)
    true_classes = np.argmax(y_test_list[i], axis=1)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(true_classes, predicted_classes, zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(true_classes, predicted_classes)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=np.unique(true_classes),
                yticklabels=np.unique(true_classes))
    plt.title(f'Confusion Matrix - {target_col}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC (Jika target biner atau multiclass-one-vs-rest)
    # Ini akan bekerja jika jumlah kelas untuk target tersebut > 1
    if num_classes_per_target[i] > 1:
        try:
            # ROC AUC untuk multiclass (rata-rata per kelas)
            auc_score = roc_auc_score(y_test_list[i], pred_prob, multi_class='ovr', average='weighted')
            print(f"ROC AUC Score (Weighted - OvR): {auc_score:.4f}")

            # Plot ROC Curve (Contoh untuk beberapa kelas pertama jika multiclass)
            plt.figure(figsize=(8, 6))
            for j in range(num_classes_per_target[i]):
                fpr, tpr, _ = roc_curve(y_test_list[i][:, j], pred_prob[:, j])
                plt.plot(fpr, tpr, label=f'Class {j} (AUC = {roc_auc_score(y_test_list[i][:, j], pred_prob[:, j]):.2f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {target_col} (One-vs-Rest)')
            plt.legend(loc="lower right")
            plt.grid(True)
            plt.show()

        except ValueError as e:
            print(f"Tidak dapat menghitung ROC AUC untuk {target_col}: {e}")
    else:
        print(f"ROC AUC tidak berlaku untuk target biner '{target_col}' jika hanya ada satu kelas.")

"""# Penanganan Nilai Hilang, Encoding, dan Scaling Menggunakan Pipeline"""

# Penanganan Nilai Hilang, Encoding, dan Scaling Menggunakan Pipeline ---

# Identifikasi kolom kategorikal dan numerik
categorical_features = X.select_dtypes(include=['object', 'category']).columns
numerical_features = X.select_dtypes(include=np.number).columns

# Buat pipeline preprocessing
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# # Pemisahan Data Latih dan Uji ---
# X_train2, X_test2, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=42, stratify=y)

# print("\nBentuk X_train:", X_train2.shape)
# print("Bentuk X_test:", X_test2.shape)
# print("Bentuk y_train:", y_train.shape)
# print("Bentuk y_test:", y_test.shape)

# X_train2

# Pemisahan Data Latih dan Uji ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("\nBentuk X_train:", X_train.shape)
print("Bentuk X_test:", X_test.shape)
print("Bentuk y_train:", y_train.shape)
print("Bentuk y_test:", y_test.shape)

"""# Pembangunan Model Multi-output Classification"""

import numpy as np
from sklearn.datasets import make_multilabel_classification
from sklearn.multioutput import MultiOutputClassifier
from sklearn.linear_model import LogisticRegression

"""## RandomForest"""

X_train

from sklearn.multioutput import MultiOutputClassifier
from sklearn.ensemble import RandomForestClassifier
# Menggunakan RandomForestClassifier sebagai base estimator
# MultiOutputClassifier akan melatih satu classifier untuk setiap target
model = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42)))])

model2 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42)))])

# Atau menggunakan GradientBoostingClassifier
# model = Pipeline(steps=[('preprocessor', preprocessor),
#                         ('classifier', MultiOutputClassifier(GradientBoostingClassifier(n_estimators=100, random_state=42)))])

print("\nMemulai pelatihan model...")
model.fit(X_train, y_train)
print("Pelatihan model selesai.")

import pickle
import tensorflowjs as tfjs

# Simpan model format SavedModel
model.export("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf1")

# Simpan model format HDF5
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf1.h5")

# Simpan model format Keras
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf1.keras")

# Simpan model format TFLite
converter1 = tf.lite.TFLiteConverter.from_saved_model("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf1")
tflite_model1 = converter1.convert()
with open("/content/drive/My Drive/Capstone LaskarAI/rf1.tflite", "wb") as f:
    f.write(tflite_model1)

# Simpan History Model
# with open("/content/drive/My Drive/Capstone LaskarAI/rf1.pkl", "wb") as f:
#    pickle.dump(history.history, f)

# Simpan model format tensorflow.js
tfjs1_path = "/content/drive/My Drive/Capstone LaskarAI/tfjs_model/rf1"
tfjs.converters.save_keras_model(model, tfjs1_path)
print("TensorFlow.js model saved successfully")

import pickle
import tensorflowjs as tfjs

# Simpan model format SavedModel
# model.export("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf2")

# Simpan model format HDF5
# model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf2.h5")

# Simpan model format Keras
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf2.keras")

# Simpan model format TFLite
converter1 = tf.lite.TFLiteConverter.from_saved_model("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf2")
tflite_model1 = converter1.convert()
with open("/content/drive/My Drive/Capstone LaskarAI/rf2.tflite", "wb") as f:
    f.write(tflite_model1)

# Simpan History Model
# with open("/content/drive/My Drive/Capstone LaskarAI/rf1.pkl", "wb") as f:
#    pickle.dump(history.history, f)

# Simpan model format tensorflow.js
tfjs1_path = "/content/drive/My Drive/Capstone LaskarAI/tfjs_model/rf2"
tfjs.converters.save_keras_model(model, tfjs1_path)
print("TensorFlow.js model saved successfully")

# Simpan Model
with open("/content/drive/My Drive/Capstone LaskarAI/rf2.pkl", "wb") as f:
    pickle.dump(model, f)

print("\nMemulai pelatihan model...")
model2.fit(X_train2, y_train)
print("Pelatihan model selesai.")

y_pred2 = model2.predict(X_test2)
y_pred_proba2 = model2.predict_proba(X_test2) # Untuk ROC AUC

# Mengubah y_pred_proba menjadi format yang dapat digunakan oleh roc_auc_score
# predict_proba mengembalikan daftar array untuk setiap target
# Misalnya: [array([[0.9, 0.1], ...]), array([[0.8, 0.2], ...]), ...]
# Kita perlu mengambil probabilitas kelas positif (indeks 1) untuk setiap target
y_pred_proba_formatted2 = np.array([pred[:, 1] for pred in y_pred_proba2]).T

"""### Evaluasi Model"""

# Lokasi file model yang disimpan
file_path = "/content/drive/My Drive/Capstone LaskarAI/rf2.pkl"

# Memuat model dari file
with open(file_path, "rb") as f:
    model = pickle.load(f)

y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test) # Untuk ROC AUC

# Mengubah y_pred_proba menjadi format yang dapat digunakan oleh roc_auc_score
# predict_proba mengembalikan daftar array untuk setiap target
# Misalnya: [array([[0.9, 0.1], ...]), array([[0.8, 0.2], ...]), ...]
# Kita perlu mengambil probabilitas kelas positif (indeks 1) untuk setiap target
y_pred_proba_formatted = np.array([pred[:, 1] for pred in y_pred_proba]).T

print("\n--- Evaluasi Model ---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred[:, i])}")
    try:
        # ROC AUC hanya untuk kelas biner
        if len(y_test.iloc[:, i].unique()) == 2:
            roc_auc = roc_auc_score(y_test.iloc[:, i], y_pred_proba_formatted[:, i])
            print(f"ROC AUC: {roc_auc:.4f}")
        else:
            print("ROC AUC tidak berlaku untuk target non-biner.")
    except ValueError as e:
        print(f"Tidak dapat menghitung ROC AUC untuk {target}: {e}")

# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro = np.mean([f1_score(y_test.iloc[:, i], y_pred[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro:.4f}")

print("\n--- Evaluasi Model RF 2---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred2[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred2[:, i])}")
    try:
        # ROC AUC hanya untuk kelas biner
        if len(y_test.iloc[:, i].unique()) == 2:
            roc_auc = roc_auc_score(y_test.iloc[:, i], y_pred_proba_formatted2[:, i])
            print(f"ROC AUC: {roc_auc:.4f}")
        else:
            print("ROC AUC tidak berlaku untuk target non-biner.")
    except ValueError as e:
        print(f"Tidak dapat menghitung ROC AUC untuk {target}: {e}")

# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro2 = np.mean([f1_score(y_test.iloc[:, i], y_pred2[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro2:.4f}")

"""### Inferences"""

# --- 10. Prediksi untuk Data Baru (Contoh) ---
print("\n--- Contoh Prediksi Data Baru ---")

# Buat contoh data baru dengan struktur kolom yang sama seperti X_train
# Nilai-nilai ini HANYA CONTOH. Anda harus menggantinya dengan data nyata.
new_data = pd.DataFrame([{
    'age': 55,
    'sex': 'Female',
    'bmi': 28.5,
    'smoking_history': 'never smoked',
    'hypertension': 0,
    'heart_disease': 0, # Ini akan diabaikan sebagai fitur, tetapi penting untuk struktur dataframe
    'stroke': 0,       # Ini akan diabaikan sebagai fitur
    'diabetes': 0,     # Ini akan diabaikan sebagai fitur
    'cholesterol': 200,
    'blood_glucose_level': 100,
    'physactivity': 1,
    'fruits': 1,
    'veggies': 1,
    'hvyalcoholconsump': 0,
    'anyhealthcare': 1,
    'nodocbccare': 0,
    'genhlth': 3, # Skala 1-5 (1=Excellent, 5=Poor)
    'menthlth': 0, # Jumlah hari mental health buruk
    'physhlth': 0, # Jumlah hari fisik health buruk
    'diffwalk': 0, # Sulit jalan
    'avg_glucose_level': 100,
    'cholesterol_high': 0 # Contoh fitur lain yang mungkin ada dari dataset lain
}], columns=features_columns) # Pastikan kolom sesuai dengan X_train

# Lakukan prediksi
risk_predictions = model.predict(new_data)
risk_probabilities = model.predict_proba(new_data)

print("\nPrediksi Risiko:")
for i, target in enumerate(target_columns):
    print(f"Risiko {target}: {'Tinggi' if risk_predictions[0, i] == 1 else 'Rendah'} "
          f"(Probabilitas: {risk_probabilities[i][0, 1]:.4f})") # Probabilitas kelas positif

"""## RegLog"""

from sklearn.linear_model import LogisticRegression
clf = MultiOutputClassifier(LogisticRegression()).fit(X_train2, y_train)
clf.predict(X_test2)

model3 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', MultiOutputClassifier(LogisticRegression()))])
model3.fit(X_train2, y_train)

y_pred3 = model3.predict(X_test2)
y_pred_proba3 = model3.predict_proba(X_test2) # Untuk ROC AUC

# Mengubah y_pred_proba menjadi format yang dapat digunakan oleh roc_auc_score
# predict_proba mengembalikan daftar array untuk setiap target
# Misalnya: [array([[0.9, 0.1], ...]), array([[0.8, 0.2], ...]), ...]
# Kita perlu mengambil probabilitas kelas positif (indeks 1) untuk setiap target
y_pred_proba_formatted3 = np.array([pred[:, 1] for pred in y_pred_proba3]).T

y_pred3 = clf.predict(X_test2)

print("\n--- Evaluasi Model ---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred3[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred3[:, i])}")
    try:
        # ROC AUC hanya untuk kelas biner
        if len(y_test.iloc[:, i].unique()) == 2:
            roc_auc = roc_auc_score(y_test.iloc[:, i], y_pred_proba_formatted3[:, i])
            print(f"ROC AUC: {roc_auc:.4f}")
        else:
            print("ROC AUC tidak berlaku untuk target non-biner.")
    except ValueError as e:
        print(f"Tidak dapat menghitung ROC AUC untuk {target}: {e}")

# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro3 = np.mean([f1_score(y_test.iloc[:, i], y_pred3[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro2:.4f}")

print("\n--- Evaluasi Model RegLog 1---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred3[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred3[:, i])}")


# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro = np.mean([f1_score(y_test.iloc[:, i], y_pred3[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro:.4f}")

"""## Xtra Tree"""



from sklearn.tree import ExtraTreeClassifier
# Menggunakan RandomForestClassifier sebagai base estimator
# MultiOutputClassifier akan melatih satu classifier untuk setiap target
model4 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', MultiOutputClassifier(ExtraTreeClassifier()))])
model4.fit(X_train2, y_train)

y_pred4 = model4.predict(X_test2)
y_pred_proba4 = model4.predict_proba(X_test2) # Untuk ROC AUC

# Mengubah y_pred_proba menjadi format yang dapat digunakan oleh roc_auc_score
# predict_proba mengembalikan daftar array untuk setiap target
# Misalnya: [array([[0.9, 0.1], ...]), array([[0.8, 0.2], ...]), ...]
# Kita perlu mengambil probabilitas kelas positif (indeks 1) untuk setiap target
y_pred_proba_formatted4 = np.array([pred[:, 1] for pred in y_pred_proba4]).T

print("\n--- Evaluasi Model ---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred4[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred4[:, i])}")
    try:
        # ROC AUC hanya untuk kelas biner
        if len(y_test.iloc[:, i].unique()) == 2:
            roc_auc = roc_auc_score(y_test.iloc[:, i], y_pred_proba_formatted4[:, i])
            print(f"ROC AUC: {roc_auc:.4f}")
        else:
            print("ROC AUC tidak berlaku untuk target non-biner.")
    except ValueError as e:
        print(f"Tidak dapat menghitung ROC AUC untuk {target}: {e}")

# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro4 = np.mean([f1_score(y_test.iloc[:, i], y_pred4[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro4:.4f}")

"""## MLPClassifier"""

from sklearn.neural_network import MLPClassifier
# Menggunakan RandomForestClassifier sebagai base estimator
# MultiOutputClassifier akan melatih satu classifier untuk setiap target
model5 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', MultiOutputClassifier(MLPClassifier()))])
model5.fit(X_train2, y_train)

y_pred5 = model5.predict(X_test2)
y_pred_proba5 = model5.predict_proba(X_test2) # Untuk ROC AUC

# Mengubah y_pred_proba menjadi format yang dapat digunakan oleh roc_auc_score
# predict_proba mengembalikan daftar array untuk setiap target
# Misalnya: [array([[0.9, 0.1], ...]), array([[0.8, 0.2], ...]), ...]
# Kita perlu mengambil probabilitas kelas positif (indeks 1) untuk setiap target
y_pred_proba_formatted5 = np.array([pred[:, 1] for pred in y_pred_proba5]).T

print("\n--- Evaluasi Model ---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred5[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred5[:, i])}")
    try:
        # ROC AUC hanya untuk kelas biner
        if len(y_test.iloc[:, i].unique()) == 2:
            roc_auc = roc_auc_score(y_test.iloc[:, i], y_pred_proba_formatted5[:, i])
            print(f"ROC AUC: {roc_auc:.4f}")
        else:
            print("ROC AUC tidak berlaku untuk target non-biner.")
    except ValueError as e:
        print(f"Tidak dapat menghitung ROC AUC untuk {target}: {e}")

# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro5 = np.mean([f1_score(y_test.iloc[:, i], y_pred5[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro5:.4f}")

"""## Lazy"""

!pip install lazypredict

from lazypredict.Supervised import LazyClassifier
clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)
models,predictions = clf.fit(X_train2, X_test2, y_train, y_test)
models

"""# Pembangunan Model Multi-output Neural Network"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# --- 2. Preprocessing Data ---
# Identifikasi kolom numerik dan kategori
categorical_cols = X.select_dtypes(include=['object', 'bool']).columns.tolist()
numerical_cols = X.select_dtypes(include=np.number).columns.tolist()


# --- Buat Preprocessing Pipeline ---
# Pipeline untuk fitur numerik: Imputasi Mean -> Scaling
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Pipeline untuk fitur kategorikal: Imputasi Most Frequent -> One-Hot Encoding
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Gabungkan transformer menggunakan ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# --- One-Hot Encode Target Variables secara terpisah ---
y_encoded_list = []
num_classes_per_target = []
target_encoders = {}
for target_col in targets:
    num_classes = y[target_col].nunique()
    print(f"Target '{target_col}': {num_classes} unique classes.")
    if num_classes < 2:
        # Jika target biner (0 atau 1) tetapi hanya ada 1 nilai unik di data,
        # paksa OneHotEncoder untuk menghasilkan 2 kolom (untuk 0 dan 1).
        # Ini hanya jika Anda yakin target memang biner.
        encoder = OneHotEncoder(sparse_output=False, categories=[[0, 1]])
    else:
        encoder = OneHotEncoder(sparse_output=False, categories='auto')

    y_encoded_list.append(encoder.fit_transform(y[[target_col]]))
    num_classes_per_target.append(encoder.categories_[0].shape[0]) # Ambil jumlah kelas yang dihasilkan encoder
    target_encoders[target_col] = encoder

print(f"Jumlah kelas per target yang dihasilkan oleh encoder: {num_classes_per_target}")


# --- Split Data (Training dan Testing) ---
# Gunakan operator * untuk membongkar list y_encoded_list menjadi argumen terpisah
# train_test_split akan mengembalikan tuple panjang, jadi sesuaikan penerimaannya
X_train_raw, X_test_raw, *y_splits = train_test_split(
    X, *y_encoded_list, test_size=0.2, random_state=42
)

# y_splits akan berisi list yang alternating antara train dan test untuk setiap target
# Contoh: [y1_train, y1_test, y2_train, y2_test, y3_train, y3_test]
# Kita perlu mengorganisirnya kembali

y_train_list = []
y_test_list = []
for i in range(len(targets)):
    y_train_list.append(y_splits[i*2])     # 0, 2, 4
    y_test_list.append(y_splits[i*2 + 1]) # 1, 3, 5

# --- Terapkan preprocessing pada X ---
X_train_processed = preprocessor.fit_transform(X_train_raw)
X_test_processed = preprocessor.transform(X_test_raw)

print(f"Shape X_train_processed: {X_train_processed.shape}")
print(f"Shape X_test_processed: {X_test_processed.shape}")

print("\nShape of y_train_list elements (after split):")
for i, y_arr in enumerate(y_train_list):
    print(f"  y_train_list[{i}] (for {targets[i]}): {y_arr.shape}")

print("\nShape of y_test_list elements (after split):")
for i, y_arr in enumerate(y_test_list):
    print(f"  y_test_list[{i}] (for {targets[i]}): {y_arr.shape}")

print(f"Shape X_train_processed: {X_train_processed.shape}")
print(f"Shape X_test_processed: {X_test_processed.shape}")
print(f"Jumlah kelas per target: {num_classes_per_target}")

"""## MNN 1"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy

X.info()

X.select_dtypes(include=['category', 'object']).columns

X.select_dtypes(include=np.number).columns

targets = y.columns
print(targets)

# --- 2. Preprocessing Data ---
# Identifikasi kolom numerik dan kategori
categorical_cols = X.select_dtypes(include=['category', 'object']).columns
numerical_cols = X.select_dtypes(include=np.number).columns

# Buat preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])

# Split data X dan y terlebih dahulu
X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Terapkan preprocessing pada X
X_train_processed = preprocessor.fit_transform(X_train_raw)
X_test_processed = preprocessor.transform(X_test_raw)

# One-hot encode target variables separately for train and test sets
y_train_list = []
y_test_list = []
num_classes_per_target = []

for target_col in targets:
    # Ambil nilai unik dari target untuk menentukan jumlah kelas
    num_classes = y_train_raw[target_col].nunique() # Use train data to determine classes
    num_classes_per_target.append(num_classes)
    # Lakukan one-hot encoding
    # encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # Add handle_unknown for consistency
    y_train_list.append(y_train_raw[[target_col]])
    y_test_list.append(y_test_raw[[target_col]]) # Use transform on test data

# Now y_train_list and y_test_list are lists of arrays with consistent sample numbers

X_train_raw

X_test_processed.shape

X_train_processed

y_train_raw

y_train_list

# --- 3. Membangun Model Multiclass/Multioutput dengan TensorFlow ---

# Definisikan input layer
input_shape = X_train_processed.shape[1]
input_layer = Input(shape=(input_shape,), name='input_features')

# Hidden layers
x = Dense(128, activation='relu')(input_layer)
x = Dense(64, activation='relu')(x)

# Output layers for each target variable
# Penting: Pastikan jumlah unit di setiap output layer sesuai dengan jumlah kelas untuk target tersebut
# dan gunakan activation='softmax' untuk klasifikasi multiclass.
output_layers = []
for i, num_classes in enumerate(num_classes_per_target):
    output_layer = Dense(num_classes, activation='softmax', name=f'output_target_{i+1}')(x)
    output_layers.append(output_layer)

# Buat model
model = Model(inputs=input_layer, outputs=output_layers)

# --- 4. Compile Model ---

# Definisikan loss function dan metrics untuk setiap output
# CategoricalCrossentropy cocok untuk one-hot encoded targets
# CategoricalAccuracy juga cocok
losses = {}
metrics = {}
for i, _ in enumerate(targets):
    losses[f'output_target_{i+1}'] = CategoricalCrossentropy()
    metrics[f'output_target_{i+1}'] = CategoricalAccuracy(name=f'accuracy_target_{i+1}')

model.compile(optimizer=Adam(learning_rate=0.001),
              loss=losses,
              metrics=metrics)

model.summary()

# --- 5. Latih Model ---

# Format y_train_list dan y_test_list untuk feed ke model
# y_train_list adalah list of arrays, kita perlu mengubahnya menjadi dictionary
# dengan nama output layer sebagai kunci
y_train_dict = {f'output_target_{i+1}': y_train_list[i] for i in range(len(y_train_list))}
y_test_dict = {f'output_target_{i+1}': y_test_list[i] for i in range(len(y_test_list))}


history = model.fit(X_train_processed, y_train_dict,
                    epochs=50,  # Sesuaikan jumlah epoch
                    batch_size=32,
                    validation_data=(X_test_processed, y_test_dict))

import pickle
import tensorflowjs as tfjs

import pickle
import tensorflowjs as tfjs

# Simpan model format SavedModel
model.export("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn1")

# Simpan model format HDF5
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn1.h5")

# Simpan model format Keras
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn1.keras")

# Simpan model format TFLite
converter1 = tf.lite.TFLiteConverter.from_saved_model("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn1")
tflite_model1 = converter1.convert()
with open("/content/drive/My Drive/Capstone LaskarAI/mnn1.tflite", "wb") as f:
    f.write(tflite_model1)

# Simpan History Model
with open("/content/drive/My Drive/Capstone LaskarAI/mnn1.pkl", "wb") as f:
    pickle.dump(history.history, f)

# Simpan model format tensorflow.js
tfjs1_path = "/content/drive/My Drive/Capstone LaskarAI/tfjs_model/mnn1"
tfjs.converters.save_keras_model(model, tfjs1_path)
print("TensorFlow.js model saved successfully")

import os

# Buat direktori untuk menyimpan model jika belum ada
model_save_dir = "/content/drive/My Drive/Capstone LaskarAI/saved_model"
os.makedirs(model_save_dir, exist_ok=True)

# Simpan model dalam format SavedModel (rekomendasi TensorFlow)
model_path = os.path.join(model_save_dir, 'multioutput_classification_model1')
model.save(model_path)
print(f"\nModel berhasil disimpan di: {model_path}")

# Jika Anda ingin memuat kembali model:
# loaded_model = tf.keras.models.load_model(model_path)
# print("Model berhasil dimuat kembali.")

# --- 6. Evaluasi Model ---
print("\nEvaluasi Model pada Data Uji:")
evaluation_results = model.evaluate(X_test_processed, y_test_dict, verbose=0)

# Hasil evaluasi akan menjadi list, urutan sesuai dengan compile metrics
print(f"Total Loss: {evaluation_results[0]:.4f}")
# Cetak metrik untuk setiap output
for i in range(len(targets)):
    print(f"  Target {i+1} Loss: {evaluation_results[i+1]:.4f}")
    print(f"  Target {i+1} Accuracy: {evaluation_results[i+1+len(targets)]:.4f}") # Perhatikan indeksnya

# --- 7. Membuat Prediksi ---
print("\nMembuat Prediksi:")
predictions = model.predict(X_test_processed[:5]) # Prediksi untuk 5 sampel pertama

# Prediksi akan berupa list of arrays, satu array untuk setiap output
for i, pred_output in enumerate(predictions):
    print(f"Prediksi untuk Target {i+1} (5 sampel pertama):")
    # Untuk klasifikasi, Anda mungkin ingin mendapatkan kelas prediksi (indeks dengan probabilitas tertinggi)
    predicted_classes = np.argmax(pred_output, axis=1)
    print(predicted_classes)
    print("-" * 30)

# Plotting history (opsional)
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluasi model pada test set
y_train_dict = {
    'heart_disease': y_train['HeartDisease'].values,
    'stroke': y_train['Stroke'].values,
    'diabetes': y_train['Diabetes'].values
}
y_test_dict = {
    'heart_disease': y_test['HeartDisease'].values,
    'stroke': y_test['Stroke'].values,
    'diabetes': y_test['Diabetes'].values
}

# Convert X_test to a NumPy array
X_test_array = X_test.values

# Evaluate the model using a list of target arrays and the NumPy array of features
# The order of target arrays should match the order of outputs in the model: Diabetes, HeartDisease, Stroke
loss, diabetes_loss, heart_loss, stroke_loss, diabetes_acc, heart_acc, stroke_acc = model.evaluate(
    X_test_array,
    [y_test_dict['diabetes'], y_test_dict['heart_disease'], y_test_dict['stroke']],
    verbose=0
)

print(f"\nTotal Test Loss: {loss:.4f}")
print("\n--- Evaluasi untuk Heart Disease ---")
print(f"Loss: {heart_loss:.4f}")
print(f"Accuracy: {heart_acc:.4f}")


print("\n--- Evaluasi untuk Stroke ---")
print(f"Loss: {stroke_loss:.4f}")
print(f"Accuracy: {stroke_acc:.4f}")


print("\n--- Evaluasi untuk Diabetes ---")
print(f"Loss: {diabetes_loss:.4f}")
print(f"Accuracy: {diabetes_acc:.4f}")

# ROC AUC can be calculated separately if needed
# For calculating AUC, you would typically predict probabilities and then use sklearn's roc_auc_score
# y_pred_prob = model.predict(X_test_array) # Use the NumPy array for prediction as well
# heart_auc = roc_auc_score(y_test['HeartDisease'].values, y_pred_prob[1]) # Index 1 for Heart Disease output
# stroke_auc = roc_auc_score(y_test['Stroke'].values, y_pred_prob[2]) # Index 2 for Stroke output
# diabetes_auc = roc_auc_score(y_test['Diabetes'].values, y_pred_prob[0]) # Index 0 for Diabetes output

# print(f"Heart Disease AUC: {heart_auc:.4f}")
# print(f"Stroke AUC: {stroke_auc:.4f}")
# print(f"Diabetes AUC: {diabetes_auc:.4f}")

"""## MNN 2"""

targets = y.columns
print(targets)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# --- Identifikasi kolom numerik dan kategorikal ---
numerical_cols = X.select_dtypes(include=np.number).columns.tolist()
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()

# --- Buat Preprocessing Pipeline ---
# Pipeline untuk fitur numerik: Imputasi Mean -> Scaling
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Pipeline untuk fitur kategorikal: Imputasi Most Frequent -> One-Hot Encoding
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Gabungkan transformer menggunakan ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# --- One-Hot Encode Target Variables secara terpisah ---
y_encoded_list = []
num_classes_per_target = []
target_encoders = {} # Untuk menyimpan encoder jika perlu invers transformasi
for target_col in targets:
    num_classes = y[target_col].nunique()
    num_classes_per_target.append(num_classes)
    # encoder = OneHotEncoder(sparse_output=False, categories='auto')
    y_encoded_list.append(y[[target_col]])
    # target_encoders[target_col] = encoder # Simpan encoder

# --- Split Data (Training dan Testing) ---
# X_train_raw, X_test_raw digunakan untuk memastikan ColumnTransformer tidak terfit pada data test
X_train_raw, X_test_raw, y_train_list, y_test_list = train_test_split(
    X, y.to_numpy(), test_size=0.2, random_state=42
)

# Terapkan preprocessing pada X
X_train_processed = preprocessor.fit_transform(X_train_raw)
X_test_processed = preprocessor.transform(X_test_raw)

print(f"Shape X_train_processed: {X_train_processed.shape}")
print(f"Shape X_test_processed: {X_test_processed.shape}")
print(f"Jumlah kelas per target: {num_classes_per_target}")

y.to_numpy()

y_encoded_list

X_train_processed.shape

y_train_dict.shape

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.callbacks import EarlyStopping

# --- Definisikan Input Layer ---
input_shape = X_train_processed.shape[1]
input_layer = Input(shape=(input_shape,), name='input_features')

# --- Hidden Layers dengan Dropout ---
x = Dense(256, activation='relu')(input_layer)
x = Dropout(0.3)(x) # Dropout dengan probabilitas 30%
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(64, activation='relu')(x)

# --- Output Layers untuk Setiap Target ---
output_layers = []
for i, num_classes in enumerate(num_classes_per_target):
    output_layer = Dense(num_classes, activation='softmax', name=f'output_target_{i+1}')(x)
    output_layers.append(output_layer)

# --- Buat Model ---
model = Model(inputs=input_layer, outputs=output_layers)

# --- Compile Model ---
losses = {f'output_target_{i+1}': CategoricalCrossentropy() for i in range(len(targets))}
metrics = {f'output_target_{i+1}': CategoricalAccuracy(name=f'accuracy_target_{i+1}') for i in range(len(targets))}

model.compile(optimizer=Adam(learning_rate=0.001),
              loss=losses,
              metrics=metrics)

model.summary()

# --- Callback Early Stopping ---
# Monitor 'val_loss' (loss pada data validasi)
# patience=10 berarti akan berhenti jika val_loss tidak membaik selama 10 epoch berturut-turut
# restore_best_weights=True akan mengembalikan bobot model dari epoch terbaik
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

# --- Format y_train_list dan y_test_list untuk feed ke model ---
# Access elements of y_train_list and y_test_list using their index i
y_train_dict = {f'output_target_{i+1}': y_train_list[i] for i in range(len(y_train_list))}
y_test_dict = {f'output_target_{i+1}': y_test_list[i] for i in range(len(y_test_list))}


# --- Latih Model ---
history = model.fit(X_train_processed, y_train_dict,
                    epochs=100, # Set epoch lebih tinggi karena Early Stopping akan menghentikan secara otomatis
                    batch_size=32,
                    validation_data=(X_test_processed, y_test_dict),
                    callbacks=[early_stopping], # Tambahkan callback EarlyStopping
                    verbose=1)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.callbacks import EarlyStopping

# --- Definisikan Input Layer ---
input_shape = X_train_processed.shape[1]
input_layer = Input(shape=(input_shape,), name='input_features')

# --- Hidden Layers dengan Dropout ---
x = Dense(256, activation='relu')(input_layer)
x = Dropout(0.3)(x) # Dropout dengan probabilitas 30%
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(64, activation='relu')(x)

# --- Output Layers untuk Setiap Target ---
output_layers = []
for i, num_classes in enumerate(num_classes_per_target):
    output_layer = Dense(num_classes, activation='softmax', name=f'output_target_{i+1}')(x)
    output_layers.append(output_layer)

# --- Buat Model ---
model = Model(inputs=input_layer, outputs=output_layers)

# --- Compile Model ---
losses = {f'output_target_{i+1}': CategoricalCrossentropy() for i in range(len(targets))}
metrics = {f'output_target_{i+1}': CategoricalAccuracy(name=f'accuracy_target_{i+1}') for i in range(len(targets))}

model.compile(optimizer=Adam(learning_rate=0.001),
              loss=losses,
              metrics=metrics)

model.summary()

# --- Callback Early Stopping ---
# Monitor 'val_loss' (loss pada data validasi)
# patience=10 berarti akan berhenti jika val_loss tidak membaik selama 10 epoch berturut-turut
# restore_best_weights=True akan mengembalikan bobot model dari epoch terbaik
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

# --- Format y_train_list dan y_test_list untuk feed ke model ---
# Access elements of y_train_list and y_test_list using their index i
y_train_dict = {f'output_target_{i+1}': y_train_list[i] for i in range(len(y_train_list))}
y_test_dict = {f'output_target_{i+1}': y_test_list[i] for i in range(len(y_test_list))}


# --- Latih Model ---
history = model.fit(X_train_processed, y_train_dict,
                    epochs=100, # Set epoch lebih tinggi karena Early Stopping akan menghentikan secara otomatis
                    batch_size=32,
                    validation_data=(X_test_processed, y_test_dict),
                    callbacks=[early_stopping], # Tambahkan callback EarlyStopping
                    verbose=1)

y_train_dict

# Simpan model format SavedModel
model.export("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn2")

# Simpan model format HDF5
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn2.h5")

# Simpan model format Keras
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn2.keras")

# Simpan model format TFLite
converter2 = tf.lite.TFLiteConverter.from_saved_model("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn2")
tflite_model2 = converter2.convert()
with open("/content/drive/My Drive/Capstone LaskarAI/mnn2.tflite", "wb") as f:
    f.write(tflite_model2)

# Simpan History Model
with open("/content/drive/My Drive/Capstone LaskarAI/mnn2.pkl", "wb") as f:
    pickle.dump(history.history, f)

# Simpan model format tensorflow.js
tfjs1_path = "/content/drive/My Drive/Capstone LaskarAI/tfjs_model/mnn2"
tfjs.converters.save_keras_model(model, tfjs1_path)
print("TensorFlow.js model saved successfully")

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

# --- Evaluasi Model pada Data Uji ---
print("\n=== Evaluasi Model pada Data Uji ===")
evaluation_results = model.evaluate(X_test_processed, y_test_dict, verbose=0)

# Hasil evaluasi akan menjadi list, urutan sesuai dengan compile metrics
print(f"Total Loss: {evaluation_results[0]:.4f}")
# Cetak metrik untuk setiap output
for i in range(len(targets)):
    print(f"  Target {i+1} Loss: {evaluation_results[i+1]:.4f}")
    print(f"  Target {i+1} Accuracy: {evaluation_results[i+1+len(targets)]:.4f}")

# --- Membuat Prediksi ---
predictions = model.predict(X_test_processed)

# --- Evaluasi Detail per Target Output ---
for i, target_col in enumerate(targets):
    print(f"\n--- Metrik Evaluasi untuk Target: {target_col} ---")

    # Ambil probabilitas prediksi untuk target ini
    pred_prob = predictions[i]
    # Konversi probabilitas ke kelas prediksi (indeks dengan probabilitas tertinggi)
    predicted_classes = np.argmax(pred_prob, axis=1)

    # Ambil kelas sebenarnya (dari one-hot encoded ke integer)
    true_classes = np.argmax(y_test_list[i], axis=1)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(true_classes, predicted_classes, zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(true_classes, predicted_classes)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=np.unique(true_classes),
                yticklabels=np.unique(true_classes))
    plt.title(f'Confusion Matrix - {target_col}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC (Jika target biner atau multiclass-one-vs-rest)
    # Ini akan bekerja jika jumlah kelas untuk target tersebut > 1
    if num_classes_per_target[i] > 1:
        try:
            # ROC AUC untuk multiclass (rata-rata per kelas)
            auc_score = roc_auc_score(y_test_list[i], pred_prob, multi_class='ovr', average='weighted')
            print(f"ROC AUC Score (Weighted - OvR): {auc_score:.4f}")

            # Plot ROC Curve (Contoh untuk beberapa kelas pertama jika multiclass)
            plt.figure(figsize=(8, 6))
            for j in range(num_classes_per_target[i]):
                fpr, tpr, _ = roc_curve(y_test_list[i][:, j], pred_prob[:, j])
                plt.plot(fpr, tpr, label=f'Class {j} (AUC = {roc_auc_score(y_test_list[i][:, j], pred_prob[:, j]):.2f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {target_col} (One-vs-Rest)')
            plt.legend(loc="lower right")
            plt.grid(True)
            plt.show()

        except ValueError as e:
            print(f"Tidak dapat menghitung ROC AUC untuk {target_col}: {e}")
    else:
        print(f"ROC AUC tidak berlaku untuk target biner '{target_col}' jika hanya ada satu kelas.")

"""## MNN 4"""

targets = y.columns
print(targets)

X.info()

# --- 2. Preprocessing Data ---

targets = y.columns
print(targets)

X_prep = X.copy()

categorical_cols = X_prep.select_dtypes(include=['object', 'category']).columns
numerical_cols = X_prep.select_dtypes(include=np.number).columns

# Buat preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])

# Split data X dan y terlebih dahulu
X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
    X_prep, y, test_size=0.2, random_state=42
)

# Terapkan preprocessing pada X
X_train_processed = preprocessor.fit_transform(X_train_raw)
X_test_processed = preprocessor.transform(X_test_raw)

# One-hot encode target variables separately for train and test sets
y_train_list = []
y_test_list = []
num_classes_per_target = []

for target_col in targets:
    # Ambil nilai unik dari target untuk menentukan jumlah kelas
    num_classes = y_train_raw[target_col].nunique() # Use train data to determine classes
    num_classes_per_target.append(num_classes)
    # Lakukan one-hot encoding
    # encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # Add handle_unknown for consistency
    y_train_list.append(y_train_raw[[target_col]])
    y_test_list.append(y_test_raw[[target_col]]) # Use transform on test data

# Now y_train_list and y_test_list are lists of arrays with consistent sample numbers

X_train_processed.shape

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import BinaryAccuracy
from tensorflow.keras.callbacks import EarlyStopping

# --- Definisikan Input Layer ---
input_shape = X_train_processed.shape[1]
input_layer = Input(shape=(input_shape,), name='input_features')

# --- Hidden Layers dengan Dropout ---
x = Dense(256, activation='relu')(input_layer)
x = Dropout(0.3)(x) # Dropout dengan probabilitas 30%
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(64, activation='relu')(x)

# --- Output Layers untuk Setiap Target ---
output_layers = []
for i, num_classes in enumerate(num_classes_per_target):
    output_layer = Dense(1, activation='sigmoid', name=f'output_target_{i+1}')(x)
    output_layers.append(output_layer)

# --- Buat Model ---
model = Model(inputs=input_layer, outputs=output_layers)

# --- Compile Model ---
losses = {f'output_target_{i+1}': BinaryCrossentropy() for i in range(len(targets))}
metrics = {f'output_target_{i+1}': BinaryAccuracy(name=f'accuracy_target_{i+1}') for i in range(len(targets))}

model.compile(optimizer=Adam(learning_rate=0.001),
              loss=losses,
              metrics=metrics)

model.summary()

# --- Callback Early Stopping ---
# Monitor 'val_loss' (loss pada data validasi)
# patience=10 berarti akan berhenti jika val_loss tidak membaik selama 10 epoch berturut-turut
# restore_best_weights=True akan mengembalikan bobot model dari epoch terbaik
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

# --- Format y_train_list dan y_test_list untuk feed ke model ---
# Access elements of y_train_list and y_test_list using their index i
y_train_dict = {f'output_target_{i+1}': y_train_list[i] for i in range(len(y_train_list))}
y_test_dict = {f'output_target_{i+1}': y_test_list[i] for i in range(len(y_test_list))}


# --- Latih Model ---
history = model.fit(X_train_processed, y_train_dict,
                    epochs=100, # Set epoch lebih tinggi karena Early Stopping akan menghentikan secara otomatis
                    batch_size=32,
                    validation_data=(X_test_processed, y_test_dict),
                    callbacks=[early_stopping], # Tambahkan callback EarlyStopping
                    verbose=1)

y_train_dict

import pickle

# Menyimpan history ke file
with open('history.pkl', 'wb') as f:
    pickle.dump(history, f)

# Memuat history dari file
with open('history.pkl', 'rb') as f:
    loaded_history = pickle.load(f)

# Simpan model format SavedModel
model.export("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn4")

# Simpan model format HDF5
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn4.h5")

# Simpan model format Keras
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn4.keras")

# Simpan model format TFLite
converter4 = tf.lite.TFLiteConverter.from_saved_model("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn4")
tflite_model4 = converter4.convert()
with open("/content/drive/My Drive/Capstone LaskarAI/mnn4.tflite", "wb") as f:
    f.write(tflite_model4)

# Simpan History Model
with open("/content/drive/My Drive/Capstone LaskarAI/mnn4.pkl", "wb") as f:
    pickle.dump(history, f)

# Simpan model format tensorflow.js
tfjs4_path = "/content/drive/My Drive/Capstone LaskarAI/tfjs_model/mnn4"
tfjs.converters.save_keras_model(model, tfjs4_path)
print("TensorFlow.js model saved successfully")

# Visualisasi akurasi pelatihan dan validasi untuk mendeteksi overfitting
plt.plot(history.history['accuracy_target_1'], label='Train Accuracy for Target 1')
plt.plot(history.history['val_accuracy_target_1'], label='Validation Accuracy for Target 1')
plt.plot(history.history['accuracy_target_2'], label='Train Accuracy for Target 2')
plt.plot(history.history['val_accuracy_target_2'], label='Validation Accuracy for Target 2')
plt.plot(history.history['accuracy_target_3'], label='Train Accuracy for Target 3')
plt.plot(history.history['val_accuracy_target_3'], label='Validation Accuracy for Target 3')

plt.title('Accuracy per Epoch for Multi-Output Classification')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

# Visualisasi loss pelatihan dan validasi
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')

plt.title('Loss per Epoch for Multi-Output Classification')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.show()

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# --- Evaluasi Model pada Data Uji ---
print("\n=== Evaluasi Model pada Data Uji ===")
evaluation_results = model.evaluate(X_test_processed, y_test_dict, verbose=0)

# Hasil evaluasi akan menjadi list, urutan sesuai dengan compile metrics
print(f"Total Loss: {evaluation_results[0]:.4f}")
# Cetak metrik untuk setiap output
for i in range(len(targets)):
    print(f"  Target {i+1} Loss: {evaluation_results[i+1]:.4f}")
    print(f"  Target {i+1} Accuracy: {evaluation_results[i+1+len(targets)]:.4f}")

# --- Membuat Prediksi ---
predictions = model.predict(X_test_processed)

# --- Evaluasi Detail per Target Output ---
for i, target_col in enumerate(targets):
    print(f"\n--- Metrik Evaluasi untuk Target: {target_col} ---")

    # Ambil probabilitas prediksi untuk target ini
    pred_prob = predictions[i]
    # Konversi probabilitas ke kelas prediksi (indeks dengan probabilitas tertinggi)
    predicted_classes = (pred_prob > 0.5).astype(int)  # Ambil 1 jika probabilitas > 0.5, jika tidak ambil 0

    # Ambil kelas sebenarnya (dari one-hot encoded ke integer)
    true_classes = np.argmax(y_test_list[i], axis=1)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(true_classes, predicted_classes, zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(true_classes, predicted_classes)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=np.unique(true_classes),
                yticklabels=np.unique(true_classes))
    plt.title(f'Confusion Matrix - {target_col}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC (Untuk target biner)
    # Karena ini adalah binary classification, kita bisa langsung menghitung ROC AUC
    try:
        auc_score = roc_auc_score(true_classes, pred_prob)
        print(f"ROC AUC Score (Weighted - OvR) untuk {target_col}: {auc_score:.4f}")

        # Plot ROC Curve
        fpr, tpr, _ = roc_curve(true_classes, pred_prob)
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, label=f'{target_col} (AUC = {auc_score:.2f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve - {target_col}')
        plt.legend(loc="lower right")
        plt.grid(True)
        plt.show()

    except ValueError as e:
        print(f"Tidak dapat menghitung ROC AUC untuk {target_col}: {e}")

    # Akurasi
    accuracy = accuracy_score(true_classes, predicted_classes)
    print(f"Akurasi untuk {target_col}: {accuracy:.4f}")

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

# --- Evaluasi Model pada Data Uji ---
print("\n=== Evaluasi Model pada Data Uji ===")
evaluation_results = model.evaluate(X_test_processed, y_test_dict, verbose=0)

# Hasil evaluasi akan menjadi list, urutan sesuai dengan compile metrics
print(f"Total Loss: {evaluation_results[0]:.4f}")
# Cetak metrik untuk setiap output
for i in range(len(targets)):
    print(f"  Target {i+1} Loss: {evaluation_results[i+1]:.4f}")
    print(f"  Target {i+1} Accuracy: {evaluation_results[i+1+len(targets)]:.4f}")

# --- Membuat Prediksi ---
predictions = model.predict(X_test_processed)

# --- Evaluasi Detail per Target Output ---
for i, target_col in enumerate(targets):
    print(f"\n--- Metrik Evaluasi untuk Target: {target_col} ---")

    # Ambil probabilitas prediksi untuk target ini
    pred_prob = predictions[i]
    # Konversi probabilitas ke kelas prediksi (indeks dengan probabilitas tertinggi)
    predicted_classes = np.argmax(pred_prob, axis=1)

    # Ambil kelas sebenarnya (dari one-hot encoded ke integer)
    true_classes = np.argmax(y_test_list[i], axis=1)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(true_classes, predicted_classes, zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(true_classes, predicted_classes)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=np.unique(true_classes),
                yticklabels=np.unique(true_classes))
    plt.title(f'Confusion Matrix - {target_col}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC (Jika target biner atau multiclass-one-vs-rest)
    # Ini akan bekerja jika jumlah kelas untuk target tersebut > 1
    if num_classes_per_target[i] > 1:
        try:
            # ROC AUC untuk multiclass (rata-rata per kelas)
            auc_score = roc_auc_score(y_test_list[i], pred_prob, multi_class='ovr', average='weighted')
            print(f"ROC AUC Score (Weighted - OvR): {auc_score:.4f}")

            # Plot ROC Curve (Contoh untuk beberapa kelas pertama jika multiclass)
            plt.figure(figsize=(8, 6))
            for j in range(num_classes_per_target[i]):
                fpr, tpr, _ = roc_curve(y_test_list[i][:, j], pred_prob[:, j])
                plt.plot(fpr, tpr, label=f'Class {j} (AUC = {roc_auc_score(y_test_list[i][:, j], pred_prob[:, j]):.2f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {target_col} (One-vs-Rest)')
            plt.legend(loc="lower right")
            plt.grid(True)
            plt.show()

        except ValueError as e:
            print(f"Tidak dapat menghitung ROC AUC untuk {target_col}: {e}")
    else:
        print(f"ROC AUC tidak berlaku untuk target biner '{target_col}' jika hanya ada satu kelas.")

"""## MNN 3"""

X.info()

y.info()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTEN
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import StratifiedKFold
import numpy as np

X_prep = X.copy()

# Kolom berdasarkan informasi gambar
binary_cols = [
    'Hypertension', 'HighChol', 'CholCheck', 'SmokingStatus',
    'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump',
    'AnyHealthcare', 'NoDocbcCost', 'DiffWalk', 'Gender'
]

onehot_cols = [
    'GenHlth', 'Education', 'Income'
]

# 1. Ubah kolom binary menjadi tipe integer
X_prep[binary_cols] = X_prep[binary_cols].astype(int)

# 2. One-hot encoding untuk kolom kategorikal dengan unique > 2
X_prep = pd.get_dummies(X_prep, columns=onehot_cols, drop_first=False).astype(int)

print(f"Original dataset size: {X.shape[0]} samples")
print(f"Preprocessing dataset size: {X_prep.shape[0]} samples")

X_prep.info()

X_prep

y.info()

# Identifikasi kolom numerik yang perlu di-scaling (misalnya, 'BMI', 'PhysicalHealth', 'MentalHealth', 'SleepTime')
# Jangan scaling kolom yang sudah kategorikal atau biner
#numerical_features = ['BMI', 'PhysHlth', 'MentHlth', 'Age', 'Glucose']
numerical_features = ['BMI', 'PhysHlth', 'MentHlth', 'Age']

# Perhatikan: Nama kolom Anda mungkin sudah berubah karena proses SMOTE.
# Pastikan Anda menggunakan nama kolom yang benar dari X_train_tf.columns

scaler = StandardScaler()

# Terapkan scaling HANYA pada fitur numerik di data training dan testing
X_prep[numerical_features] = scaler.fit_transform(X_prep[numerical_features])

X_array = X_prep.to_numpy()
y_array = y.to_numpy().astype('int32')

X_array

y_array

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import BinaryAccuracy
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import StratifiedKFold

# --- Mendefinisikan K-fold Cross Validation ---
num_folds = 5  # Jumlah fold untuk cross-validation
kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

# --- Cross-validation loop ---
for fold, (train_idx, val_idx) in enumerate(kfold.split(X_array, np.argmax(y_array, axis=1))):
    print(f"\n--- FOLD {fold+1} ---")

    # Membagi data menjadi data pelatihan dan validasi untuk fold tertentu
    X_train, X_val = X_array[train_idx], X_array[val_idx]
    y_train, y_val = y_array[train_idx], y_array[val_idx]

    # --- Membangun Model ---
    input_shape = X_train.shape[1]
    model = Sequential([
        Input(shape=(X_train.shape[1],)),
        Dense(256, activation='relu'),
        Dropout(0.3),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dense(3, activation='sigmoid')  # 3 output untuk HeartDisease, Stroke, Diabetes
    ])

    # --- Compile Model ---
    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='binary_crossentropy',
                  metrics=[BinaryAccuracy()])

    # --- Early Stopping ---
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

    # --- Latih Model ---
    history = model.fit(X_train,
                        y_train,
                        epochs=100,
                        batch_size=32,
                        validation_data=(X_val, y_val),
                        callbacks=[early_stopping],
                        verbose=1)

    # --- Evaluasi Model pada Data Validasi ---
    y_pred = model.predict(X_val)

    # --- Menghitung ROC AUC dan Classification Report ---
    for i, target in enumerate(['HeartDisease', 'Stroke', 'Diabetes']):
        print(f"\n--- Metrik Evaluasi untuk Target: {target} ---")
        target_pred = (y_pred[i] > 0.5).astype(int)
        target_true = y_val[target].values
        print(classification_report(target_true, target_pred))

        # --- Menghitung ROC AUC Score ---
        roc_auc = roc_auc_score(target_true, y_pred[i])
        print(f"ROC AUC Score (Target {target}): {roc_auc:.4f}")

# Simpan model format SavedModel
model.export("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn3")

# Simpan model format HDF5
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn3.h5")

# Simpan model format Keras
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn3.keras")

# Simpan model format TFLite
converter3 = tf.lite.TFLiteConverter.from_saved_model("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn3")
tflite_model3 = converter3.convert()
with open("/content/drive/My Drive/Capstone LaskarAI/mnn3.tflite", "wb") as f:
    f.write(tflite_model3)

# Simpan History Model
with open("/content/drive/My Drive/Capstone LaskarAI/mnn3.pkl", "wb") as f:
    pickle.dump(history.history, f)

# Simpan model format tensorflow.js
tfjs3_path = "/content/drive/My Drive/Capstone LaskarAI/tfjs_model/mnn3"
tfjs.converters.save_keras_model(model, tfjs3_path)
print("TensorFlow.js model saved successfully")

y_pred

y_val

y_pred = model.predict(X_val)

target_true

# --- Menghitung ROC AUC dan Classification Report ---
for i, target in enumerate(['HeartDisease', 'Stroke', 'Diabetes']):
        print(f"\n--- Metrik Evaluasi untuk Target: {target} ---")
        target_pred = (y_pred[i] > 0.5).astype(int)
        target_true = y_val.to_numpy().astype('int32')
        print(classification_report(target_true, target_pred))

        # --- Menghitung ROC AUC Score ---
        roc_auc = roc_auc_score(target_true, y_pred[i])
        print(f"ROC AUC Score (Target {target}): {roc_auc:.4f}")

"""### 2"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import BinaryAccuracy
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import StratifiedKFold

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.callbacks import EarlyStopping

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.callbacks import EarlyStopping

# Split data X dan y terlebih dahulu
X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
    X_prep, y, test_size=0.2, random_state=42
)

# Terapkan preprocessing pada X
X_train_processed = X_train_raw.to_numpy()
X_test_processed = X_test_raw.to_numpy()

# One-hot encode target variables separately for train and test sets
y_train_list = []
y_test_list = []
num_classes_per_target = []

for target_col in targets:
    # Ambil nilai unik dari target untuk menentukan jumlah kelas
    num_classes = y_train_raw[target_col].nunique() # Use train data to determine classes
    num_classes_per_target.append(num_classes)
    # Lakukan one-hot encoding
    # encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # Add handle_unknown for consistency
    y_train_list.append(y_train_raw[[target_col]])
    y_test_list.append(y_test_raw[[target_col]]) # Use transform on test data

# Now y_train_list and y_test_list are lists of arrays with consistent sample numbers

# --- Definisikan Input Layer ---
input_shape = X_train_processed.shape[1]
input_layer = Input(shape=(input_shape,), name='input_features')

# --- Hidden Layers dengan Dropout ---
x = Dense(256, activation='relu')(input_layer)
x = Dropout(0.3)(x) # Dropout dengan probabilitas 30%
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(64, activation='relu')(x)

# --- Output Layers untuk Setiap Target ---
output_layers = []
for i, num_classes in enumerate(num_classes_per_target):
    output_layer = Dense(num_classes, activation='softmax', name=f'output_target_{i+1}')(x)
    output_layers.append(output_layer)

# --- Buat Model ---
model = Model(inputs=input_layer, outputs=output_layers)

# --- Compile Model ---
losses = {f'output_target_{i+1}': CategoricalCrossentropy() for i in range(len(targets))}
metrics = {f'output_target_{i+1}': CategoricalAccuracy(name=f'accuracy_target_{i+1}') for i in range(len(targets))}

model.compile(optimizer=Adam(learning_rate=0.001),
              loss=losses,
              metrics=metrics)

model.summary()

# --- Callback Early Stopping ---
# Monitor 'val_loss' (loss pada data validasi)
# patience=10 berarti akan berhenti jika val_loss tidak membaik selama 10 epoch berturut-turut
# restore_best_weights=True akan mengembalikan bobot model dari epoch terbaik
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

# --- Format y_train_list dan y_test_list untuk feed ke model ---
# Access elements of y_train_list and y_test_list using their index i
y_train_dict = {f'output_target_{i+1}': y_train_list[i] for i in range(len(y_train_list))}
y_test_dict = {f'output_target_{i+1}': y_test_list[i] for i in range(len(y_test_list))}


# --- Latih Model ---
history = model.fit(X_train_processed, y_train_dict,
                    epochs=100, # Set epoch lebih tinggi karena Early Stopping akan menghentikan secara otomatis
                    batch_size=32,
                    validation_data=(X_test_processed, y_test_dict),
                    callbacks=[early_stopping], # Tambahkan callback EarlyStopping
                    verbose=1)

targets = y.columns
print(targets)

X.info()

# --- 2. Preprocessing Data ---
# Identifikasi kolom numerik dan kategori
categorical_cols = X.select_dtypes(include=['object', 'bool']).columns
numerical_cols = X.select_dtypes(include=np.number).columns

# Buat preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])

# Split data X dan y terlebih dahulu
X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
    X_prep, y, test_size=0.2, random_state=42
)

# Terapkan preprocessing pada X
X_train_processed = X_train_raw.to_numpy()
X_test_processed = X_test_raw.to_numpy()

# One-hot encode target variables separately for train and test sets
y_train_list = []
y_test_list = []
num_classes_per_target = []

for target_col in targets:
    # Ambil nilai unik dari target untuk menentukan jumlah kelas
    num_classes = y_train_raw[target_col].nunique() # Use train data to determine classes
    num_classes_per_target.append(num_classes)
    # Lakukan one-hot encoding
    # encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # Add handle_unknown for consistency
    y_train_list.append(y_train_raw[[target_col]])
    y_test_list.append(y_test_raw[[target_col]]) # Use transform on test data

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.callbacks import EarlyStopping

# --- Definisikan Input Layer ---
input_shape = X_train_processed.shape[1]
input_layer = Input(shape=(input_shape,), name='input_features')

# --- Hidden Layers dengan Dropout ---
x = Dense(256, activation='relu')(input_layer)
x = Dropout(0.3)(x) # Dropout dengan probabilitas 30%
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(64, activation='relu')(x)

# --- Output Layers untuk Setiap Target ---
output_layers = []
for i, num_classes in enumerate(num_classes_per_target):
    output_layer = Dense(num_classes, activation='softmax', name=f'output_target_{i+1}')(x)
    output_layers.append(output_layer)

# --- Buat Model ---
model = Model(inputs=input_layer, outputs=output_layers)

# --- Compile Model ---
losses = {f'output_target_{i+1}': CategoricalCrossentropy() for i in range(len(targets))}
metrics = {f'output_target_{i+1}': CategoricalAccuracy(name=f'accuracy_target_{i+1}') for i in range(len(targets))}

model.compile(optimizer=Adam(learning_rate=0.001),
              loss=losses,
              metrics=metrics)

model.summary()

# --- Callback Early Stopping ---
# Monitor 'val_loss' (loss pada data validasi)
# patience=10 berarti akan berhenti jika val_loss tidak membaik selama 10 epoch berturut-turut
# restore_best_weights=True akan mengembalikan bobot model dari epoch terbaik
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

# --- Format y_train_list dan y_test_list untuk feed ke model ---
# Access elements of y_train_list and y_test_list using their index i
y_train_dict = {f'output_target_{i+1}': y_train_list[i] for i in range(len(y_train_list))}
y_test_dict = {f'output_target_{i+1}': y_test_list[i] for i in range(len(y_test_list))}


# --- Latih Model ---
history = model.fit(X_train_processed, y_train_dict,
                    epochs=100, # Set epoch lebih tinggi karena Early Stopping akan menghentikan secara otomatis
                    batch_size=32,
                    validation_data=(X_test_processed, y_test_dict),
                    callbacks=[early_stopping], # Tambahkan callback EarlyStopping
                    verbose=1)

X_train_raw

X_train_processed

# --- Mendefinisikan K-fold Cross Validation ---
num_folds = 2  # Jumlah fold untuk cross-validation
kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

# --- Cross-validation loop ---
for fold, (train_idx, val_idx) in enumerate(kfold.split(X_array, np.argmax(y, axis=1))):
    print(f"\n--- FOLD {fold+1} ---")

    # Membagi data menjadi data pelatihan dan validasi untuk fold tertentu
    X_train, X_val = X_array[train_idx], X_array[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    y_train_list = []
    y_val_list = []
    num_classes_per_target = []

    for target_col in targets:
        # Ambil nilai unik dari target untuk menentukan jumlah kelas
        num_classes = y_train[target_col].nunique() # Use train data to determine classes
        num_classes_per_target.append(num_classes)
        # Lakukan one-hot encoding
        # encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # Add handle_unknown for consistency
        y_train_list.append(y_train[[target_col]])
        y_val_list.append(y_val[[target_col]]) # Use transform on test data

    # --- Definisikan Input Layer ---
    input_shape = X_train.shape[1]
    input_layer = Input(shape=(input_shape,), name='input_features')

    # --- Hidden Layers dengan Dropout ---
    x = Dense(256, activation='relu')(input_layer)
    x = Dropout(0.3)(x) # Dropout dengan probabilitas 30%
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)
    x = Dense(64, activation='relu')(x)

    # --- Output Layers untuk Setiap Target ---
    output_layers = []
    for i in range(len(targets)):
        output_layer = Dense(1, activation='sigmoid', name=f'output_target_{i+1}')(x)  # Sigmoid untuk binary classification
        output_layers.append(output_layer)

    # --- Buat Model ---
    model2 = Model(inputs=input_layer, outputs=output_layers)

    # --- Compile Model ---
    losses = {f'output_target_{i+1}': BinaryCrossentropy() for i in range(len(targets))}
    metrics = {f'output_target_{i+1}': BinaryAccuracy(name=f'accuracy_target_{i+1}') for i in range(len(targets))}

    model2.compile(optimizer=Adam(learning_rate=0.001),
                  loss=losses,
                  metrics=metrics)

    model2.summary()

    # --- Callback Early Stopping ---
    # Monitor 'val_loss' (loss pada data validasi)
    # patience=10 berarti akan berhenti jika val_loss tidak membaik selama 10 epoch berturut-turut
    # restore_best_weights=True akan mengembalikan bobot model dari epoch terbaik
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

    # --- Format y_train_list dan y_test_list untuk feed ke model ---
    # Access elements of y_train_list and y_test_list using their index i
    y_train_dict = {f'output_target_{i+1}': y_train_list[i] for i in range(len(y_train_list))}
    y_val_dict = {f'output_target_{i+1}': y_val_list[i] for i in range(len(y_val_list))}

# --- Membuat Prediksi ---
predictions = model.predict(X_val)

predictions

y_val_list

y_test_list

true_classes

true_classes

predicted_classes

# --- Evaluasi Detail per Target Output ---
for i, target_col in enumerate(targets):
    print(f"\n--- Metrik Evaluasi untuk Target: {target_col} ---")

    # Ambil probabilitas prediksi untuk target ini
    pred_prob = predictions[i]

    # Untuk binary classification, gunakan threshold 0.5
    predicted_classes = (pred_prob > 0.5).astype(int)  # Jika probabilitas > 0.5, kelas = 1, jika <= 0.5, kelas = 0

    # Ambil kelas sebenarnya (dari one-hot encoded ke integer)
    true_classes = y_val_list[1]  # Jika sudah dalam format one-hot, gunakan argmax untuk mengubah ke integeri
    true_classes = np.argmax(true_classes, axis=1)  # Mengonversi one-hot ke integer
    #true_classes = np.argmax(y_test_list[i], axis=1)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(true_classes, predicted_classes, zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(true_classes, predicted_classes)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=np.unique(true_classes),
                yticklabels=np.unique(true_classes))
    plt.title(f'Confusion Matrix - {target_col}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC (Jika target biner atau multiclass-one-vs-rest)
    # Ini akan bekerja jika jumlah kelas untuk target tersebut > 1
    if num_classes_per_target[i] > 1:
        try:
            # ROC AUC untuk multiclass (rata-rata per kelas)
            auc_score = roc_auc_score(true_classes, pred_prob, average='weighted')
            print(f"ROC AUC Score (Weighted - OvR): {auc_score:.4f}")

            # Plot ROC Curve (Contoh untuk beberapa kelas pertama jika multiclass)
            plt.figure(figsize=(8, 6))
            fpr, tpr, _ = roc_curve(true_classes, pred_prob)
            plt.plot(fpr, tpr, label=f'Class {i} (AUC = {auc_score:.2f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {target_col}')
            plt.legend(loc="lower right")
            plt.grid(True)
            plt.show()

        except ValueError as e:
            print(f"Tidak dapat menghitung ROC AUC untuk {target_col}: {e}")
    else:
        print(f"ROC AUC tidak berlaku untuk target biner '{target_col}' jika hanya ada satu kelas.")

# --- Membuat Prediksi ---
predictions = model.predict(X_val)

# --- Evaluasi Detail per Target Output ---
for i, target_col in enumerate(targets):
    print(f"\n--- Metrik Evaluasi untuk Target: {target_col} ---")

    # Ambil probabilitas prediksi untuk target ini
    pred_prob = predictions[i]
    # Konversi probabilitas ke kelas prediksi (indeks dengan probabilitas tertinggi)
    predicted_classes = np.argmax(pred_prob, axis=1)

    # Ambil kelas sebenarnya (dari one-hot encoded ke integer)
    true_classes = np.argmax(y_val_list[i], axis=1)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(true_classes, predicted_classes, zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(true_classes, predicted_classes)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=np.unique(true_classes),
                yticklabels=np.unique(true_classes))
    plt.title(f'Confusion Matrix - {target_col}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC (Jika target biner atau multiclass-one-vs-rest)
    # Ini akan bekerja jika jumlah kelas untuk target tersebut > 1
    if num_classes_per_target[i] > 1:
        try:
            # ROC AUC untuk multiclass (rata-rata per kelas)
            auc_score = roc_auc_score(y_val_list[i], pred_prob, multi_class='ovr', average='weighted')
            print(f"ROC AUC Score (Weighted - OvR): {auc_score:.4f}")

            # Plot ROC Curve (Contoh untuk beberapa kelas pertama jika multiclass)
            plt.figure(figsize=(8, 6))
            for j in range(num_classes_per_target[i]):
                fpr, tpr, _ = roc_curve(y_val_list[i][:, j], pred_prob[:, j])
                plt.plot(fpr, tpr, label=f'Class {j} (AUC = {roc_auc_score(y_val_list[i][:, j], pred_prob[:, j]):.2f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {target_col} (One-vs-Rest)')
            plt.legend(loc="lower right")
            plt.grid(True)
            plt.show()

        except ValueError as e:
            print(f"Tidak dapat menghitung ROC AUC untuk {target_col}: {e}")
    else:
        print(f"ROC AUC tidak berlaku untuk target biner '{target_col}' jika hanya ada satu kelas.")

num_classes

X_array = X_prep.to_numpy()
y_array = y.to_numpy().astype('int32')

X_prep

X_array.shape

X_train_processed.shape

X_train.shape

len(y_train_dict)

y_train

"""### 3"""

from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report, accuracy_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# --- Mendefinisikan K-fold Cross Validation ---
num_folds = 5  # Jumlah fold untuk cross-validation
kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

# --- Cross-validation loop ---
for fold, (train_idx, val_idx) in enumerate(kfold.split(X_array, np.argmax(y, axis=1))):
    print(f"\n--- FOLD {fold+1} ---")

    # Membagi data menjadi data pelatihan dan validasi untuk fold tertentu
    X_train, X_val = X_array[train_idx], X_array[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    y_train_list = []
    y_val_list = []
    num_classes_per_target = []

    for target_col in targets:
        # Ambil nilai unik dari target untuk menentukan jumlah kelas
        num_classes = y_train[target_col].nunique() # Use train data to determine classes
        num_classes_per_target.append(num_classes)
        # Lakukan one-hot encoding
        # encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # Add handle_unknown for consistency
        y_train_list.append(y_train[[target_col]])
        y_val_list.append(y_val[[target_col]]) # Use transform on test data

    # --- Definisikan Input Layer ---
    input_shape = X_train.shape[1]
    input_layer = Input(shape=(input_shape,), name='input_features')

    # --- Hidden Layers dengan Dropout ---
    x = Dense(256, activation='relu')(input_layer)
    x = Dropout(0.3)(x) # Dropout dengan probabilitas 30%
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)
    x = Dense(64, activation='relu')(x)

    # --- Output Layers untuk Setiap Target ---
    output_layers = []
    for i in range(len(targets)):
        output_layer = Dense(1, activation='sigmoid', name=f'output_target_{i+1}')(x)  # Sigmoid untuk binary classification
        output_layers.append(output_layer)

    # --- Buat Model ---
    model2 = Model(inputs=input_layer, outputs=output_layers)

    # --- Compile Model ---
    losses = {f'output_target_{i+1}': BinaryCrossentropy() for i in range(len(targets))}
    metrics = {f'output_target_{i+1}': BinaryAccuracy(name=f'accuracy_target_{i+1}') for i in range(len(targets))}

    model2.compile(optimizer=Adam(learning_rate=0.001),
                  loss=losses,
                  metrics=metrics)

    model2.summary()

    # --- Callback Early Stopping ---
    # Monitor 'val_loss' (loss pada data validasi)
    # patience=10 berarti akan berhenti jika val_loss tidak membaik selama 10 epoch berturut-turut
    # restore_best_weights=True akan mengembalikan bobot model dari epoch terbaik
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

    # --- Format y_train_list dan y_test_list untuk feed ke model ---
    # Access elements of y_train_list and y_test_list using their index i
    y_train_dict = {f'output_target_{i+1}': y_train_list[i] for i in range(len(y_train_list))}
    y_val_dict = {f'output_target_{i+1}': y_val_list[i] for i in range(len(y_val_list))}


    # --- Latih Model ---
    history2 = model2.fit(X_train, y_train_dict,
                        epochs=1, # Set epoch lebih tinggi karena Early Stopping akan menghentikan secara otomatis
                        batch_size=32,
                        validation_data=(X_val, y_val_dict),
                        callbacks=[early_stopping], # Tambahkan callback EarlyStopping
                        verbose=1)

    # --- Evaluasi Model pada Data Validasi ---
    y_pred = model2.predict(X_val)

    # --- Evaluasi Confusion Matrix, Classification Report, ROC AUC, dan Akurasi per Target ---
    for i, target in enumerate(targets):
        # Mengambil prediksi untuk target ke-i
        target_pred = (y_pred[i] > 0.5).astype(int)  # Threshold 0.5 untuk binary classification
        target_true = y_val_list[i].values  # Ground truth untuk target ini

        # Confusion Matrix
        cm = confusion_matrix(target_true, target_pred)
        print(f"\n--- Confusion Matrix untuk {target} ---")
        print(cm)

        # Plot Confusion Matrix
        plt.figure(figsize=(5,5))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["No", "Yes"], yticklabels=["No", "Yes"])
        plt.title(f"Confusion Matrix untuk {target}")
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.show()

        # Classification Report
        print(f"\n--- Classification Report untuk {target} ---")
        print(classification_report(target_true, target_pred))

        # ROC AUC Score
        roc_auc = roc_auc_score(target_true, y_pred[i])
        print(f"ROC AUC Score untuk {target}: {roc_auc:.4f}")

        # Plot ROC Curve
        fpr, tpr, _ = roc_curve(target_true, y_pred[i])
        plt.figure(figsize=(5,5))
        plt.plot(fpr, tpr, color='blue', label=f'ROC Curve {target}')
        plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
        plt.title(f"ROC Curve untuk {target}")
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.legend(loc='lower right')
        plt.show()

        # Akurasi
        accuracy = accuracy_score(target_true, target_pred)
        print(f"Akurasi untuk {target}: {accuracy:.4f}")

import matplotlib.pyplot as plt

# Visualisasi Loss dan Accuracy untuk Cross-Validation
plt.figure(figsize=(12, 6))

# Loss
plt.subplot(1, 2, 1)
for i in range(num_folds):
    plt.plot(history.history['loss'], label=f'Fold {i+1} Train Loss')
    plt.plot(history.history['val_loss'], label=f'Fold {i+1} Val Loss')
plt.title('Loss per Epoch for Cross-Validation')
plt.legend()

# Accuracy
plt.subplot(1, 2, 2)
for i in range(num_folds):
    plt.plot(history.history['categorical_accuracy'], label=f'Fold {i+1} Train Accuracy')
    plt.plot(history.history['val_categorical_accuracy'], label=f'Fold {i+1} Val Accuracy')
plt.title('Accuracy per Epoch for Cross-Validation')
plt.legend()

plt.show()

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

# --- Evaluasi Model pada Data Uji ---
print("\n=== Evaluasi Model pada Data Uji ===")
evaluation_results = model.evaluate(X_val, y_test_dict, verbose=0)

# Hasil evaluasi akan menjadi list, urutan sesuai dengan compile metrics
print(f"Total Loss: {evaluation_results[0]:.4f}")
# Cetak metrik untuk setiap output
for i in range(len(targets)):
    print(f"  Target {i+1} Loss: {evaluation_results[i+1]:.4f}")
    print(f"  Target {i+1} Accuracy: {evaluation_results[i+1+len(targets)]:.4f}")

# --- Membuat Prediksi ---
predictions = model.predict(X_test_processed)

# --- Evaluasi Detail per Target Output ---
for i, target_col in enumerate(targets):
    print(f"\n--- Metrik Evaluasi untuk Target: {target_col} ---")

    # Ambil probabilitas prediksi untuk target ini
    pred_prob = predictions[i]
    # Konversi probabilitas ke kelas prediksi (indeks dengan probabilitas tertinggi)
    predicted_classes = np.argmax(pred_prob, axis=1)

    # Ambil kelas sebenarnya (dari one-hot encoded ke integer)
    true_classes = np.argmax(y_test_list[i], axis=1)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(true_classes, predicted_classes, zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(true_classes, predicted_classes)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=np.unique(true_classes),
                yticklabels=np.unique(true_classes))
    plt.title(f'Confusion Matrix - {target_col}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC (Jika target biner atau multiclass-one-vs-rest)
    # Ini akan bekerja jika jumlah kelas untuk target tersebut > 1
    if num_classes_per_target[i] > 1:
        try:
            # ROC AUC untuk multiclass (rata-rata per kelas)
            auc_score = roc_auc_score(y_test_list[i], pred_prob, multi_class='ovr', average='weighted')
            print(f"ROC AUC Score (Weighted - OvR): {auc_score:.4f}")

            # Plot ROC Curve (Contoh untuk beberapa kelas pertama jika multiclass)
            plt.figure(figsize=(8, 6))
            for j in range(num_classes_per_target[i]):
                fpr, tpr, _ = roc_curve(y_test_list[i][:, j], pred_prob[:, j])
                plt.plot(fpr, tpr, label=f'Class {j} (AUC = {roc_auc_score(y_test_list[i][:, j], pred_prob[:, j]):.2f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {target_col} (One-vs-Rest)')
            plt.legend(loc="lower right")
            plt.grid(True)
            plt.show()

        except ValueError as e:
            print(f"Tidak dapat menghitung ROC AUC untuk {target_col}: {e}")
    else:
        print(f"ROC AUC tidak berlaku untuk target biner '{target_col}' jika hanya ada satu kelas.")

"""## TensorFlow RF"""

targets = y.columns
print(targets)

df = pd.DataFrame(X.join(y))
print("\nGabungan dengan X.join(y) (default left join):")
print(df)

df.info()

num_rows = df.shape[0]
print(f"Number of rows: {num_rows}")

!pip install ydf

# Install YDF!
# If you haven't already, run this in your environment:
# pip install ydf -U

import ydf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    confusion_matrix,
    roc_curve,
    accuracy_score
)
import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. Load Your Dataset ---
# Replace this with how you load your actual data.
# For demonstration, I'm creating a dummy DataFrame that matches your description.
df = pd.DataFrame(X.join(y))
np.random.seed(42)
num_samples = df.shape[0] # Matches your 'count'

# --- Penting: Identifikasi dan Konversi Tipe Data Kategorik ---
# Berdasarkan deskripsi Anda, beberapa kolom adalah kategorik/biner tetapi mungkin dimuat sebagai int.
# YDF dapat menangani int sebagai kategorik, tetapi lebih eksplisit jika diubah ke tipe 'category' di Pandas.

# Kolom-kolom yang diduga kategorik/biner (selain target)
categorical_features = df.select_dtypes(include=['object', 'category']).columns

# Pastikan kolom-kolom ini adalah tipe 'category' di Pandas
for col in categorical_features:
    # Hanya lakukan jika kolom ada di DataFrame
    if col in df.columns:
        df[col] = df[col].astype('category')

# Kolom 'Age', 'Education', 'Income' mungkin adalah ordinal categorical,
# Jika Anda menganggapnya sebagai ordinal, YDF akan menanganinya secara cerdas.
# Jika Anda ingin YDF memperlakukannya sebagai numerik, biarkan saja sebagai integer/float.
# Contoh di sini, kita anggap mereka kategorikal ordinal.

print("Data loaded and dummy data created successfully.")
print("\nDataFrame Info after categorical conversion:")
print(df.info())
print("\nHead of DataFrame:")
print(df.head())


# --- 2. Define Target Columns ---
#TARGET_COLUMNS = ['HeartDisease', 'Stroke', 'Diabetes']
targets = y.columns
features = [col for col in df.columns if col not in targets + ['stratify_label']] # Tambahkan 'stratify_label' agar tidak jadi fitur


# --- Stratifikasi Kustom untuk Multi-Label ---
df['stratify_label'] = df[targets].astype(str).agg(''.join, axis=1)

# --- 3. Split Data into Training and Testing Sets ---
train_df, test_df = train_test_split(
    df, test_size=0.2, random_state=42,
    stratify=df['stratify_label']
)

# Hapus kolom 'stratify_label' dari DataFrame training dan testing
train_df = train_df.drop(columns=['stratify_label'])
test_df = test_df.drop(columns=['stratify_label'])


print(f"\nTraining set size: {len(train_df)} samples")
print(f"Testing set size: {len(test_df)} samples")

# --- 4. Train and Evaluate a Random Forest Model for Each Target ---

# Dictionary to store trained models
trained_models = {}

for target in targets:
    print(f"\n--- Training Random Forest for target: {target} ---")

    rf_learner = ydf.RandomForestLearner(
        label=target,
        task=ydf.Task.CLASSIFICATION,
        num_trees=100, # Number of trees
        max_depth=10,  # Maximum depth of trees
        min_examples=5, # Minimum examples per leaf
        # random_seed=42 # For reproducibility
        # Contoh cara mengatur parameter fitur (opsional, YDF biasanya cukup cerdas)
        # features=[ydf.ColumnDef.categorical_column(col) for col in categorical_features_as_int] +
        #          [ydf.ColumnDef.numerical_column(col) for col in numerical_features]
    )

    model = rf_learner.train(train_df)
    trained_models[target] = model

    print(f"Model for {target} trained successfully.")
    # model.describe() # Uncomment to see full model description and logs

    # --- 5. Evaluate the Model ---
    print(f"\n--- Evaluating Model for target: {target} ---")
    evaluation_results = model.evaluate(test_df)

    true_labels = test_df[target].values
    predictions_proba = model.predict(test_df)

    if predictions_proba.ndim > 1 and predictions_proba.shape[1] > 1:
        predicted_prob_positive_class = predictions_proba[:, 1]
    else:
        predicted_prob_positive_class = predictions_proba.flatten()

    predicted_classes = (predicted_prob_positive_class > 0.5).astype(int)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(true_labels, predicted_classes, zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(true_labels, predicted_classes)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No', 'Yes'],
                yticklabels=['No', 'Yes'])
    plt.title(f'Confusion Matrix - {target}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC Score
    auc_score = roc_auc_score(true_labels, predicted_prob_positive_class)
    print(f"ROC AUC Score for {target}: {auc_score:.4f}")

    # Plot ROC Curve
    fpr, tpr, _ = roc_curve(true_labels, predicted_prob_positive_class)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'{target} (AUC = {auc_score:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {target}')
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

    # Accuracy
    accuracy = accuracy_score(true_labels, predicted_classes)
    print(f"Accuracy for {target}: {accuracy:.4f}")

    # --- 6. Save the Model ---
    model_save_dir = f"/content/drive/My Drive/Capstone LaskarAI/ydf_model_{target}"
    model.save(model_save_dir)
    print(f"Model for {target} saved to: {model_save_dir}")

print("\n--- All models trained, evaluated, and saved ---")

"""## TensorFlow XGBoost"""

targets = y.columns
print(targets)

df = pd.DataFrame(X.join(y))
print("\nGabungan dengan X.join(y) (default left join):")
print(df)

df.info()

num_rows = df.shape[0]
print(f"Number of rows: {num_rows}")

!pip install ydf

# Install YDF!
# If you haven't already, run this in your environment:
# pip install ydf -U

import ydf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    confusion_matrix,
    roc_curve,
    accuracy_score
)
import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. Load Your Dataset ---
# Replace this with how you load your actual data.
# For demonstration, I'm creating a dummy DataFrame that matches your description.
df = pd.DataFrame(X.join(y))
np.random.seed(42)
num_samples = df.shape[0] # Matches your 'count'

# --- Penting: Identifikasi dan Konversi Tipe Data Kategorik ---
# Berdasarkan deskripsi Anda, beberapa kolom adalah kategorik/biner tetapi mungkin dimuat sebagai int.
# YDF dapat menangani int sebagai kategorik, tetapi lebih eksplisit jika diubah ke tipe 'category' di Pandas.

# Kolom-kolom yang diduga kategorik/biner (selain target)
categorical_features = df.select_dtypes(include=['object', 'category']).columns

# Pastikan kolom-kolom ini adalah tipe 'category' di Pandas
for col in categorical_features:
    # Hanya lakukan jika kolom ada di DataFrame
    if col in df.columns:
        df[col] = df[col].astype('category')

# Kolom 'Age', 'Education', 'Income' mungkin adalah ordinal categorical,
# Jika Anda menganggapnya sebagai ordinal, YDF akan menanganinya secara cerdas.
# Jika Anda ingin YDF memperlakukannya sebagai numerik, biarkan saja sebagai integer/float.
# Contoh di sini, kita anggap mereka kategorikal ordinal.

print("Data loaded and dummy data created successfully.")
print("\nDataFrame Info after categorical conversion:")
print(df.info())
print("\nHead of DataFrame:")
print(df.head())


# --- 2. Define Target Columns ---
#TARGET_COLUMNS = ['HeartDisease', 'Stroke', 'Diabetes']
targets = y.columns
features = [col for col in df.columns if col not in targets + ['stratify_label']] # Tambahkan 'stratify_label' agar tidak jadi fitur


# --- Stratifikasi Kustom untuk Multi-Label ---
df['stratify_label'] = df[targets].astype(str).agg(''.join, axis=1)

# --- 3. Split Data into Training and Testing Sets ---
train_df, test_df = train_test_split(
    df, test_size=0.2, random_state=42,
    stratify=df['stratify_label']
)

# Hapus kolom 'stratify_label' dari DataFrame training dan testing
train_df = train_df.drop(columns=['stratify_label'])
test_df = test_df.drop(columns=['stratify_label'])


print(f"\nTraining set size: {len(train_df)} samples")
print(f"Testing set size: {len(test_df)} samples")

# --- 4. Train and Evaluate a Random Forest Model for Each Target ---

# Dictionary to store trained models
trained_models = {}

for target in targets:
    print(f"\n--- Training Random Forest for target: {target} ---")

    rf_learner = ydf.GradientBoostedTreesLearner(
        label=target,
        task=ydf.Task.CLASSIFICATION,
        num_trees=100, # Number of trees
        max_depth=10,  # Maximum depth of trees
        min_examples=5, # Minimum examples per leaf
        # random_seed=42 # For reproducibility
        # Contoh cara mengatur parameter fitur (opsional, YDF biasanya cukup cerdas)
        # features=[ydf.ColumnDef.categorical_column(col) for col in categorical_features_as_int] +
        #          [ydf.ColumnDef.numerical_column(col) for col in numerical_features]
    )

    model = rf_learner.train(train_df)
    trained_models[target] = model

    print(f"Model for {target} trained successfully.")
    # model.describe() # Uncomment to see full model description and logs

    # --- 5. Evaluate the Model ---
    print(f"\n--- Evaluating Model for target: {target} ---")
    evaluation_results = model.evaluate(test_df)

    true_labels = test_df[target].values
    predictions_proba = model.predict(test_df)

    if predictions_proba.ndim > 1 and predictions_proba.shape[1] > 1:
        predicted_prob_positive_class = predictions_proba[:, 1]
    else:
        predicted_prob_positive_class = predictions_proba.flatten()

    predicted_classes = (predicted_prob_positive_class > 0.5).astype(int)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(true_labels, predicted_classes, zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(true_labels, predicted_classes)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No', 'Yes'],
                yticklabels=['No', 'Yes'])
    plt.title(f'Confusion Matrix - {target}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC Score
    auc_score = roc_auc_score(true_labels, predicted_prob_positive_class)
    print(f"ROC AUC Score for {target}: {auc_score:.4f}")

    # Plot ROC Curve
    fpr, tpr, _ = roc_curve(true_labels, predicted_prob_positive_class)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'{target} (AUC = {auc_score:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {target}')
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

    # Accuracy
    accuracy = accuracy_score(true_labels, predicted_classes)
    print(f"Accuracy for {target}: {accuracy:.4f}")

    # --- 6. Save the Model ---
    model_save_dir = f"/content/drive/My Drive/Capstone LaskarAI/ydf_gboost_{target}"
    model.save(model_save_dir)
    print(f"Model for {target} saved to: {model_save_dir}")

print("\n--- All models trained, evaluated, and saved ---")

"""# Prediksi dan Interpretasi Output"""

y_test_dict

# Lakukan prediksi pada data baru (contoh data test)
predictions = model.predict(X_test_processed)

# Prediksi probabilitas untuk setiap penyakit
pred_heart_prob = predictions[0]
pred_stroke_prob = predictions[1]
pred_diabetes_prob = predictions[2]

# Konversi probabilitas menjadi kelas biner (risiko tinggi/rendah)
# Anda perlu menentukan threshold (misalnya 0.5)
threshold = 0.5
pred_heart_class = (pred_heart_prob > threshold).astype(int)
pred_stroke_class = (pred_stroke_prob > threshold).astype(int)
pred_diabetes_class = (pred_diabetes_prob > threshold).astype(int)

# Hitung metrik tambahan (Presisi, Recall, F1-Score, Confusion Matrix)
# Penting: Lakukan ini hanya untuk baris di mana label target tidak NaN.
# Anda perlu memfilter y_test_dict dan pred_..._class.

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import numpy as np

# --- Hitung metrik tambahan (Presisi, Recall, F1-Score, Confusion Matrix) ---

# Contoh untuk Heart Disease
# Convert one-hot encoded true and predicted labels to 1D arrays
true_classes_heart = np.argmax(y_test_dict['output_target_1'], axis=1)
predicted_classes_heart = np.argmax(pred_heart_class, axis=1) # Use np.argmax on the predicted classes as well

print("\n--- Metrik Lebih Lanjut untuk Heart Disease ---")
# Use zero_division parameter to handle cases with no predicted positive samples
print(f"Precision: {precision_score(true_classes_heart, predicted_classes_heart, zero_division=0):.4f}")
print(f"Recall: {recall_score(true_classes_heart, predicted_classes_heart, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(true_classes_heart, predicted_classes_heart, zero_division=0):.4f}")
print("Confusion Matrix:\n", confusion_matrix(true_classes_heart, predicted_classes_heart))

# Example for Stroke
# Convert one-hot encoded true and predicted labels to 1D arrays
true_classes_stroke = np.argmax(y_test_dict['output_target_2'], axis=1)
predicted_classes_stroke = np.argmax(pred_stroke_class, axis=1) # Use np.argmax on the predicted classes as well

print("\n--- Metrik Lebih Lanjut untuk Stroke ---")
print(f"Precision: {precision_score(true_classes_stroke, predicted_classes_stroke, zero_division=0):.4f}")
print(f"Recall: {recall_score(true_classes_stroke, predicted_classes_stroke, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(true_classes_stroke, predicted_classes_stroke, zero_division=0):.4f}")
print("Confusion Matrix:\n", confusion_matrix(true_classes_stroke, predicted_classes_stroke))

# Example for Diabetes
# Convert one-hot encoded true and predicted labels to 1D arrays
true_classes_diabetes = np.argmax(y_test_dict['output_target_3'], axis=1)
predicted_classes_diabetes = np.argmax(pred_diabetes_class, axis=1) # Use np.argmax on the predicted classes as well

print("\n--- Metrik Lebih Lanjut untuk Diabetes ---")
print(f"Precision: {precision_score(true_classes_diabetes, predicted_classes_diabetes, zero_division=0):.4f}")
print(f"Recall: {recall_score(true_classes_diabetes, predicted_classes_diabetes, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(true_classes_diabetes, predicted_classes_diabetes, zero_division=0):.4f}")
print("Confusion Matrix:\n", confusion_matrix(true_classes_diabetes, predicted_classes_diabetes))


# Contoh interpretasi untuk satu individu (misalkan individu pertama di test set)
sample_index = 0
print(f"\n--- Prediksi untuk Sampel Individu ke-{sample_index} ---")
# Use NumPy indexing instead of .iloc for X_test_processed
print(f"Fitur Input (Processed): {X_test_processed[sample_index][:5]}...")

# Access true labels from the 1D arrays
print(f"Label Aktual Heart Disease: {true_classes_heart[sample_index]}")
# Access predicted probabilities correctly from the 2D array
print(f"Prediksi Probabilitas Heart Disease (Class 1): {pred_heart_prob[sample_index][1]:.4f}")
# Access predicted class correctly from the 1D array
print(f"Prediksi Kelas Heart Disease: {'Risiko Tinggi' if predicted_classes_heart[sample_index] == 1 else 'Risiko Rendah'}")

print(f"Label Aktual Stroke: {true_classes_stroke[sample_index]}")
print(f"Prediksi Probabilitas Stroke (Class 1): {pred_stroke_prob[sample_index][1]:.4f}")
print(f"Prediksi Kelas Stroke: {'Risiko Tinggi' if predicted_classes_stroke[sample_index] == 1 else 'Risiko Rendah'}")

print(f"Label Aktual Diabetes: {true_classes_diabetes[sample_index]}")
print(f"Prediksi Probabilitas Diabetes (Class 1): {pred_diabetes_prob[sample_index][1]:.4f}")
print(f"Prediksi Kelas Diabetes: {'Risiko Tinggi' if predicted_classes_diabetes[sample_index] == 1 else 'Risiko Rendah'}")

# Untuk interpretasi faktor pendorong risiko, Anda perlu menggunakan library seperti SHAP
# Ini akan menjadi langkah analisis terpisah setelah model dilatih.
# import shap
# explainer = shap.DeepExplainer(model, X_train_sample_for_shap) # Butuh sample data untuk DeepExplainer
# shap_values = explainer.shap_values(X_test_sample) # Pilih sample dari X_test untuk diinterpretasi
# shap.summary_plot(shap_values[0], X_test_sample, feature_names=feature_names, plot_type="bar", show=False)
# plt.title('SHAP Values for Heart Disease Prediction')
# plt.show()
# # Lakukan ini untuk setiap output head

import pandas as pd

# Buat contoh DataFrame pertama
data1 = {'col1': [1, 2], 'col2': [3, 4]}
df1 = pd.DataFrame(data1)
print("DataFrame 1:")
print(df1)

# Buat contoh DataFrame kedua
data2 = {'col1': [5, 6], 'col2': [7, 8]}
df2 = pd.DataFrame(data2)
print("\nDataFrame 2:")
print(df2)

# Gabungkan kedua DataFrame secara vertikal (axis=0, default)
df_combined_vertical = pd.concat([df1, df2], ignore_index=True)
print("\nDataFrame Gabungan (Vertikal):")
print(df_combined_vertical)

# Buat contoh DataFrame ketiga dengan kolom yang berbeda untuk penggabungan horizontal
data3 = {'col3': [9, 10], 'col4': [11, 12]}
df3 = pd.DataFrame(data3)
print("\nDataFrame 3 (untuk penggabungan horizontal):")
print(df3)

# Gabungkan df1 dan df3 secara horizontal (axis=1)
df_combined_horizontal = pd.concat([df1, df3], axis=1)
print("\nDataFrame Gabungan (Horizontal):")
print(df_combined_horizontal)