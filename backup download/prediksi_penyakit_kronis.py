# -*- coding: utf-8 -*-
"""Prediksi Penyakit Kronis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d5P-mptyyLKymGpUwjSKY9t9hps_wIir
"""

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

"""# Import Library"""

!pip install split-folders
!pip install tensorflow
!pip install keras_tuner
!pip install tensorflowjs

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.multioutput import MultiOutputClassifier

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

import pickle
import tensorflowjs as tfjs

from google.colab import drive
drive.mount('/content/drive')

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder

from google.colab import files
files.upload()

"""# Setup Folder dan Split Dataset Manual"""

!ls -lha kaggle.json
!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d iammustafatz/diabetes-prediction-dataset
!unzip diabetes-prediction-dataset.zip

!kaggle datasets download -d fedesoriano/stroke-prediction-dataset
!unzip stroke-prediction-dataset.zip

!kaggle datasets download -d alexteboul/heart-disease-health-indicators-dataset
!unzip heart-disease-health-indicators-dataset.zip

!kaggle datasets download -d alexteboul/diabetes-health-indicators-dataset
!unzip diabetes-health-indicators-dataset.zip

# Muat setiap dataset
df_heart = pd.read_csv('heart_disease_health_indicators_BRFSS2015.csv')
df_stroke = pd.read_csv('healthcare-dataset-stroke-data.csv')
df_diabetes = pd.read_csv('diabetes_prediction_dataset.csv')
# df_lung_cancer = pd.read_csv('lung_cancer_prediction_dataset.csv') # Opsional, jika ingin menambahkan kanker paru

print("Dataset berhasil dimuat.")
print(f"Shape Heart Disease: {df_heart.shape}")
print(f"Shape Stroke: {df_stroke.shape}")
print(f"Shape Diabetes: {df_diabetes.shape}")
# print(f"Shape Lung Cancer: {df_lung_cancer.shape}")

df1 = pd.read_csv('diabetes_012_health_indicators_BRFSS2015.csv')
df2 = pd.read_csv('diabetes_binary_5050split_health_indicators_BRFSS2015.csv')
df3 = pd.read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')

df_heart1 = pd.read_csv('heart_disease_health_indicators_BRFSS2015.csv')
df_heart1.describe(include = 'all')

df1.describe(include = 'all')

df_heart

df_stroke

df_diabetes.head()

"""# Eksplorasi Data Awal (EDA) dan Pembersihan

## EDA untuk Heart Disease Dataset
"""

print("\n--- EDA untuk Heart Disease Dataset ---")
print(df_heart.head())
print(df_heart.info())
print("\nMissing Value:\n", df_heart.isnull().sum())
print("\n", df_heart['HeartDiseaseorAttack'].value_counts(normalize=True)) # Distribusi target

'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk'

kolom_kategorik1 = ['Sex', 'Education', 'Smoker', 'Diabetes', 'Income', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk']
for kolom in kolom_kategorik1:
    print(f"Unique values di kolom '{kolom}':")
    print(df_heart[kolom].unique())
    print('-' * 40)

df_heart = df_heart[df_heart['Diabetes'] != 1]

print("\nBanyaknya Data perstatus Stroke ", df_heart['Stroke'].value_counts(normalize=False)) # Distribusi target
print("\nBanyaknya Data perstatus Diabetes ", df_heart['Diabetes'].value_counts(normalize=False)) # Distribusi target
print("\nBanyaknya Data perstatus Penyakit Jantung ", df_heart['HeartDiseaseorAttack'].value_counts(normalize=False)) # Distribusi target

"""## EDA untuk Stroke Dataset"""

print("\n--- EDA untuk Stroke Dataset ---")
print(df_stroke.head())
print("\n", df_stroke.info())
print("\nMissing Value:\n", df_stroke.isnull().sum())
print("\nBanyaknya Data per-Target ", df_stroke['stroke'].value_counts(normalize=False)) # Distribusi target
print("\nDistribusi Target ", df_stroke['stroke'].value_counts(normalize=True)) # Distribusi target

print("\nBanyaknya Data MV per-ever_married ", df_stroke['ever_married'].value_counts(normalize=False)) # Distribusi target
print("\nBanyaknya Data MV per-work_type ", df_stroke['work_type'].value_counts(normalize=False)) # Distribusi target
print("\nBanyaknya Data MV per-Residence_type ", df_stroke['Residence_type'].value_counts(normalize=False)) # Distribusi target

df_stroke_mv = df_stroke[df_stroke['bmi'].isnull()]
print("\nBanyaknya Data MV per-Target ", df_stroke_mv['stroke'].value_counts(normalize=False)) # Distribusi target
print("\nDistribusi Data MV Target ", df_stroke_mv['stroke'].value_counts(normalize=True)) # Distribusi target
df_stroke_mv

df_stroke['bmi'] = df_stroke.groupby(['stroke','gender', 'hypertension', 'heart_disease', 'smoking_status'])['bmi'].transform(
    lambda x: x.fillna(x.median()))

print(df_stroke.head())
print("\nMissing Value:\n", df_stroke.isnull().sum())
print("\nBanyaknya Data per-Target ", df_stroke['stroke'].value_counts(normalize=False)) # Distribusi target
print("\nDistribusi Target ", df_stroke['stroke'].value_counts(normalize=True)) # Distribusi target

kolom_kategorik2 = df_stroke.select_dtypes(include=['object', 'category']).columns
for kolom in kolom_kategorik2:
    print(f"Unique values di kolom '{kolom}':")
    print(df_stroke[kolom].unique())
    print('-' * 40)

df_stroke_go = df_stroke[df_stroke['gender'] == 'Other']
print("\nBanyaknya Data gender = other per-Target ", df_stroke_go['stroke'].value_counts(normalize=False)) # Distribusi target
df_stroke_go

df_stroke = df_stroke[df_stroke['gender'] != 'Other']
kolom_kategorik2 = df_stroke.select_dtypes(include=['object', 'category']).columns
for kolom in kolom_kategorik2:
    print(f"Unique values di kolom '{kolom}':")
    print(df_stroke[kolom].unique())
    print('-' * 40)

"""## EDA untuk Diabetes Dataset"""

print("\n--- EDA untuk Diabetes Dataset ---")
print(df_diabetes.head())
print(df_diabetes.info())
print("\nMissing Value:\n", df_diabetes.isnull().sum())
print("\nBanyaknya Data MV per-Target ", df_diabetes['diabetes'].value_counts(normalize=False)) # Distribusi target
print("\nDistribusi Target ", df_diabetes['diabetes'].value_counts(normalize=True)) # Distribusi target

kolom_kategorik3 = df_diabetes.select_dtypes(include=['object', 'category']).columns
for kolom in kolom_kategorik3:
    print(f"Unique values di kolom '{kolom}':")
    print(df_diabetes[kolom].unique())
    print('-' * 40)

df_diabetes_go = df_diabetes[df_diabetes['gender'] == 'Other']
print("\nBanyaknya Data gender = other per-Target ", df_diabetes_go['diabetes'].value_counts(normalize=False)) # Distribusi target
df_diabetes_go

df_diabetes = df_diabetes[df_diabetes['gender'] != 'Other']
kolom_kategorik3 = df_diabetes.select_dtypes(include=['object', 'category']).columns
for kolom in kolom_kategorik3:
    print(f"Unique values di kolom '{kolom}':")
    print(df_diabetes[kolom].unique())
    print('-' * 40)

"""## Visualisasi Data"""

# Contoh visualisasi distribusi IMT dari dataset stroke
plt.figure(figsize=(8, 6))
sns.histplot(df_stroke['bmi'].dropna(), kde=True)
plt.title('Distribusi BMI di Dataset Stroke')
plt.xlabel('BMI')
plt.ylabel('Frekuensi')
plt.show()

plt.figure(figsize=(12, 10))
sns.heatmap(df_stroke.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap Korelasi untuk Stroke Dataset')
plt.show()

# Visualisasi korelasi di dataset jantung
plt.figure(figsize=(12, 10))
sns.heatmap(df_heart.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap Korelasi untuk Heart Disease Dataset')
plt.show()

# Contoh visualisasi distribusi IMT dari dataset dabetes
plt.figure(figsize=(8, 6))
sns.histplot(df_diabetes['bmi'].dropna(), kde=True)
plt.title('Distribusi BMI di Dataset Diabetes')
plt.xlabel('BMI')
plt.ylabel('Frekuensi')
plt.show()

plt.figure(figsize=(12, 10))
sns.heatmap(df_diabetes.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap Korelasi untuk Diabetes Dataset')
plt.show()

"""# Harmonisasi dan Penggabungan Dataset"""

# Rename kolom target untuk kejelasan dan konsistensi
df_heart = df_heart.rename(columns={'HeartDiseaseorAttack': 'HeartDisease'})
df_stroke = df_stroke.rename(columns={'stroke': 'Stroke'})
df_diabetes = df_diabetes.rename(columns={'diabetes': 'Diabetes'})

# Pilih fitur umum yang akan digunakan dan harmonisasikan nama kolom
# Ini adalah contoh, Anda mungkin perlu menyesuaikannya berdasarkan EDA Anda
common_features = [
    'Age', 'Sex', 'BMI', 'SmokingStatus', 'Hypertension', 'HeartDisease',
    'Glucose'
] # Ini harus diverifikasi lagi dengan kolom asli dari setiap dataset!

# Perbaiki nama kolom yang berbeda antar dataset (contoh)
df_heart = df_heart.rename(columns={
    'Age': 'Age', 'Sex': 'Gender', 'BMI': 'BMI', 'Smoker': 'SmokingStatus',
    'HighBP': 'Hypertension'
})
df_stroke = df_stroke.rename(columns={
    'gender': 'Gender', 'age': 'Age', 'avg_glucose_level': 'Glucose',
    'bmi': 'BMI', 'hypertension': 'Hypertension', 'heart_disease': 'HeartDisease',
    'smoking_status': 'SmokingStatus'
})
df_diabetes = df_diabetes.rename(columns={
    'gender': 'Gender', 'age': 'Age', 'bmi': 'BMI', 'hypertension': 'Hypertension',
    'heart_disease': 'HeartDisease', 'smoking_history': 'SmokingStatus',
    'blood_glucose_level': 'Glucose'
})

df_heart['Diabetes'] = df_heart['Diabetes'].map({0:0, 2: 1})
df_stroke['Gender'] = df_stroke['Gender'].map({'Female':0, 'Male':1})
df_diabetes['Gender'] = df_diabetes['Gender'].map({'Female':0, 'Male':1})
df_stroke['SmokingStatus'] = df_stroke['SmokingStatus'].map({'never smoked': 0, 'smokes': 1, 'formerly smoked': 2, 'Unknown': 3})
df_diabetes['SmokingStatus'] = df_diabetes['SmokingStatus'].map({'never': 0, 'current': 1, 'former': 2, 'ever': 2, 'not current': 2, 'No Info': 3})

df_heart

"""## Imputasi Nilai Glucose Dataset HeartDisease"""

df_heart['Glucose'] = np.nan

df_heart

df_diabetes

selected_features = ['Gender', 'Age', 'Hypertension', 'HeartDisease', 'SmokingStatus', 'BMI', 'Diabetes']

# Fitur dan target untuk model
X_train = df_diabetes[selected_features]
y_train = df_diabetes['Glucose']

X_missing = df_heart[selected_features]

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_missing_scaled = scaler.transform(X_missing)

# Menggunakan Random Forest
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

predicted_glucose = model.predict(X_missing_scaled)
# Masukkan prediksi ke df_missing
df_heart.loc[:, 'Glucose'] = predicted_glucose
df_heart['Glucose'] = df_heart['Glucose'].round(0).astype(int)

df_heart

"""# Pemisahan Fitur dan Target"""

# --- 3. Pemisahan Fitur dan Target ---
# Fitur (X) dan Target (Y)
# Target di sini adalah multi-output: penyakit jantung, stroke, diabetes
target_columns = ['HeartDisease', 'Stroke', 'Diabetes']

#features_columns = [col for col in df_heart.columns if col not in target_columns]
features_columns = ['Hypertension', 'Gender', 'BMI', 'SmokingStatus', 'Age', 'Glucose']
features_columns1 = ['Hypertension', 'Gender', 'BMI', 'SmokingStatus', 'Age', 'Glucose']

X2 = df_heart[features_columns]
X = df_heart.drop(columns=target_columns)
y = df_heart[target_columns]


print("\nBentuk X:", X.shape)
# print("Bentuk X1:", X1.shape)
print("Bentuk y:", y.shape)

X2.columns

X2

X

X.info()

X.hist(figsize=(12, 8), bins=20)
plt.tight_layout()
plt.show()

X2.hist(figsize=(12, 8), bins=20)
plt.tight_layout()
plt.show()

"""# Penanganan Data Imbalance

## 1

SMOTE per label
"""

X_balanced_list = []
y_balanced_list = []

min_class_len = []

"""SMOTE untuk setiap label, dan catat jumlah minimum antar kelas"""

for col in target_columns:
    smote = SMOTE(random_state=42)
    X_res, y_res = smote.fit_resample(X, y[col])

    # Cari jumlah terkecil antara kelas 0 dan 1
    count_0 = (y_res == 0).sum()
    count_1 = (y_res == 1).sum()
    min_len = min(count_0, count_1)
    min_class_len.append(min_len)

    # Simpan hasil SMOTE sementara
    X_balanced_list.append(X_res)
    y_balanced_list.append(y_res)

"""Ambil jumlah minimum per label setelah SMOTE dan potong ke jumlah itu per kelas"""

# Cari min_len global dari hasil SMOTE per label
min_len_global = min(
    min((y_res == 0).sum(), (y_res == 1).sum())
    for y_res in y_balanced_list
)

# Inisialisasi ulang untuk final list
final_X_list = []
final_y_list = []

for i in range(len(target_columns)):
    X_res = X_balanced_list[i]
    y_res = y_balanced_list[i]

    # Ambil indeks 0 dan 1 sesuai jumlah min_len_global
    idx_0 = np.where(y_res == 0)[0][:min_len_global]
    idx_1 = np.where(y_res == 1)[0][:min_len_global]
    selected_idx = np.concatenate([idx_0, idx_1])

    final_X_list.append(X_res.iloc[selected_idx])
    final_y_list.append(y_res.iloc[selected_idx])

"""Menggabungkan semuanya dan konversi ke DF"""

X_final = final_X_list[0]
y_final = np.column_stack(final_y_list)

X = pd.DataFrame(X_final, columns=X.columns)
y = pd.DataFrame(y_final, columns=target_columns)

X

"""Pembulatan Nilai"""

# List variabel (kolom) yang akan dikecualikan dari pembulatan
exclude_cols = ['Glucose']

# Identifikasi kolom numerik yang akan dibulatkan
numeric_cols_to_round = [col for col in X.select_dtypes(include=np.number).columns if col not in exclude_cols]

# Bulatkan kolom numerik yang terpilih menjadi 0 angka desimal
for col in numeric_cols_to_round:
    X[col] = X[col].round(0) # Perubahan di sini: .round(0)

# Tentukan kolom-kolom float yang ingin diubah menjadi int
cols_to_convert = numeric_cols_to_round

# Lakukan perubahan tipe data
for col in cols_to_convert:
    # Pastikan tidak ada NaN atau isi NaN jika perlu (misal dengan 0)
    # Jika Anda yakin tidak ada NaN, baris di bawah ini bisa diabaikan
    # df[col] = df[col].fillna(0) # Contoh: mengisi NaN dengan 0

    X[col] = X[col].astype(int)

print("\nDataFrame setelah pembulatan menjadi 0 angka desimal:")
X

# Saring kolom yang dikecualikan
cols_to_convert_to_cat = ['Hypertension', 'HighChol', 'CholCheck', 'Gender', 'Education', 'SmokingStatus', 'Income', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'DiffWalk']

print(f"\nKolom yang akan diubah ke 'category': {cols_to_convert_to_cat}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_cat:
    X[col] = X[col].astype('category')

print("\nX Info:", X.info())
X

"""Cek distribusi tiap label"""

for col in y.columns:
    print(f"Label '{col}' value counts:")
    print(y[col].value_counts())
    print()

print("\nX Info:", y.info())
y

# Untuk mengunduh sebagai CSV
y.to_csv('y_final_data.csv', index=False)
print("File 'y_final_data.csv' telah berhasil disimpan di sesi Colab Anda.")

# Saring kolom yang dikecualikan
cols_to_convert_to_int = y.columns
print(f"\nKolom yang akan diubah ke 'integer': {cols_to_convert_to_int}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_int:
    y[col] = y[col].astype('int')

print("\nX Info:", y.info())
y

df_target = y.copy()
df_target['combined_target'] = df_target['HeartDisease'].astype(str) + \
                        df_target['Stroke'].astype(str) + \
                        df_target['Diabetes'].astype(str)

kolomm = ['combined_target']
for kolom in kolomm:
    print(f"Unique values di kolom '{kolom}':")
    print(df_target[kolom].unique())
    print('-' * 40)

"""## 2"""

import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTEN # Contoh teknik balancing untuk data kategorikal
from collections import Counter

# Asumsi: Anda sudah memiliki DataFrame Anda bernama 'df'

dataframe = df_heart.copy()
cols_to_convert_to_int = target_columns
print(f"\nKolom yang akan diubah ke 'integer': {cols_to_convert_to_int}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_int:
    dataframe[col] = dataframe[col].astype('int')

print("\nX Info:", dataframe.info())
dataframe

print("Shape DataFrame Awal:", dataframe.shape)
print("\nDistribusi Variabel Target Awal:")
print(dataframe[['HeartDisease', 'Stroke', 'Diabetes']].apply(pd.Series.value_counts))

# --- 2. Menggabungkan 3 Variabel Target menjadi 1 Variabel 8 Kelas ---

# Membuat string kombinasi dari 3 target
# Contoh: '000', '001', '010', '011', '100', '101', '110', '111'
dataframe['combined_target'] = dataframe['HeartDisease'].astype(str) + \
                        dataframe['Stroke'].astype(str) + \
                        dataframe['Diabetes'].astype(str)

# Untuk SMOTE, variabel target (y) harus berupa integer atau numerik.
# Kita perlu mengonversi string kombinasi menjadi nilai numerik.
# Salah satu caranya adalah dengan menganggapnya sebagai bilangan biner, lalu mengonversinya ke desimal.
# '000' -> 0, '001' -> 1, '010' -> 2, ..., '111' -> 7
dataframe['combined_target_numeric'] = dataframe['combined_target'].apply(lambda x: int(x, 2))


# Mengubah string kombinasi menjadi nilai numerik/kategorikal
# Kita bisa mapping string ini ke integer 0-7, atau biarkan sebagai string
# Untuk SMOTEN, lebih baik jika target adalah integer atau string kategorikal
# Biarkan sebagai string untuk kemudahan representasi dan SMOTEN dapat menanganinya.

print("\nDistribusi 'combined_target_numeric' sebelum balancing:")
print(dataframe['combined_target_numeric'].value_counts())

# Memisahkan fitur (X) dan target (y)
X = dataframe.drop(['HeartDisease', 'Stroke', 'Diabetes', 'combined_target', 'combined_target_numeric'], axis=1)
y = dataframe['combined_target_numeric']

# --- 3. Melakukan Prosedur Balancing Data ---

# Inisialisasi SMOTEN (untuk data dengan fitur dan target kategorikal/campuran)
# Jika fitur Anda hanya numerik, Anda bisa pakai SMOTE atau ADASYN.
# Jika fitur Anda campuran (numerik dan kategorikal), SMOTENC atau SMOTEN lebih cocok.
# SMOTEN secara spesifik cocok untuk dataset di mana fitur dan target bisa berupa kategorikal.
smoten = SMOTEN(random_state=42) # random_state untuk reproduktifitas

print(f"\nJumlah sampel per kelas sebelum balancing: {Counter(y)}")

X_resampled, y_resampled = smoten.fit_resample(X, y)

print(f"Jumlah sampel per kelas setelah balancing: {Counter(y_resampled)}")
print(f"Shape X_resampled setelah balancing: {X_resampled.shape}")

# Menggabungkan kembali X_resampled dan y_resampled menjadi DataFrame baru
df_balanced = pd.DataFrame(X_resampled, columns=X.columns)
df_balanced['combined_target_numeric'] = y_resampled

print("\nShape DataFrame setelah balancing:", df_balanced.shape)

# --- 4. Mengubah Format Variabel Target Kembali Menjadi 3 Variabel Target Semula ---

# Mengonversi kembali 'combined_target_numeric' ke string biner 3 digit
# Misalnya, 0 -> '000', 1 -> '001', ..., 7 -> '111'
df_balanced['combined_target_str'] = df_balanced['combined_target_numeric'].apply(lambda x: format(x, '03b'))

# Memisahkan 'combined_target' menjadi 3 kolom target asli
df_balanced['HeartDisease'] = df_balanced['combined_target_str'].str[0].astype(int)
df_balanced['Stroke'] = df_balanced['combined_target_str'].str[1].astype(int)
df_balanced['Diabetes'] = df_balanced['combined_target_str'].str[2].astype(int)

# Opsional: Hapus kolom 'combined_target_numeric' dan 'combined_target_str' jika tidak diperlukan lagi
df_balanced = df_balanced.drop(['combined_target_numeric', 'combined_target_str'], axis=1)

print("\nDistribusi Variabel Target Setelah Resampling dan Pemisahan:")
print(df_balanced[['HeartDisease', 'Stroke', 'Diabetes']].apply(pd.Series.value_counts))

print("\nDataFrame Akhir setelah balancing dan pemisahan kembali (beberapa baris pertama):")
print(df_balanced.head())

file_path = "/content/drive/My Drive/Capstone LaskarAI/Data SMOTEN.csv"  # Nama file output
df_balanced.to_csv(file_path, index=True)  # index=False agar indeks tidak disertakan dalam file Excel

#df_balanced = pd.read_csv('/content/drive/My Drive/Capstone LaskarAI/Data SMOTEN.csv')

# atau upload file nya
df_balanced = pd.read_csv('Data SMOTEN.csv')
df_balanced.drop(columns=['Unnamed: 0'], inplace=True)
df_balanced

df_balanced = pd.read_csv('Data SMOTEN.csv')
df_balanced.drop(columns=['Unnamed: 0'], inplace=True)
df_balanced

X = df_balanced.drop(columns=target_columns)
y = df_balanced[target_columns]

X

# List variabel (kolom) yang akan dikecualikan dari pembulatan
exclude_cols = ['Glucose']

# Identifikasi kolom numerik yang akan dibulatkan
numeric_cols_to_round = [col for col in X.select_dtypes(include=np.number).columns if col not in exclude_cols]

# Bulatkan kolom numerik yang terpilih menjadi 0 angka desimal
for col in numeric_cols_to_round:
    X[col] = X[col].round(0).astype(int) # Perubahan di sini: .round(0)

print("\nDataFrame setelah pembulatan menjadi 0 angka desimal:")
X

# Saring kolom yang dikecualikan
cols_to_convert_to_cat = ['Hypertension', 'HighChol', 'CholCheck', 'Gender', 'Education', 'SmokingStatus', 'Income', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'DiffWalk']

print(f"\nKolom yang akan diubah ke 'category': {cols_to_convert_to_cat}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_cat:
    X[col] = X[col].astype(str)

print("\nX Info:", X.info())
X

print("\nX Info:", y.info())
y

for col in y.columns:
    print(f"Label '{col}' value counts:")
    print(y[col].value_counts())
    print()

# Saring kolom yang dikecualikan
cols_to_convert_to_int = y.columns
print(f"\nKolom yang akan diubah ke 'integer': {cols_to_convert_to_int}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_int:
    y[col] = y[col].astype('int')

print("\nX Info:", y.info())
y

df_target = y.copy()
df_target['combined_target'] = df_target['HeartDisease'].astype(str) + \
                        df_target['Stroke'].astype(str) + \
                        df_target['Diabetes'].astype(str)

kolomm = ['combined_target']
for kolom in kolomm:
    print(f"Unique values di kolom '{kolom}':")
    print(df_target[kolom].unique())
    print('-' * 40)

y

"""## 3"""

import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE # Menggunakan SMOTE
from collections import Counter

# Asumsi: Anda sudah memiliki DataFrame Anda bernama 'df'

dataframe = df_heart.copy()
cols_to_convert_to_int = target_columns
print(f"\nKolom yang akan diubah ke 'integer': {cols_to_convert_to_int}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_int:
    dataframe[col] = dataframe[col].astype('int')

print("\nX Info:", dataframe.info())
dataframe

print("Shape DataFrame Awal:", dataframe.shape)
print("\nDistribusi Variabel Target Awal:")
print(dataframe[['HeartDisease', 'Stroke', 'Diabetes']].apply(pd.Series.value_counts))

# --- 2. Menggabungkan 3 Variabel Target menjadi 1 Variabel 8 Kelas ---

# Membuat string kombinasi dari 3 target
# Contoh: '000', '001', '010', '011', '100', '101', '110', '111'
dataframe['combined_target'] = dataframe['HeartDisease'].astype(str) + \
                        dataframe['Stroke'].astype(str) + \
                        dataframe['Diabetes'].astype(str)

# Untuk SMOTE, variabel target (y) harus berupa integer atau numerik.
# Kita perlu mengonversi string kombinasi menjadi nilai numerik.
# Salah satu caranya adalah dengan menganggapnya sebagai bilangan biner, lalu mengonversinya ke desimal.
# '000' -> 0, '001' -> 1, '010' -> 2, ..., '111' -> 7
dataframe['combined_target_numeric'] = dataframe['combined_target'].apply(lambda x: int(x, 2))


print("\nDistribusi 'combined_target_numeric' sebelum balancing:")
print(dataframe['combined_target_numeric'].value_counts())

# Memisahkan fitur (X) dan target (y)
# Pastikan X hanya berisi fitur numerik saat menggunakan SMOTE
X = dataframe.drop(['HeartDisease', 'Stroke', 'Diabetes', 'combined_target', 'combined_target_numeric'], axis=1)
y = dataframe['combined_target_numeric'] # Menggunakan target numerik

# --- 3. Melakukan Prosedur Balancing Data ---

# Inisialisasi SMOTE
# SMOTE bekerja dengan fitur input numerik.
smote = SMOTE(random_state=42) # random_state untuk reproduktifitas

print(f"\nJumlah sampel per kelas sebelum balancing: {Counter(y)}")

X_resampled, y_resampled = smote.fit_resample(X, y)

print(f"Jumlah sampel per kelas setelah balancing: {Counter(y_resampled)}")
print(f"Shape X_resampled setelah balancing: {X_resampled.shape}")

# Menggabungkan kembali X_resampled dan y_resampled menjadi DataFrame baru
df_balanced = pd.DataFrame(X_resampled, columns=X.columns)
df_balanced['combined_target_numeric'] = y_resampled

print("\nShape DataFrame setelah balancing:", df_balanced.shape)

# --- 4. Mengubah Format Variabel Target Kembali Menjadi 3 Variabel Target Semula ---

# Mengonversi kembali 'combined_target_numeric' ke string biner 3 digit
# Misalnya, 0 -> '000', 1 -> '001', ..., 7 -> '111'
df_balanced['combined_target_str'] = df_balanced['combined_target_numeric'].apply(lambda x: format(x, '03b'))

# Memisahkan 'combined_target' menjadi 3 kolom target asli
df_balanced['HeartDisease'] = df_balanced['combined_target_str'].str[0].astype(int)
df_balanced['Stroke'] = df_balanced['combined_target_str'].str[1].astype(int)
df_balanced['Diabetes'] = df_balanced['combined_target_str'].str[2].astype(int)

# Opsional: Hapus kolom 'combined_target_numeric' dan 'combined_target_str' jika tidak diperlukan lagi
df_balanced = df_balanced.drop(['combined_target_numeric', 'combined_target_str'], axis=1)

print("\nDistribusi Variabel Target Setelah Resampling dan Pemisahan:")
print(df_balanced[['HeartDisease', 'Stroke', 'Diabetes']].apply(pd.Series.value_counts))

print("\nDataFrame Akhir setelah balancing dan pemisahan kembali (beberapa baris pertama):")
print(df_balanced.head())

X = df_balanced.drop(columns=target_columns)
y = df_balanced[target_columns]

X

# List variabel (kolom) yang akan dikecualikan dari pembulatan
exclude_cols = ['Glucose']

# Identifikasi kolom numerik yang akan dibulatkan
numeric_cols_to_round = [col for col in X.select_dtypes(include=np.number).columns if col not in exclude_cols]

# Bulatkan kolom numerik yang terpilih menjadi 0 angka desimal
for col in numeric_cols_to_round:
    X[col] = X[col].round(0) # Perubahan di sini: .round(0)

# Tentukan kolom-kolom float yang ingin diubah menjadi int
cols_to_convert = numeric_cols_to_round

# Lakukan perubahan tipe data
for col in cols_to_convert:
    # Pastikan tidak ada NaN atau isi NaN jika perlu (misal dengan 0)
    # Jika Anda yakin tidak ada NaN, baris di bawah ini bisa diabaikan
    # df[col] = df[col].fillna(0) # Contoh: mengisi NaN dengan 0

    X[col] = X[col].astype(int)

print("\nDataFrame setelah pembulatan menjadi 0 angka desimal:")
X

# Saring kolom yang dikecualikan
cols_to_convert_to_cat = ['Hypertension', 'HighChol', 'CholCheck', 'Gender', 'Education', 'SmokingStatus', 'Income', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'DiffWalk']

print(f"\nKolom yang akan diubah ke 'category': {cols_to_convert_to_cat}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_cat:
    X[col] = X[col].astype('category')

print("\nX Info:", X.info())
X

print("\nX Info:", y.info())
y

for col in y.columns:
    print(f"Label '{col}' value counts:")
    print(y[col].value_counts())
    print()

# Saring kolom yang dikecualikan
cols_to_convert_to_int = y.columns
print(f"\nKolom yang akan diubah ke 'integer': {cols_to_convert_to_int}")

# --- Lakukan perubahan tipe data ---
for col in cols_to_convert_to_int:
    y[col] = y[col].astype('int')

print("\nX Info:", y.info())
y

"""# Tensorflow"""

X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

X_train_tf.info()

# Kolom berdasarkan informasi gambar
binary_cols = [
    'Hypertension', 'HighChol', 'CholCheck', 'SmokingStatus',
    'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump',
    'AnyHealthcare', 'NoDocbcCost', 'DiffWalk', 'Gender'
]

onehot_cols = [
    'GenHlth', 'Education', 'Income'
]

# 1. Ubah kolom binary menjadi tipe integer
X_train_tf[binary_cols] = X_train_tf[binary_cols].astype(int)

# 2. One-hot encoding untuk kolom kategorikal dengan unique > 2
X_train_tf = pd.get_dummies(X_train_tf, columns=onehot_cols, drop_first=False)

X_train_tf.info()

y_train_tf.info()

X_train_tf.describe(include="all").T

# Identifikasi kolom numerik yang perlu di-scaling (misalnya, 'BMI', 'PhysicalHealth', 'MentalHealth', 'SleepTime')
# Jangan scaling kolom yang sudah kategorikal atau biner
numerical_features = ['BMI', 'PhysHlth', 'MentHlth', 'Age', 'Glucose'] # Contoh, sesuaikan dengan kolom X Anda

# Perhatikan: Nama kolom Anda mungkin sudah berubah karena proses SMOTE.
# Pastikan Anda menggunakan nama kolom yang benar dari X_train_tf.columns

scaler = StandardScaler()

# Terapkan scaling HANYA pada fitur numerik di data training dan testing
X_train_tf[numerical_features] = scaler.fit_transform(X_train_tf[numerical_features])
X_test_tf[numerical_features] = scaler.transform(X_test_tf[numerical_features])

# Jika ada fitur kategorikal yang belum di-one-hot-encode,
# Anda perlu melakukan One-Hot Encoding pada X_train_tf dan X_test_tf
# sebelum masuk ke Neural Network, karena NN biasanya membutuhkan input numerik.
# Namun, karena Anda sudah mengubah ke tipe 'category' di Pandas,
# Keras dapat menanganinya jika Anda menggunakan embedding layer,
# atau Anda bisa secara manual melakukan pd.get_dummies()
# Jika fitur kategorikal Anda sudah berupa 0/1 integer, itu sudah siap.

# Jumlah fitur input
input_shape = X_train_tf.shape[1]
# Jumlah label output (jumlah kolom di y)
output_labels = y_train_tf.shape[1]

model = keras.Sequential([
    # Layer input (Dense: setiap neuron terhubung ke setiap neuron di layer sebelumnya)
    layers.Dense(128, activation='relu', input_shape=(input_shape,)),
    layers.Dropout(0.3), # Dropout untuk mengurangi overfitting

    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),

    # Output layer untuk klasifikasi multi-label
    # Jumlah unit sama dengan jumlah kolom target Anda (misal 3 untuk HeartDisease, Stroke, Diabetes)
    # 'sigmoid' karena setiap label adalah klasifikasi biner independen (Ya/Tidak)
    layers.Dense(output_labels, activation='sigmoid')
])

# Tampilkan ringkasan model
model.summary()

# Optimizer: Adam adalah pilihan yang umum dan bagus
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# Loss function: BinaryCrossentropy karena setiap label adalah biner independen
loss_function = tf.keras.losses.BinaryCrossentropy()

# Metrics: Untuk memantau performa selama pelatihan
metrics = ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]

model.compile(optimizer=optimizer,
              loss=loss_function,
              metrics=metrics)

epochs = 50 # Jumlah iterasi pelatihan
batch_size = 32 # Ukuran batch data yang diproses per iterasi

history = model.fit(X_train_tf, y_train_tf,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_split=0.2, # Mengambil 20% dari data training untuk validasi
                    verbose=1) # Menampilkan progress pelatihan

"""# Penanganan Nilai Hilang, Encoding, dan Scaling Menggunakan Pipeline"""

# Penanganan Nilai Hilang, Encoding, dan Scaling Menggunakan Pipeline ---

# Identifikasi kolom kategorikal dan numerik
categorical_features = X.select_dtypes(include=['object', 'bool']).columns
numerical_features = X.select_dtypes(include=np.number).columns

# Buat pipeline preprocessing
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# # Pemisahan Data Latih dan Uji ---
# X_train2, X_test2, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=42, stratify=y)

# print("\nBentuk X_train:", X_train2.shape)
# print("Bentuk X_test:", X_test2.shape)
# print("Bentuk y_train:", y_train.shape)
# print("Bentuk y_test:", y_test.shape)

# X_train2

# Pemisahan Data Latih dan Uji ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("\nBentuk X_train:", X_train.shape)
print("Bentuk X_test:", X_test.shape)
print("Bentuk y_train:", y_train.shape)
print("Bentuk y_test:", y_test.shape)

"""# Pembangunan Model Multi-output Classification"""

import numpy as np
from sklearn.datasets import make_multilabel_classification
from sklearn.multioutput import MultiOutputClassifier
from sklearn.linear_model import LogisticRegression

"""## RandomForest"""

from sklearn.multioutput import MultiOutputClassifier
from sklearn.ensemble import RandomForestClassifier
# Menggunakan RandomForestClassifier sebagai base estimator
# MultiOutputClassifier akan melatih satu classifier untuk setiap target
model = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42)))])

model2 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42)))])

# Atau menggunakan GradientBoostingClassifier
# model = Pipeline(steps=[('preprocessor', preprocessor),
#                         ('classifier', MultiOutputClassifier(GradientBoostingClassifier(n_estimators=100, random_state=42)))])

print("\nMemulai pelatihan model...")
model.fit(X_train, y_train)
print("Pelatihan model selesai.")

import pickle
import tensorflowjs as tfjs

# Simpan model format SavedModel
model.export("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf1")

# Simpan model format HDF5
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf1.h5")

# Simpan model format Keras
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf1.keras")

# Simpan model format TFLite
converter1 = tf.lite.TFLiteConverter.from_saved_model("/content/drive/My Drive/Capstone LaskarAI/saved_model/rf1")
tflite_model1 = converter1.convert()
with open("/content/drive/My Drive/Capstone LaskarAI/rf1.tflite", "wb") as f:
    f.write(tflite_model1)

# Simpan History Model
# with open("/content/drive/My Drive/Capstone LaskarAI/rf1.pkl", "wb") as f:
#    pickle.dump(history.history, f)

# Simpan model format tensorflow.js
tfjs1_path = "/content/drive/My Drive/Capstone LaskarAI/tfjs_model/rf1"
tfjs.converters.save_keras_model(model, tfjs1_path)
print("TensorFlow.js model saved successfully")

y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test) # Untuk ROC AUC

# Mengubah y_pred_proba menjadi format yang dapat digunakan oleh roc_auc_score
# predict_proba mengembalikan daftar array untuk setiap target
# Misalnya: [array([[0.9, 0.1], ...]), array([[0.8, 0.2], ...]), ...]
# Kita perlu mengambil probabilitas kelas positif (indeks 1) untuk setiap target
y_pred_proba_formatted = np.array([pred[:, 1] for pred in y_pred_proba]).T

print("\nMemulai pelatihan model...")
model2.fit(X_train2, y_train)
print("Pelatihan model selesai.")

y_pred2 = model2.predict(X_test2)
y_pred_proba2 = model2.predict_proba(X_test2) # Untuk ROC AUC

# Mengubah y_pred_proba menjadi format yang dapat digunakan oleh roc_auc_score
# predict_proba mengembalikan daftar array untuk setiap target
# Misalnya: [array([[0.9, 0.1], ...]), array([[0.8, 0.2], ...]), ...]
# Kita perlu mengambil probabilitas kelas positif (indeks 1) untuk setiap target
y_pred_proba_formatted2 = np.array([pred[:, 1] for pred in y_pred_proba2]).T

"""### Evaluasi Model"""

print("\n--- Evaluasi Model ---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred[:, i])}")
    try:
        # ROC AUC hanya untuk kelas biner
        if len(y_test.iloc[:, i].unique()) == 2:
            roc_auc = roc_auc_score(y_test.iloc[:, i], y_pred_proba_formatted[:, i])
            print(f"ROC AUC: {roc_auc:.4f}")
        else:
            print("ROC AUC tidak berlaku untuk target non-biner.")
    except ValueError as e:
        print(f"Tidak dapat menghitung ROC AUC untuk {target}: {e}")

# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro = np.mean([f1_score(y_test.iloc[:, i], y_pred[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro:.4f}")

print("\n--- Evaluasi Model RF 2---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred2[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred2[:, i])}")
    try:
        # ROC AUC hanya untuk kelas biner
        if len(y_test.iloc[:, i].unique()) == 2:
            roc_auc = roc_auc_score(y_test.iloc[:, i], y_pred_proba_formatted2[:, i])
            print(f"ROC AUC: {roc_auc:.4f}")
        else:
            print("ROC AUC tidak berlaku untuk target non-biner.")
    except ValueError as e:
        print(f"Tidak dapat menghitung ROC AUC untuk {target}: {e}")

# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro2 = np.mean([f1_score(y_test.iloc[:, i], y_pred2[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro2:.4f}")

"""### Inferences"""

# --- 10. Prediksi untuk Data Baru (Contoh) ---
print("\n--- Contoh Prediksi Data Baru ---")

# Buat contoh data baru dengan struktur kolom yang sama seperti X_train
# Nilai-nilai ini HANYA CONTOH. Anda harus menggantinya dengan data nyata.
new_data = pd.DataFrame([{
    'age': 55,
    'sex': 'Female',
    'bmi': 28.5,
    'smoking_history': 'never smoked',
    'hypertension': 0,
    'heart_disease': 0, # Ini akan diabaikan sebagai fitur, tetapi penting untuk struktur dataframe
    'stroke': 0,       # Ini akan diabaikan sebagai fitur
    'diabetes': 0,     # Ini akan diabaikan sebagai fitur
    'cholesterol': 200,
    'blood_glucose_level': 100,
    'physactivity': 1,
    'fruits': 1,
    'veggies': 1,
    'hvyalcoholconsump': 0,
    'anyhealthcare': 1,
    'nodocbccare': 0,
    'genhlth': 3, # Skala 1-5 (1=Excellent, 5=Poor)
    'menthlth': 0, # Jumlah hari mental health buruk
    'physhlth': 0, # Jumlah hari fisik health buruk
    'diffwalk': 0, # Sulit jalan
    'race': 'White',
    'ever_married': 'Yes',
    'work_type': 'Private',
    'residence_type': 'Urban',
    'avg_glucose_level': 100,
    'cholesterol_high': 0 # Contoh fitur lain yang mungkin ada dari dataset lain
}], columns=features_columns) # Pastikan kolom sesuai dengan X_train

# Lakukan prediksi
risk_predictions = model.predict(new_data)
risk_probabilities = model.predict_proba(new_data)

print("\nPrediksi Risiko:")
for i, target in enumerate(target_columns):
    print(f"Risiko {target}: {'Tinggi' if risk_predictions[0, i] == 1 else 'Rendah'} "
          f"(Probabilitas: {risk_probabilities[i][0, 1]:.4f})") # Probabilitas kelas positif

"""## RegLog"""

from sklearn.linear_model import LogisticRegression
clf = MultiOutputClassifier(LogisticRegression()).fit(X_train2, y_train)
clf.predict(X_test2)

model3 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', MultiOutputClassifier(LogisticRegression()))])
model3.fit(X_train2, y_train)

y_pred3 = model3.predict(X_test2)
y_pred_proba3 = model3.predict_proba(X_test2) # Untuk ROC AUC

# Mengubah y_pred_proba menjadi format yang dapat digunakan oleh roc_auc_score
# predict_proba mengembalikan daftar array untuk setiap target
# Misalnya: [array([[0.9, 0.1], ...]), array([[0.8, 0.2], ...]), ...]
# Kita perlu mengambil probabilitas kelas positif (indeks 1) untuk setiap target
y_pred_proba_formatted3 = np.array([pred[:, 1] for pred in y_pred_proba3]).T

y_pred3 = clf.predict(X_test2)

print("\n--- Evaluasi Model ---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred3[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred3[:, i])}")
    try:
        # ROC AUC hanya untuk kelas biner
        if len(y_test.iloc[:, i].unique()) == 2:
            roc_auc = roc_auc_score(y_test.iloc[:, i], y_pred_proba_formatted3[:, i])
            print(f"ROC AUC: {roc_auc:.4f}")
        else:
            print("ROC AUC tidak berlaku untuk target non-biner.")
    except ValueError as e:
        print(f"Tidak dapat menghitung ROC AUC untuk {target}: {e}")

# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro3 = np.mean([f1_score(y_test.iloc[:, i], y_pred3[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro2:.4f}")

print("\n--- Evaluasi Model RegLog 1---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred3[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred3[:, i])}")


# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro = np.mean([f1_score(y_test.iloc[:, i], y_pred3[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro:.4f}")

"""## Xtra Tree"""



from sklearn.tree import ExtraTreeClassifier
# Menggunakan RandomForestClassifier sebagai base estimator
# MultiOutputClassifier akan melatih satu classifier untuk setiap target
model4 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', MultiOutputClassifier(ExtraTreeClassifier()))])
model4.fit(X_train2, y_train)

y_pred4 = model4.predict(X_test2)
y_pred_proba4 = model4.predict_proba(X_test2) # Untuk ROC AUC

# Mengubah y_pred_proba menjadi format yang dapat digunakan oleh roc_auc_score
# predict_proba mengembalikan daftar array untuk setiap target
# Misalnya: [array([[0.9, 0.1], ...]), array([[0.8, 0.2], ...]), ...]
# Kita perlu mengambil probabilitas kelas positif (indeks 1) untuk setiap target
y_pred_proba_formatted4 = np.array([pred[:, 1] for pred in y_pred_proba4]).T

print("\n--- Evaluasi Model ---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred4[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred4[:, i])}")
    try:
        # ROC AUC hanya untuk kelas biner
        if len(y_test.iloc[:, i].unique()) == 2:
            roc_auc = roc_auc_score(y_test.iloc[:, i], y_pred_proba_formatted4[:, i])
            print(f"ROC AUC: {roc_auc:.4f}")
        else:
            print("ROC AUC tidak berlaku untuk target non-biner.")
    except ValueError as e:
        print(f"Tidak dapat menghitung ROC AUC untuk {target}: {e}")

# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro4 = np.mean([f1_score(y_test.iloc[:, i], y_pred4[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro4:.4f}")

"""## MLPClassifier"""

from sklearn.neural_network import MLPClassifier
# Menggunakan RandomForestClassifier sebagai base estimator
# MultiOutputClassifier akan melatih satu classifier untuk setiap target
model5 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', MultiOutputClassifier(MLPClassifier()))])
model5.fit(X_train2, y_train)

y_pred5 = model5.predict(X_test2)
y_pred_proba5 = model5.predict_proba(X_test2) # Untuk ROC AUC

# Mengubah y_pred_proba menjadi format yang dapat digunakan oleh roc_auc_score
# predict_proba mengembalikan daftar array untuk setiap target
# Misalnya: [array([[0.9, 0.1], ...]), array([[0.8, 0.2], ...]), ...]
# Kita perlu mengambil probabilitas kelas positif (indeks 1) untuk setiap target
y_pred_proba_formatted5 = np.array([pred[:, 1] for pred in y_pred_proba5]).T

print("\n--- Evaluasi Model ---")

# Akurasi per target
for i, target in enumerate(target_columns):
    print(f"\nEvaluasi untuk {target}:")
    print(f"Akurasi: {accuracy_score(y_test.iloc[:, i], y_pred5[:, i]):.4f}")
    print(f"Laporan Klasifikasi:\n{classification_report(y_test.iloc[:, i], y_pred5[:, i])}")
    try:
        # ROC AUC hanya untuk kelas biner
        if len(y_test.iloc[:, i].unique()) == 2:
            roc_auc = roc_auc_score(y_test.iloc[:, i], y_pred_proba_formatted5[:, i])
            print(f"ROC AUC: {roc_auc:.4f}")
        else:
            print("ROC AUC tidak berlaku untuk target non-biner.")
    except ValueError as e:
        print(f"Tidak dapat menghitung ROC AUC untuk {target}: {e}")

# Evaluasi keseluruhan (misalnya, rerata metrik)
# Ini bisa lebih kompleks dan tergantung pada bagaimana Anda ingin menimbang setiap target
print("\n--- Evaluasi Keseluruhan (Rata-rata Makro) ---")
# Misalnya, F1-score rata-rata makro
f1_macro5 = np.mean([f1_score(y_test.iloc[:, i], y_pred5[:, i], average='macro') for i in range(len(target_columns))])
print(f"Rata-rata F1-score (Makro): {f1_macro5:.4f}")

"""## Lazy"""

!pip install lazypredict

from lazypredict.Supervised import LazyClassifier
clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)
models,predictions = clf.fit(X_train2, X_test2, y_train, y_test)
models

"""# Pembangunan Model Multi-output Neural Network"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# --- 2. Preprocessing Data ---
# Identifikasi kolom numerik dan kategori
categorical_cols = X.select_dtypes(include=['object', 'bool']).columns.tolist()
numerical_cols = X.select_dtypes(include=np.number).columns.tolist()


# --- Buat Preprocessing Pipeline ---
# Pipeline untuk fitur numerik: Imputasi Mean -> Scaling
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Pipeline untuk fitur kategorikal: Imputasi Most Frequent -> One-Hot Encoding
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Gabungkan transformer menggunakan ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# --- One-Hot Encode Target Variables secara terpisah ---
y_encoded_list = []
num_classes_per_target = []
target_encoders = {}
for target_col in targets:
    num_classes = y[target_col].nunique()
    print(f"Target '{target_col}': {num_classes} unique classes.")
    if num_classes < 2:
        # Jika target biner (0 atau 1) tetapi hanya ada 1 nilai unik di data,
        # paksa OneHotEncoder untuk menghasilkan 2 kolom (untuk 0 dan 1).
        # Ini hanya jika Anda yakin target memang biner.
        encoder = OneHotEncoder(sparse_output=False, categories=[[0, 1]])
    else:
        encoder = OneHotEncoder(sparse_output=False, categories='auto')

    y_encoded_list.append(encoder.fit_transform(y[[target_col]]))
    num_classes_per_target.append(encoder.categories_[0].shape[0]) # Ambil jumlah kelas yang dihasilkan encoder
    target_encoders[target_col] = encoder

print(f"Jumlah kelas per target yang dihasilkan oleh encoder: {num_classes_per_target}")


# --- Split Data (Training dan Testing) ---
# Gunakan operator * untuk membongkar list y_encoded_list menjadi argumen terpisah
# train_test_split akan mengembalikan tuple panjang, jadi sesuaikan penerimaannya
X_train_raw, X_test_raw, *y_splits = train_test_split(
    X, *y_encoded_list, test_size=0.2, random_state=42
)

# y_splits akan berisi list yang alternating antara train dan test untuk setiap target
# Contoh: [y1_train, y1_test, y2_train, y2_test, y3_train, y3_test]
# Kita perlu mengorganisirnya kembali

y_train_list = []
y_test_list = []
for i in range(len(targets)):
    y_train_list.append(y_splits[i*2])     # 0, 2, 4
    y_test_list.append(y_splits[i*2 + 1]) # 1, 3, 5

# --- Terapkan preprocessing pada X ---
X_train_processed = preprocessor.fit_transform(X_train_raw)
X_test_processed = preprocessor.transform(X_test_raw)

print(f"Shape X_train_processed: {X_train_processed.shape}")
print(f"Shape X_test_processed: {X_test_processed.shape}")

print("\nShape of y_train_list elements (after split):")
for i, y_arr in enumerate(y_train_list):
    print(f"  y_train_list[{i}] (for {targets[i]}): {y_arr.shape}")

print("\nShape of y_test_list elements (after split):")
for i, y_arr in enumerate(y_test_list):
    print(f"  y_test_list[{i}] (for {targets[i]}): {y_arr.shape}")

print(f"Shape X_train_processed: {X_train_processed.shape}")
print(f"Shape X_test_processed: {X_test_processed.shape}")
print(f"Jumlah kelas per target: {num_classes_per_target}")

"""## MNN 1"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy

X.info()

X.select_dtypes(include=['category', 'bool']).columns

X.select_dtypes(include=np.number).columns

targets = y.columns
print(targets)

# --- 2. Preprocessing Data ---
# Identifikasi kolom numerik dan kategori
categorical_cols = X.select_dtypes(include=['category', 'bool']).columns
numerical_cols = X.select_dtypes(include=np.number).columns

# Buat preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])

# Split data X dan y terlebih dahulu
X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Terapkan preprocessing pada X
X_train_processed = preprocessor.fit_transform(X_train_raw)
X_test_processed = preprocessor.transform(X_test_raw)

# One-hot encode target variables separately for train and test sets
y_train_list = []
y_test_list = []
num_classes_per_target = []

for target_col in targets:
    # Ambil nilai unik dari target untuk menentukan jumlah kelas
    num_classes = y_train_raw[target_col].nunique() # Use train data to determine classes
    num_classes_per_target.append(num_classes)
    # Lakukan one-hot encoding
    # encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # Add handle_unknown for consistency
    y_train_list.append(y_train_raw[[target_col]])
    y_test_list.append(y_test_raw[[target_col]]) # Use transform on test data

# Now y_train_list and y_test_list are lists of arrays with consistent sample numbers

X_train_processed

y_train_raw

y_train_list

# --- 3. Membangun Model Multiclass/Multioutput dengan TensorFlow ---

# Definisikan input layer
input_shape = X_train_processed.shape[1]
input_layer = Input(shape=(input_shape,), name='input_features')

# Hidden layers
x = Dense(128, activation='relu')(input_layer)
x = Dense(64, activation='relu')(x)

# Output layers for each target variable
# Penting: Pastikan jumlah unit di setiap output layer sesuai dengan jumlah kelas untuk target tersebut
# dan gunakan activation='softmax' untuk klasifikasi multiclass.
output_layers = []
for i, num_classes in enumerate(num_classes_per_target):
    output_layer = Dense(num_classes, activation='softmax', name=f'output_target_{i+1}')(x)
    output_layers.append(output_layer)

# Buat model
model = Model(inputs=input_layer, outputs=output_layers)

# --- 4. Compile Model ---

# Definisikan loss function dan metrics untuk setiap output
# CategoricalCrossentropy cocok untuk one-hot encoded targets
# CategoricalAccuracy juga cocok
losses = {}
metrics = {}
for i, _ in enumerate(targets):
    losses[f'output_target_{i+1}'] = CategoricalCrossentropy()
    metrics[f'output_target_{i+1}'] = CategoricalAccuracy(name=f'accuracy_target_{i+1}')

model.compile(optimizer=Adam(learning_rate=0.001),
              loss=losses,
              metrics=metrics)

model.summary()

# --- 5. Latih Model ---

# Format y_train_list dan y_test_list untuk feed ke model
# y_train_list adalah list of arrays, kita perlu mengubahnya menjadi dictionary
# dengan nama output layer sebagai kunci
y_train_dict = {f'output_target_{i+1}': y_train_list[i] for i in range(len(y_train_list))}
y_test_dict = {f'output_target_{i+1}': y_test_list[i] for i in range(len(y_test_list))}


history = model.fit(X_train_processed, y_train_dict,
                    epochs=50,  # Sesuaikan jumlah epoch
                    batch_size=32,
                    validation_data=(X_test_processed, y_test_dict))

import pickle
import tensorflowjs as tfjs

import pickle
import tensorflowjs as tfjs

# Simpan model format SavedModel
model.export("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn1")

# Simpan model format HDF5
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn1.h5")

# Simpan model format Keras
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn1.keras")

# Simpan model format TFLite
converter1 = tf.lite.TFLiteConverter.from_saved_model("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn1")
tflite_model1 = converter1.convert()
with open("/content/drive/My Drive/Capstone LaskarAI/mnn1.tflite", "wb") as f:
    f.write(tflite_model1)

# Simpan History Model
with open("/content/drive/My Drive/Capstone LaskarAI/mnn1.pkl", "wb") as f:
    pickle.dump(history.history, f)

# Simpan model format tensorflow.js
tfjs1_path = "/content/drive/My Drive/Capstone LaskarAI/tfjs_model/mnn1"
tfjs.converters.save_keras_model(model, tfjs1_path)
print("TensorFlow.js model saved successfully")

import os

# Buat direktori untuk menyimpan model jika belum ada
model_save_dir = "/content/drive/My Drive/Capstone LaskarAI/saved_model"
os.makedirs(model_save_dir, exist_ok=True)

# Simpan model dalam format SavedModel (rekomendasi TensorFlow)
model_path = os.path.join(model_save_dir, 'multioutput_classification_model1')
model.save(model_path)
print(f"\nModel berhasil disimpan di: {model_path}")

# Jika Anda ingin memuat kembali model:
# loaded_model = tf.keras.models.load_model(model_path)
# print("Model berhasil dimuat kembali.")

# --- 6. Evaluasi Model ---
print("\nEvaluasi Model pada Data Uji:")
evaluation_results = model.evaluate(X_test_processed, y_test_dict, verbose=0)

# Hasil evaluasi akan menjadi list, urutan sesuai dengan compile metrics
print(f"Total Loss: {evaluation_results[0]:.4f}")
# Cetak metrik untuk setiap output
for i in range(len(targets)):
    print(f"  Target {i+1} Loss: {evaluation_results[i+1]:.4f}")
    print(f"  Target {i+1} Accuracy: {evaluation_results[i+1+len(targets)]:.4f}") # Perhatikan indeksnya

# --- 7. Membuat Prediksi ---
print("\nMembuat Prediksi:")
predictions = model.predict(X_test_processed[:5]) # Prediksi untuk 5 sampel pertama

# Prediksi akan berupa list of arrays, satu array untuk setiap output
for i, pred_output in enumerate(predictions):
    print(f"Prediksi untuk Target {i+1} (5 sampel pertama):")
    # Untuk klasifikasi, Anda mungkin ingin mendapatkan kelas prediksi (indeks dengan probabilitas tertinggi)
    predicted_classes = np.argmax(pred_output, axis=1)
    print(predicted_classes)
    print("-" * 30)

# Plotting history (opsional)
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluasi model pada test set
y_train_dict = {
    'heart_disease': y_train['HeartDisease'].values,
    'stroke': y_train['Stroke'].values,
    'diabetes': y_train['Diabetes'].values
}
y_test_dict = {
    'heart_disease': y_test['HeartDisease'].values,
    'stroke': y_test['Stroke'].values,
    'diabetes': y_test['Diabetes'].values
}

# Convert X_test to a NumPy array
X_test_array = X_test.values

# Evaluate the model using a list of target arrays and the NumPy array of features
# The order of target arrays should match the order of outputs in the model: Diabetes, HeartDisease, Stroke
loss, diabetes_loss, heart_loss, stroke_loss, diabetes_acc, heart_acc, stroke_acc = model.evaluate(
    X_test_array,
    [y_test_dict['diabetes'], y_test_dict['heart_disease'], y_test_dict['stroke']],
    verbose=0
)

print(f"\nTotal Test Loss: {loss:.4f}")
print("\n--- Evaluasi untuk Heart Disease ---")
print(f"Loss: {heart_loss:.4f}")
print(f"Accuracy: {heart_acc:.4f}")


print("\n--- Evaluasi untuk Stroke ---")
print(f"Loss: {stroke_loss:.4f}")
print(f"Accuracy: {stroke_acc:.4f}")


print("\n--- Evaluasi untuk Diabetes ---")
print(f"Loss: {diabetes_loss:.4f}")
print(f"Accuracy: {diabetes_acc:.4f}")

# ROC AUC can be calculated separately if needed
# For calculating AUC, you would typically predict probabilities and then use sklearn's roc_auc_score
# y_pred_prob = model.predict(X_test_array) # Use the NumPy array for prediction as well
# heart_auc = roc_auc_score(y_test['HeartDisease'].values, y_pred_prob[1]) # Index 1 for Heart Disease output
# stroke_auc = roc_auc_score(y_test['Stroke'].values, y_pred_prob[2]) # Index 2 for Stroke output
# diabetes_auc = roc_auc_score(y_test['Diabetes'].values, y_pred_prob[0]) # Index 0 for Diabetes output

# print(f"Heart Disease AUC: {heart_auc:.4f}")
# print(f"Stroke AUC: {stroke_auc:.4f}")
# print(f"Diabetes AUC: {diabetes_auc:.4f}")

"""## MNN 2"""

targets = y.columns
print(targets)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.callbacks import EarlyStopping

# --- Definisikan Input Layer ---
input_shape = X_train_processed.shape[1]
input_layer = Input(shape=(input_shape,), name='input_features')

# --- Hidden Layers dengan Dropout ---
x = Dense(256, activation='relu')(input_layer)
x = Dropout(0.3)(x) # Dropout dengan probabilitas 30%
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(64, activation='relu')(x)

# --- Output Layers untuk Setiap Target ---
output_layers = []
for i, num_classes in enumerate(num_classes_per_target):
    output_layer = Dense(num_classes, activation='softmax', name=f'output_target_{i+1}')(x)
    output_layers.append(output_layer)

# --- Buat Model ---
model = Model(inputs=input_layer, outputs=output_layers)

# --- Compile Model ---
losses = {f'output_target_{i+1}': CategoricalCrossentropy() for i in range(len(targets))}
metrics = {f'output_target_{i+1}': CategoricalAccuracy(name=f'accuracy_target_{i+1}') for i in range(len(targets))}

model.compile(optimizer=Adam(learning_rate=0.001),
              loss=losses,
              metrics=metrics)

model.summary()

# --- Callback Early Stopping ---
# Monitor 'val_loss' (loss pada data validasi)
# patience=10 berarti akan berhenti jika val_loss tidak membaik selama 10 epoch berturut-turut
# restore_best_weights=True akan mengembalikan bobot model dari epoch terbaik
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

# --- Format y_train_list dan y_test_list untuk feed ke model ---
# Access elements of y_train_list and y_test_list using their index i
y_train_dict = {f'output_target_{i+1}': y_train_list[i] for i in range(len(y_train_list))}
y_test_dict = {f'output_target_{i+1}': y_test_list[i] for i in range(len(y_test_list))}


# --- Latih Model ---
history = model.fit(X_train_processed, y_train_dict,
                    epochs=100, # Set epoch lebih tinggi karena Early Stopping akan menghentikan secara otomatis
                    batch_size=32,
                    validation_data=(X_test_processed, y_test_dict),
                    callbacks=[early_stopping], # Tambahkan callback EarlyStopping
                    verbose=1)

y_train_dict

# Simpan model format SavedModel
model.export("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn2")

# Simpan model format HDF5
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn2.h5")

# Simpan model format Keras
model.save("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn2.keras")

# Simpan model format TFLite
converter2 = tf.lite.TFLiteConverter.from_saved_model("/content/drive/My Drive/Capstone LaskarAI/saved_model/mnn2")
tflite_model2 = converter2.convert()
with open("/content/drive/My Drive/Capstone LaskarAI/mnn2.tflite", "wb") as f:
    f.write(tflite_model2)

# Simpan History Model
with open("/content/drive/My Drive/Capstone LaskarAI/mnn2.pkl", "wb") as f:
    pickle.dump(history.history, f)

# Simpan model format tensorflow.js
tfjs1_path = "/content/drive/My Drive/Capstone LaskarAI/tfjs_model/mnn2"
tfjs.converters.save_keras_model(model, tfjs1_path)
print("TensorFlow.js model saved successfully")

import os

# Buat direktori untuk menyimpan model jika belum ada
model_save_dir = "/content/drive/My Drive/Capstone LaskarAI/saved_model"
os.makedirs(model_save_dir, exist_ok=True)

# Simpan model dalam format SavedModel (rekomendasi TensorFlow)
model_path = os.path.join(model_save_dir, 'multioutput_classification_model2')
model.save(model_path)
print(f"\nModel berhasil disimpan di: {model_path}")

# Jika Anda ingin memuat kembali model:
# loaded_model = tf.keras.models.load_model(model_path)
# print("Model berhasil dimuat kembali.")

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

# --- Evaluasi Model pada Data Uji ---
print("\n=== Evaluasi Model pada Data Uji ===")
evaluation_results = model.evaluate(X_test_processed, y_test_dict, verbose=0)

# Hasil evaluasi akan menjadi list, urutan sesuai dengan compile metrics
print(f"Total Loss: {evaluation_results[0]:.4f}")
# Cetak metrik untuk setiap output
for i in range(len(targets)):
    print(f"  Target {i+1} Loss: {evaluation_results[i+1]:.4f}")
    print(f"  Target {i+1} Accuracy: {evaluation_results[i+1+len(targets)]:.4f}")

# --- Membuat Prediksi ---
predictions = model.predict(X_test_processed)

# --- Evaluasi Detail per Target Output ---
for i, target_col in enumerate(targets):
    print(f"\n--- Metrik Evaluasi untuk Target: {target_col} ---")

    # Ambil probabilitas prediksi untuk target ini
    pred_prob = predictions[i]
    # Konversi probabilitas ke kelas prediksi (indeks dengan probabilitas tertinggi)
    predicted_classes = np.argmax(pred_prob, axis=1)

    # Ambil kelas sebenarnya (dari one-hot encoded ke integer)
    true_classes = np.argmax(y_test_list[i], axis=1)

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(true_classes, predicted_classes, zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(true_classes, predicted_classes)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=np.unique(true_classes),
                yticklabels=np.unique(true_classes))
    plt.title(f'Confusion Matrix - {target_col}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC (Jika target biner atau multiclass-one-vs-rest)
    # Ini akan bekerja jika jumlah kelas untuk target tersebut > 1
    if num_classes_per_target[i] > 1:
        try:
            # ROC AUC untuk multiclass (rata-rata per kelas)
            auc_score = roc_auc_score(y_test_list[i], pred_prob, multi_class='ovr', average='weighted')
            print(f"ROC AUC Score (Weighted - OvR): {auc_score:.4f}")

            # Plot ROC Curve (Contoh untuk beberapa kelas pertama jika multiclass)
            plt.figure(figsize=(8, 6))
            for j in range(num_classes_per_target[i]):
                fpr, tpr, _ = roc_curve(y_test_list[i][:, j], pred_prob[:, j])
                plt.plot(fpr, tpr, label=f'Class {j} (AUC = {roc_auc_score(y_test_list[i][:, j], pred_prob[:, j]):.2f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {target_col} (One-vs-Rest)')
            plt.legend(loc="lower right")
            plt.grid(True)
            plt.show()

        except ValueError as e:
            print(f"Tidak dapat menghitung ROC AUC untuk {target_col}: {e}")
    else:
        print(f"ROC AUC tidak berlaku untuk target biner '{target_col}' jika hanya ada satu kelas.")

"""## TensorFlow RF"""

import pandas as pd
import numpy as np
import ydf # Import ydf
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import numpy as np
import tensorflow as tf
import ydf
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.callbacks import EarlyStopping # Untuk NN, kurang relevan di TF-DF

targets = y.columns
print(targets)

# Gabungkan df1 dan df3 secara horizontal (axis=1)
df = pd.concat([X, y], axis=1)
print("\nDataFrame Gabungan (Horizontal):")
print(df)

def df_to_dataset(dataframe, target_columns):
    dataframe = dataframe.copy()
    targets_data = {name: dataframe.pop(name) for name in target_columns}
    # Membuat dataset dari dictionary fitur dan dictionary target
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), targets_data))
    return ds

# --- Split Data (Training, Validation, Testing) ---
# TF-DF mengharapkan target sebagai integer, bukan one-hot encoded
# Pertama, split menjadi train_val dan test
df = pd.concat([X, y], axis=1)
print("\nDataFrame Gabungan (Horizontal):")
print(df)

print("\nShape DataFrame setelah balancing:", df.shape)
train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
# Kemudian, split train_val menjadi train dan validation
train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42) # 0.25 dari 0.8 = 0.2 dari total

print(f"Ukuran data training: {len(train_df)} sampel")
print(f"Ukuran data validation: {len(val_df)} sampel")
print(f"Ukuran data testing: {len(test_df)} sampel")

train_ds = df_to_dataset(train_df, targets)
val_ds = df_to_dataset(val_df, targets)
test_ds = df_to_dataset(test_df, targets)

# --- Penyetelan Hyperparameter ---
print("\n--- Memulai Penyetelan Hyperparameter ---")

# Inisialisasi RandomForestModel
# YDF tidak menggunakan TunerConfig secara langsung, melainkan model dapat diinisialisasi dengan hyperparameters yang ditentukan.
tuned_model = ydf.RandomForestModel(
    label=targets,  # Tentukan kolom target
    num_trees=100,   # Jumlah pohon dalam model
    max_depth=10,    # Kedalaman maksimum pohon
    min_examples=10, # Jumlah minimum contoh pada setiap node
    verbose=0        # Nonaktifkan verbose untuk proses training
)

# Latih model tanpa tuner
tuned_model.fit(
    x=train_df[features],
    y=train_df[targets],
    validation_data=(val_df[features], val_df[targets]) # Gunakan val_df untuk validasi
)

print("\n--- Penyetelan Hyperparameter Selesai ---")
print("Model telah dilatih dengan hyperparameter yang ditentukan:")

# Menampilkan hasil evaluasi model
inspector_tuned = tuned_model.make_inspector()
print(inspector_tuned.evaluation_result())

# --- Pelatihan Model Akhir dengan Hyperparameter Terbaik ---
print("\n--- Melatih Model Akhir ---")

# Dapatkan hyperparameter terbaik dari tuner
best_hyperparameters = tuned_model.make_inspector().hyperparameters()
print(f"Hyperparameter terbaik: {best_hyperparameters}")

# Buat model baru dengan hyperparameter terbaik (atau Anda bisa langsung pakai tuned_model jika sudah dilatih)
# Namun, melatih ulang dengan hyperparameter terbaik pada dataset yang lebih besar
# (jika tuning dilakukan pada subset) atau untuk memastikan convergence adalah praktik yang baik.
final_model = tfdf.keras.RandomForestModel(
    task=tfdf.keras.Task.CLASSIFICATION,
    label_columns=targets,
    # Terapkan hyperparameter terbaik yang ditemukan
    num_trees=best_hyperparameters['num_trees'],
    max_depth=best_hyperparameters['max_depth'],
    min_examples=best_hyperparameters['min_examples'],
    # ... tambahkan hyperparameter lain sesuai kebutuhan
    verbose=1 # Aktifkan verbose untuk melihat progress pelatihan model final
)

# Compile model (metrik saja)
final_model.compile(metrics=["accuracy"])

# Latih model final pada data training lengkap (atau gabungan train+val jika diinginkan)
final_model.fit(x=train_ds, validation_data=val_ds)

# --- Evaluasi Model pada Data Uji ---
print("\n--- Evaluasi Model Akhir pada Data Uji ---")
evaluation_results = final_model.evaluate(x=test_ds, return_dict=True)

print("Hasil Evaluasi Global (untuk setiap target):")
for metric_name, value in evaluation_results.items():
    print(f"  {metric_name}: {value:.4f}")

# --- Membuat Prediksi ---
print("\n--- Membuat Prediksi pada Data Uji ---")
predictions_raw = final_model.predict(test_ds)

# Konversi prediksi probabilitas ke kelas label
predicted_labels = []
for i, pred_output in enumerate(predictions_raw):
    predicted_labels.append(np.argmax(pred_output, axis=1))

# Dapatkan true labels dari test_ds (perlu iterasi karena test_ds adalah tf.data.Dataset)
true_labels_dict = {name: [] for name in targets}
for features_batch, labels_batch in test_ds.as_numpy_iterator():
    for target_name, data in labels_batch.items():
        true_labels_dict[target_name].extend(data.tolist())

true_labels_list_of_arrays = [np.array(true_labels_dict[name]) for name in targets]


# --- Metrik Evaluasi Detail per Target (F1-Score, Confusion Matrix) ---
print("\n--- Metrik Evaluasi Detail per Target (Data Uji) ---")
for i, target_col in enumerate(targets):
    print(f"\n### Metrik untuk Target: {target_col} ###")

    y_true_target = true_labels_list_of_arrays[i]
    y_pred_target = predicted_labels[i]

    # Classification Report
    print("\nClassification Report:")
    # Buat target_names berdasarkan kelas unik yang ada di y_true_target
    target_names_for_report = [str(cls) for cls in sorted(np.unique(y_true_target))]
    print(classification_report(y_true_target, y_pred_target,
                                target_names=target_names_for_report,
                                zero_division=0))

    # F1-Score (Macro dan Weighted)
    f1_macro = f1_score(y_true_target, y_pred_target, average='macro', zero_division=0)
    f1_weighted = f1_score(y_true_target, y_pred_target, average='weighted', zero_division=0)
    print(f"  F1-Score (Macro Average): {f1_macro:.4f}")
    print(f"  F1-Score (Weighted Average): {f1_weighted:.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_true_target, y_pred_target)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=np.unique(y_true_target),
                yticklabels=np.unique(y_true_target))
    plt.title(f'Confusion Matrix - {target_col}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC AUC (Jika relevan dan target bukan biner)
    if len(np.unique(y_true_target)) > 2: # Hanya untuk multiclass (bukan binary)
        try:
            # Karena F1-score sudah dihitung, kita bisa lewatkan ROC AUC jika tidak esensial
            # Jika ingin ROC AUC, Anda perlu akses ke probabilitas per kelas untuk target ini.
            # pred_prob_target = predictions_raw[i]
            # auc_score = roc_auc_score(y_true_target, pred_prob_target, multi_class='ovr', average='weighted')
            # print(f"ROC AUC Score (Weighted - OvR): {auc_score:.4f}")
            pass
        except ValueError as e:
            print(f"Tidak dapat menghitung ROC AUC untuk {target_col}: {e}")

# --- Pengecekan Overfitting ---
print("\n--- Pengecekan Overfitting ---")

# Dapatkan metrik dari model yang sudah dilatih (inspector)
inspector = final_model.make_inspector()

# Kita bisa melihat performa pada training data (in-bag) dan validation data (out-of-bag)
# Untuk RandomForestModel, OOB (Out-of-Bag) error sering digunakan sebagai indikasi validasi.
# Namun, jika kita menggunakan validation_data di .fit(), kita bisa membandingkan hasilnya.

# Performasi pada data training (gunakan model.evaluate() pada train_ds)
train_eval_results = final_model.evaluate(x=train_ds, return_dict=True, verbose=0)

# Performasi pada data validasi (sudah dilakukan sebelumnya di .evaluate())
val_eval_results = evaluation_results # Menggunakan hasil evaluation_results dari data uji

print("\nPerbandingan Metrik Training vs. Validation/Test:")
for target_name in targets:
    train_accuracy = train_eval_results.get(f'output_{target_name}_accuracy', None)
    val_accuracy = val_eval_results.get(f'output_{target_name}_accuracy', None)

    if train_accuracy is not None and val_accuracy is not None:
        print(f"\nTarget: {target_name}")
        print(f"  Training Accuracy: {train_accuracy:.4f}")
        print(f"  Validation/Test Accuracy: {val_accuracy:.4f}")

        if train_accuracy > val_accuracy and (train_accuracy - val_accuracy > 0.05): # Ambang batas bisa disesuaikan
            print(f"  --> Indikasi Overfitting Ringan/Sedang (Perbedaan Accuracy: {train_accuracy - val_accuracy:.4f})")
        elif train_accuracy <= val_accuracy:
            print(f"  Model tidak menunjukkan overfitting pada {target_name} (atau bahkan underfitting jika keduanya rendah).")
        else:
            print(f"  Model cukup seimbang pada {target_name}.")
    else:
        print(f"\nMetrik akurasi tidak ditemukan untuk {target_name} di salah satu dataset.")


# Visualisasi Training Log (jika ada) - TF-DF tidak selalu memberikan history yang sama dengan Keras NN
# Inspector juga bisa memberikan informasi training log.
print("\n--- Training Logs dari Inspector (untuk melihat convergence) ---")
# Untuk RandomForestModel, ada info mengenai OOB (Out-of-Bag) loss per tree
# Ini bisa jadi indikasi convergence
try:
    oob_loss_per_tree = inspector.oob_metrics
    if oob_loss_per_tree:
        # Asumsikan OOB Loss adalah untuk "default_task" atau task pertama
        plt.figure(figsize=(10, 6))
        for target_idx, target_col in enumerate(targets):
            # Ini mungkin perlu disesuaikan tergantung bagaimana oob_metrics disimpan untuk multi-output
            # Biasanya, ini akan per task/target.
            plt.plot(oob_loss_per_tree[f'output_{target_col}_loss'], label=f'{target_col} OOB Loss')
        plt.title('Out-of-Bag (OOB) Loss per Tree (Convergence Check)')
        plt.xlabel('Number of Trees')
        plt.ylabel('OOB Loss')
        plt.legend()
        plt.grid(True)
        plt.show()
    else:
        print("Tidak ada OOB metrics yang tersedia.")
except Exception as e:
    print(f"Tidak dapat mengakses OOB metrics: {e}")

# Visualisasi Feature Importance
print("\n--- Interpretasi Model: Feature Importance ---")
print(inspector.variable_importances())

importances = inspector.variable_importances()
if importances:
    for importance_type_info in importances:
        importance_type_name = importance_type_info.type.name
        print(f"\nFeature Importance ({importance_type_name}):")

        # Sortir dan tampilkan top N fitur
        sorted_features = sorted(importance_type_info.value, key=lambda x: x[1], reverse=True)

        features_to_plot = [x[0] for x in sorted_features[:10]] # Top 10
        scores_to_plot = [x[1] for x in sorted_features[:10]]

        if features_to_plot:
            plt.figure(figsize=(10, 6))
            plt.barh(features_to_plot, scores_to_plot)
            plt.xlabel(f"Importance Score ({importance_type_name})")
            plt.ylabel("Feature")
            plt.title(f"Top 10 Feature Importance ({importance_type_name})")
            plt.gca().invert_yaxis()
            plt.show()
        else:
            print(f"Tidak ada fitur penting untuk tipe '{importance_type_name}'.")

# --- Menyimpan Model ---
model_save_dir = 'tfdf_multioutput_model'
final_model.save(model_save_dir)
print(f"\nModel TF-DF berhasil disimpan di: {model_save_dir}")

# --- Memuat Model ---
# loaded_tfdf_model = tf.keras.models.load_model(model_save_dir)
# print("Model TF-DF berhasil dimuat kembali.")

"""# Prediksi dan Interpretasi Output"""

y_test_dict

# Lakukan prediksi pada data baru (contoh data test)
predictions = model.predict(X_test_processed)

# Prediksi probabilitas untuk setiap penyakit
pred_heart_prob = predictions[0]
pred_stroke_prob = predictions[1]
pred_diabetes_prob = predictions[2]

# Konversi probabilitas menjadi kelas biner (risiko tinggi/rendah)
# Anda perlu menentukan threshold (misalnya 0.5)
threshold = 0.5
pred_heart_class = (pred_heart_prob > threshold).astype(int)
pred_stroke_class = (pred_stroke_prob > threshold).astype(int)
pred_diabetes_class = (pred_diabetes_prob > threshold).astype(int)

# Hitung metrik tambahan (Presisi, Recall, F1-Score, Confusion Matrix)
# Penting: Lakukan ini hanya untuk baris di mana label target tidak NaN.
# Anda perlu memfilter y_test_dict dan pred_..._class.

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import numpy as np

# --- Hitung metrik tambahan (Presisi, Recall, F1-Score, Confusion Matrix) ---

# Contoh untuk Heart Disease
# Convert one-hot encoded true and predicted labels to 1D arrays
true_classes_heart = np.argmax(y_test_dict['output_target_1'], axis=1)
predicted_classes_heart = np.argmax(pred_heart_class, axis=1) # Use np.argmax on the predicted classes as well

print("\n--- Metrik Lebih Lanjut untuk Heart Disease ---")
# Use zero_division parameter to handle cases with no predicted positive samples
print(f"Precision: {precision_score(true_classes_heart, predicted_classes_heart, zero_division=0):.4f}")
print(f"Recall: {recall_score(true_classes_heart, predicted_classes_heart, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(true_classes_heart, predicted_classes_heart, zero_division=0):.4f}")
print("Confusion Matrix:\n", confusion_matrix(true_classes_heart, predicted_classes_heart))

# Example for Stroke
# Convert one-hot encoded true and predicted labels to 1D arrays
true_classes_stroke = np.argmax(y_test_dict['output_target_2'], axis=1)
predicted_classes_stroke = np.argmax(pred_stroke_class, axis=1) # Use np.argmax on the predicted classes as well

print("\n--- Metrik Lebih Lanjut untuk Stroke ---")
print(f"Precision: {precision_score(true_classes_stroke, predicted_classes_stroke, zero_division=0):.4f}")
print(f"Recall: {recall_score(true_classes_stroke, predicted_classes_stroke, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(true_classes_stroke, predicted_classes_stroke, zero_division=0):.4f}")
print("Confusion Matrix:\n", confusion_matrix(true_classes_stroke, predicted_classes_stroke))

# Example for Diabetes
# Convert one-hot encoded true and predicted labels to 1D arrays
true_classes_diabetes = np.argmax(y_test_dict['output_target_3'], axis=1)
predicted_classes_diabetes = np.argmax(pred_diabetes_class, axis=1) # Use np.argmax on the predicted classes as well

print("\n--- Metrik Lebih Lanjut untuk Diabetes ---")
print(f"Precision: {precision_score(true_classes_diabetes, predicted_classes_diabetes, zero_division=0):.4f}")
print(f"Recall: {recall_score(true_classes_diabetes, predicted_classes_diabetes, zero_division=0):.4f}")
print(f"F1-Score: {f1_score(true_classes_diabetes, predicted_classes_diabetes, zero_division=0):.4f}")
print("Confusion Matrix:\n", confusion_matrix(true_classes_diabetes, predicted_classes_diabetes))


# Contoh interpretasi untuk satu individu (misalkan individu pertama di test set)
sample_index = 0
print(f"\n--- Prediksi untuk Sampel Individu ke-{sample_index} ---")
# Use NumPy indexing instead of .iloc for X_test_processed
print(f"Fitur Input (Processed): {X_test_processed[sample_index][:5]}...")

# Access true labels from the 1D arrays
print(f"Label Aktual Heart Disease: {true_classes_heart[sample_index]}")
# Access predicted probabilities correctly from the 2D array
print(f"Prediksi Probabilitas Heart Disease (Class 1): {pred_heart_prob[sample_index][1]:.4f}")
# Access predicted class correctly from the 1D array
print(f"Prediksi Kelas Heart Disease: {'Risiko Tinggi' if predicted_classes_heart[sample_index] == 1 else 'Risiko Rendah'}")

print(f"Label Aktual Stroke: {true_classes_stroke[sample_index]}")
print(f"Prediksi Probabilitas Stroke (Class 1): {pred_stroke_prob[sample_index][1]:.4f}")
print(f"Prediksi Kelas Stroke: {'Risiko Tinggi' if predicted_classes_stroke[sample_index] == 1 else 'Risiko Rendah'}")

print(f"Label Aktual Diabetes: {true_classes_diabetes[sample_index]}")
print(f"Prediksi Probabilitas Diabetes (Class 1): {pred_diabetes_prob[sample_index][1]:.4f}")
print(f"Prediksi Kelas Diabetes: {'Risiko Tinggi' if predicted_classes_diabetes[sample_index] == 1 else 'Risiko Rendah'}")

# Untuk interpretasi faktor pendorong risiko, Anda perlu menggunakan library seperti SHAP
# Ini akan menjadi langkah analisis terpisah setelah model dilatih.
# import shap
# explainer = shap.DeepExplainer(model, X_train_sample_for_shap) # Butuh sample data untuk DeepExplainer
# shap_values = explainer.shap_values(X_test_sample) # Pilih sample dari X_test untuk diinterpretasi
# shap.summary_plot(shap_values[0], X_test_sample, feature_names=feature_names, plot_type="bar", show=False)
# plt.title('SHAP Values for Heart Disease Prediction')
# plt.show()
# # Lakukan ini untuk setiap output head

import pandas as pd

# Buat contoh DataFrame pertama
data1 = {'col1': [1, 2], 'col2': [3, 4]}
df1 = pd.DataFrame(data1)
print("DataFrame 1:")
print(df1)

# Buat contoh DataFrame kedua
data2 = {'col1': [5, 6], 'col2': [7, 8]}
df2 = pd.DataFrame(data2)
print("\nDataFrame 2:")
print(df2)

# Gabungkan kedua DataFrame secara vertikal (axis=0, default)
df_combined_vertical = pd.concat([df1, df2], ignore_index=True)
print("\nDataFrame Gabungan (Vertikal):")
print(df_combined_vertical)

# Buat contoh DataFrame ketiga dengan kolom yang berbeda untuk penggabungan horizontal
data3 = {'col3': [9, 10], 'col4': [11, 12]}
df3 = pd.DataFrame(data3)
print("\nDataFrame 3 (untuk penggabungan horizontal):")
print(df3)

# Gabungkan df1 dan df3 secara horizontal (axis=1)
df_combined_horizontal = pd.concat([df1, df3], axis=1)
print("\nDataFrame Gabungan (Horizontal):")
print(df_combined_horizontal)